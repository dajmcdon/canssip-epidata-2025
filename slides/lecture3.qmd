---
org: "CANSSI Prairies &mdash; Epi Forecasting Workshop 2025"
title: "Compartmental Models, Renewal Equations, and $R_t$ Estimation"
subtitle: "Lecture 3"
short-title: "Epi models"
format: revealjs
---

```{r setup}
#| cache: false
source(here::here("slides", "_code", "setup.R"))
library(epidatr)
library(epipredict)
library(rtestim)
library(epidatasets)
```

```{r, dev.args=list(bg=primary)}
#| include: false
#| label: cover-art
#| cache: true
rto <- estimate_rt(cancovid$incident_cases, x = cancovid$date)         
Rt <- rto$Rt
colnames(Rt) <- as.character(1:ncol(Rt))
Rt <- pivot_longer(as_tibble(Rt) |> mutate(x = cancovid$date), -x) |>
  arrange(name, x)
```

## Outline

1. Compartmental Models
1. Operationalizing Compartmental Models
1. What is $R_t$?
1. Estimating $R_t$
1. Results and features of `{rtestim}`

# Compartmental models {.inverse}


## Mathematical modelling of disease / epidemics is very old 

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\minimize}{minimize}


* [Daniel Bernoulli (1760)]{.tertiary} - studies inoculation against smallpox

* [John Snow (1855)]{.tertiary} - cholera epidemic in London tied to a water pump

* [Ronald Ross (1902)]{.tertiary} - Nobel Prize in Medicine for work on malaria

* [Kermack and McKendrick (1927-1933)]{.tertiary} - basic epidemic (mathematical) model

![Source: Shiode, et al., "The mortality rates and the space-time patterns of John Snowâ€™s cholera epidemic map," (2015)](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12942-015-0011-y/MediaObjects/12942_2015_11_Fig1_HTML.gif?as=webp){height=400px fig-align="center"}


## SIR-type (compartmental) models - Stochastic Version {.nostretch}


::: flex
::: w-75

Suppose each of N people in a bucket at time t:

[Susceptible(t)]{.secondary} : not sick, but could get sick

[Infected(t)]{.secondary} : sick, can make others sick

[Removed(t)]{.secondary} : recovered or dead; not sick, can't get sick

 ---

::: {.incremental}
* During period $h$, each $S$ meets $kh$ people.
* Assume $P( S \textrm{ meets } I \textrm{ and becomes } I ) = c$.
* Then $P( S(t) \rightarrow I(t+h) ) = 1 - (1 - c I(t)  / N )^{hk} \approx kchI(t) / N$.
* Therefore, $I(t+h) | S(t),\ I(t) \sim \textrm{Binom}(S(t),\ kchI(t) / N)$.
* Assume $P( I(t) \rightarrow R(t+h)) = \gamma h,\ \forall t$.
* Then $R(t+h) | I_t \sim \textrm{Binom}(I(t),\ \gamma h)$.
:::

:::

::: w-25

<br><br>

![](gfx/sir.svg){fig-align="center"}

:::
:::




## SIR-type (compartmental) models - Stochastic Version

::: flex
::: w-60


\begin{aligned}
C(t+h) & =  \mathrm{Binom}\left(S(t),\ \frac{\beta}{N} h I(t)\right)\\
D(t+h) & =  \mathrm{Binom}\left(I(t),\ \gamma h\right)\\
S(t+h) & =  S(t) - C(t+h)\\
I(t+h) & =  I(t) + C(t+h) - D(t+h)\\
R(t+h) & =  R(t) + D(t+h)
\end{aligned}

::: {.fragment}
 ---

In the deterministic limit, $h\rightarrow 0$

\begin{aligned}
\frac{dS}{dt} & =  -\frac{\beta}{N} S(t)I(t)\\
\frac{dI}{dt} & =  \frac{\beta}{N} I(t)S(t) - \gamma I(t)\\
\frac{dR}{dt} & =  \gamma I(t)
\end{aligned}
:::

:::

::: w-40

<br><br>

![](gfx/sir.svg){fig-align="center"}

:::
:::

::: {.fragment .box-text .absolute top=30%}

[THE]{.secondary} SIR model is often ambiguous between these.

Typically, people mean the deterministic, continuous time version.
:::


## Data issues 

- [Ideally]{.secondary} we'd observe $S(t)$, $I(t)$, $R(t)$ at all times $t$

- Easier to observe new infections, $I(t+h) - I(t)$

- Removals by death are easier to observe than removals by recovery,  
  so we mostly see $(R(t+h) - R(t)) \times \textrm{(death rate)}$

- The interval between measurements, say $\Delta$, is often $\gg h$

- Measuring $I(t)$ and $R(t)$ (or their rates of change) is hard 
    + testing/reporting is sporadic and error prone
    + Need to model test error (false positives, false negatives) _and_ who gets tested
    + Need to model lag between testing and reporting
    
- Parameters (especially, $\beta$) change during the epidemic
    + Changing behavior, changing policy, environmental factors, vaccines, variants, ...


## Connecting to Data


- Likelihood calculations are straightforward if we can measure $I_t$, $R_t$ at all times $0, h, 2h, \dots, T$

- Or $I_0$, $R_0$ and all the increments $I_{t+h} - I_t$, $R_{t+h} - R_t$

- Still have to optimize numerically

- Likelihood calculations already become difficult if the time between 
  observations $\Delta \gg h$
    + Generally, $\Delta \approx$ 1 day
    + In principle, this just defines another Markov process, with a longer 
    interval $\Delta$ between steps, but to get the likelihood of a $\Delta$ 
    step we have to sum over all possible paths of $h$ steps adding up to it

- Other complications if we don't observe all the compartments, and/or have a 
  lot of noise in our observations
    + We don't and we do.




## Connecting to Data

::: flex
::: w-65

- More tractable to avoid likelihood (Conditional least squares, simulation-based inference)

- Intrinsic issue: Initially, everything  looks exponential
    + Hard to discriminate between distinct models
    + If SIR is true, easier to estimate $\beta - \gamma$ than $(\beta, \gamma)$ or $\beta/\gamma$

- Can sometimes [calibrate]{.secondary} or fix the parameters based on other sources
    + E.g., $1/\gamma =$ average time someone is infectious in clinical studies
:::

::: w-35

<br>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I have been thinking about how different people interpret data differently. And made this xkcd style graphic to illustrate this. <a href="https://t.co/a8LvlmZxT7">pic.twitter.com/a8LvlmZxT7</a></p>&mdash; Jens von Bergmann (@vb_jens) <a href="https://twitter.com/vb_jens/status/1372251931444350976?ref_src=twsrc%5Etfw">March 17, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

:::


:::


## These models can fit well in-sample


* Track observed cases closely (they should)

* Can provide nuanced policy advice on some topics

* Many questions depend on modulating $\beta$
    1. What happens if we lock down?
    2. What happens if we mask?
    3. What happens if we have school online?
    4. Vaccine passport?
    
* Vaccination modeling is easier, directly removes susceptibles

::: {.fragment}

[What about out-of-sample?]{.secondary}

:::

::: {.fragment .absolute top=20% right=30%}

`r fontawesome::fa("exclamation-circle", fill="orange", height="10em")`

:::

# Operationalizing Compartmental Models {.inverse}

## What does this "look like"?


```{r sim-sir, echo = TRUE}
sim_SIR <- function(TT, N = 1000, beta = .1, gamma = .01) {
  
  S <- double(TT)
  I <- double(TT)
  R <- double(TT)
  S[1] <- N - 1
  I[1] <- 1
  
  for (tt in 2:TT) {
    contagions <- rbinom(1, size = S[tt - 1], prob = beta * I[tt - 1] / N)
    removals <- rbinom(1, size = I[tt - 1], prob = gamma)
    S[tt] <- S[tt - 1] - contagions
    I[tt] <- I[tt - 1] + contagions - removals
    R[tt] <- R[tt - 1] + removals
  }
  tibble(S = S, I = I, R = R, time = seq(TT))
}
```


## What does this "look like"?


```{r sim-sir-plot}
#| fig-width: 7
#| fig-height: 3.5
set.seed(123456)
beta <- c(.02, .05, .1, .15, .2)
map(beta, \(b) sim_SIR(600, beta = b) |> mutate(beta = b)) |>
  bind_rows() |>
  pivot_longer(S:R, names_to = "Compartment") |>
  mutate(Compartment = fct_relevel(Compartment, "S"), beta = factor(beta)) |>
  ggplot(aes(time, value, color = Compartment, linetype = beta)) +
  geom_line() + 
  labs(y = "# in compartment", x = "Time", linetype = expression(beta)) + 
  theme(legend.position = "right", legend.title = element_text()) +
  scale_x_continuous(expand = expansion()) +
  scale_y_continuous(expand = expansion()) +
  scale_color_delphi()
```

## So far, just simulations, how do you fit one?

$$
\begin{aligned}
x_{t+1} &= \textrm{OdeSolve}(x_t) + \epsilon_t\\
y_{t+1} &= \textrm{NegBinom}(\textrm{mean} = g(x_t),\ \kappa)
\end{aligned}
$$

* $x_t$ is all the compartments
* $y_t$ are some observations (cases and/or hospitalizations and/or deaths)
* Put priors on all the parameters (they are criminally underidentified)

::: {.fragment}
Turn Bayesian Crank in Stan or similar until you're done.
:::

## `{covidseir}` model

::: {layout="[40,60]" layout-valign="center"}
![](gfx/covidseir-ode.png)

![](gfx/covidseir-dag.png)
:::

* `R` package: <https://seananderson.github.io/covidseir/index.html>
* Paper link: <https://doi.org/10.1371/journal.pcbi.1008274>


## Fit it to BC data and produce a forecast

```{r covidseir-setup}
can_cases_deaths <- read_rds(here::here("_data", "can_cases_deaths.rds"))
can_prov <- can_cases_deaths |>
  rename(geo_value = region) |>
  as_epi_df(as_of = "2024-04-13", other_keys = "hr") |>
  complete(time_value = full_seq(time_value, period = 1), fill = list(cases = 0, deaths = 0)) |>
  sum_groups_epi_df(c(cases, deaths), group_cols = "geo_value")
early_bc <- can_prov |>
  filter(geo_value == "BC", between(time_value, ymd("2020-03-01"), ymd("2020-09-01")))
```

```{r covidseir-fit}
#| echo: true
#| results: hide
samp_frac <- c(rep(0.14, 13), rep(0.21, 38), rep(0.37, nrow(early_bc) - 51))
f_seg <- with(early_bc, case_when(time_value == "2020-03-01" ~ 0, time_value >= "2020-06-01" ~ 3,
  time_value >= "2020-05-01" ~ 2, time_value > "2020-03-01" ~ 1))
fit <- covidseir::fit_seir(daily_cases = early_bc$cases,
                           f_seg = f_seg, # change points in transmission
                           samp_frac_fixed = samp_frac,  # fraction of infections that are tested
                           iter = 500, # number of posterior samples
                           fit_type = "optimizing") # for speed only
```

```{r covid-seir-proj}
days_project <- 45
day_start_reduction <- 5
proj <- covidseir::project_seir(
  fit,
  iter = 1:50,
  forecast_days = days_project,
  f_fixed_start = max(fit$days) + day_start_reduction,
  f_multi = rep(0.67, days_project - day_start_reduction + 1),
  f_multi_seg = 3 # which f segment to use
)
tidy_proj <- covidseir::tidy_seir(proj, resample_y_rep = 30)
tidy_proj <- mutate(tidy_proj, time_value = ymd("2020-03-01") + day - 1)
```

```{r covid-seir-plot}
#| fig-width: 7
ggplot(tidy_proj, aes(time_value)) +
  geom_ribbon(aes(ymin = y_rep_0.05, ymax = y_rep_0.95), fill = primary, alpha = .3) +
  geom_ribbon(aes(ymin = y_rep_0.25, ymax = y_rep_0.75), fill = primary, alpha = .5) +
  geom_line(aes(y = y_rep_0.50), color = primary) +
  geom_point(data = early_bc, aes(y = cases), color = secondary) +
  scale_x_date(expand = expansion(), date_labels = "%b %Y") +
  scale_y_continuous(expand = expansion(c(0, 0.05))) +
  geom_vline(xintercept = max(early_bc$time_value), color = secondary, alpha = .5) +
  labs(y = "Reported cases", x = "Date")
```

## Using this or similar for forecasting

Needed to make lots of assumptions about future epi parameters

* Future transmission rate
* Future case ascertainment rate
* No new variants, or vaccinations, or influx of population
* People don't lose immunity
* Etc., etc., etc.

. . .

More on these as [forecasters]{.secondary} a bit later.

Better described as [scenario models]{.tertiary}.

# What is $R_t$? {.inverse}

## $R_0$ basic reproduction number

Dates at least to [Alfred Lotka (1920s)]{.tertiary} and others (Feller, Blackwell, etc.)

<br>

> The expected number of secondary infections due to a primary infection



::: flex
::: w-40
<br><br>

* $R_0 < 1$ [&xrArr;]{.tertiary} the epidemic will die out

<br>

* $R_0 > 1$ [&xrArr;]{.tertiary} the epidemic will grow until everyone is infected
:::

::: {.w-60 style="text-align: center;"}
<canvas id="simulation" width="800" height="500"></canvas>
<script src="_code/infections.js"></script>
:::
:::

::: {.fragment .box-text .absolute top=50 left=150}

![Source: Katelyn Jetelina, "YLE Newsletter," 17 March 2025.](gfx/yle-measles.jpg){height=600}

:::

## $R_0$ is entirely retrospective

* It's a property of the pathogen in a fully susceptible (infinite) population

* Each outbreak is like a new sample


* To estimate something like this from data, the "bonehead" way is to 
  1. Wait until the epidemic is over (no more infections circulating)
  2. Contact trace the primary infection responsible for each secondary infection
  3. Take a sample average of the number caused by each primary
  4. Possibly repeat over many outbreaks
  

::: {.fragment .box-text .absolute top=50 left=100}

![Source: Guerra, et al., "The basic reproduction number (R0) of measles," (2019).](gfx/guerra-measles.jpg){height=500}

:::

  
::: {.fragment .box-text .absolute top=300 left=300}

Of course no one actually does that

<br>

Lots of work on how to estimate $R_0$

:::

## Effective reproduction number

Suppose $s$% of the population is susceptible 

Then, "the" effective reproduction number $R=sR_0$

Allows you to reason about things like 

<br>

> The level of vaccine coverage necessary to prevent an outbreak from growing
uncontrollably.

<br>

. . .

So, for measles, if $R_0\approx 15$, the disease will die out if immunity is


$$
sR_0 \leq 1 \Longrightarrow 1-s \leq 1-1/R_0 \approx 93\%
$$


---

<iframe data-src=https://epiengage-measles.tacc.utexas.edu height=850 width=1500></iframe>

## $R(t)$ --- instantaneous reproduction number

* The effective reproduction number in the middle of an outbreak

* Some of the population is immune, others are infected, others susceptible

> The expected number of secondary infections at time $t$ caused by an earlier primary
infection

. . .

$f(a) \geq 0,\ \forall a$ --- the rate at which an infection of age $a$ produces new infections

$$
\begin{aligned}
R_0 &= \int_{0}^\infty f(a)\mathsf{d}a, \\
g(a) &= \frac{f(a)} {\int_{0}^\infty f(a)\mathsf{d}a} = f(a) / R_0.
\end{aligned}
$$

. . .

Can allow $g(t, a)$, hold this fixed for now. 

## The generation interval distribution $g(a)$

<figure>
<!-- https://www.cdc.gov/cfa-behind-the-model/php/data-research/rt-estimates/index.html -->
![Source: US CDC Center for Forecasting Analytics, "Behind the Model."](gfx/infectiousness-over-time-dfe.jpeg)
</figure>


## $R(t)$ and $R_t$ --- renewal equation

$R(t)$ is defined implicitly through the [renewal equation]{.secondary}

$$
x(t) = R(t)\int_0^\infty x(t-a)g(a)\mathsf{d}a,
$$

where $x(t)$ are infections at time $t$.

. . .

<hr/>

In discrete time, 

$$
x_t = R_t\sum_{a=0}^\infty x_{t-a}\widetilde{g}_a = R_t (x * \widetilde{g}).
$$

. . .

<hr/>

And stochasticly,

$$
\mathbb{E}\big[x_t\ |\ x_1,\ldots,x_{t-1}\big] = R_t\sum_{a=0}^\infty x_{t-a}\widetilde{g}_a = R_t (x * \widetilde{g}).
$$


::: {.fragment .box-text .absolute top=50}
Most estimators start here:
$$
\mathbb{E}\big[x_t\ |\ x_1,\ldots,x_{t-1}\big] = R_t\sum_{a=0}^\infty x_{t-a}\widetilde{g}_a.
$$

* Assume $\widetilde{g}$ is known
* Model $x_t\ |\ x_1,\ldots,x_{t-1}$ as Poisson or Negative Binomial
* Turn some inferential crank

:::

## $R_t$ for COVID-19 in the US

<iframe data-src="https://www.cdc.gov/cfa-modeling-and-forecasting/rt-estimates/state-rt-timeseries/chart-covid.html#covid-United%20States" height=700 width=1400></iframe>

[Source: US CDC Center for Forecasting Analytics]{.grey style="font-size:0.7em;"}

## $R_t$ in compartmental models

There is an equivalence between a compartmental model and the renewal equation.

$$
\begin{aligned}
R_0 &= \beta / \gamma\\
i_t &= R_0 S_{t-1} \sum_{k = 1}^t \big[\gamma (1-\gamma)^{k-1}\big] i_{t-k}
= R_t\sum_{k = 1}^t (1-\gamma)^{k-1} i_{t-k}
\end{aligned}
$$

```{r show-sir-Rt}
#| fig-width: 10
set.seed(123)
sir_sim <- sim_SIR(100, beta = 0.2)
sir_sim <- mutate(sir_sim, it = lag(S) - S, Rt = lag(S) * 0.2) |>
  pivot_longer(c(S, I, R, Rt, it)) |>
  mutate(type = case_when(
    name %in% c("S", "I", "R") ~ "compartmental model",
    name == "Rt" ~ "Rt",
    name == "it" ~ "incident infections"
  ))
xint <- filter(sir_sim, name == "Rt") |> mutate(end = (value - 1)^2) |>
  filter(end == min(end, na.rm = TRUE)) |> pull(time)
sir_sim |>
  ggplot(aes(time, value, color = name)) +
  geom_line() +
  geom_vline(xintercept = max(xint), alpha = 0.5) +
  facet_wrap(~type, nrow = 1, scales = "free_y") +
  scale_color_delphi() +
  scale_y_continuous(expand = expansion(c(0, 0.05)), name = "") +
  scale_x_continuous(expand = expansion(), name = "") +
  theme(legend.position = "right")
```

# Estimating $R_t$ {.inverse}

## Data issues

$x_t$ is [Infections]{.secondary}, but we don't ever see those

<figure>
![Source: US CDC Center for Forecasting Analytics, "Behind the Model."](https://www.cdc.gov/cfa-behind-the-model/media/images/2024/10/Fig-6_Sept2024_update.jpg)
</figure>

::: {.fragment .box-text .absolute top=150 left=150}

* Replace [infections]{.secondary} with [cases]{.tertiary}
* Replace [generation interval]{.secondary} with [serial interval]{.tertiary}
* Assume we have the [serial interval]{.tertiary}

:::

## Serial interval distribution

<br>

![](gfx/Incubation_delay.svg)


## Standard model for $R_t$

$$
\begin{aligned}
\eta_t &= \sum_{a=0}^\infty y_{t-a}p_a,\\ \\
y_t\ |\ y_1,\ldots,y_{t-1} &\sim \textrm{Poisson}(R_t\eta_t).
\end{aligned}
$$

* Using $y$ instead of $x$ to be cases or hospitalizations or deaths, [incidence]{.secondary}
* Using $p$ for serial interval distribution (discretized)
* The MLE for $R_t$ is just $y_t / \eta_t$.
* This has really high variance, but unbiased.
* So everybody smooths it.

## The state of the art

::: flex
::: w-40
1. `{EpiEstim}` (Cori, et al., 2013) 
:::
::: w-60
- Gamma prior on $R_t$, but use a trailing window
- Super fast computationally
- Smoothness is ad hoc
:::
:::

::: flex
::: w-40
2. `{EpiFilter}` (Parag, 2020)
:::
::: w-60
- State space model
- One step smoothness: $R_{s+1} \sim \textrm{Gaussian}(R_s,\ \alpha R_s)$
- Uses a discretized particle filter-type algorithm
:::
:::

::: flex
::: w-40
3. `{EpiLPS}` (Gressani, et al., 2022)
:::
::: w-60
- Negative Binomial likelihood
- Smoothness via $\log(R_t\eta_t) = \mathbf{B}_{t,:}\beta$
- $\mathbf{B}$ is cubic B-spline basis, weighted Ridge penalty on $\beta$
- More priors, use Metropolis Adjusted Langevin Algorithm
:::
:::

::: {.fragment .absolute .box-text top=50}
4. `{EpiNow2}` (CDC + CFA, Abbott, et al., 2023ish)

* Negative Binomial likelihood
* Smoothness via a GP prior
* Accommodates the sequence of delays from infection $\longrightarrow$ ??
* Adjusts for real-time issues like partial reporting
* Big Bayesian MCMC in Stan. Very slow.
:::


## Our model

Let $\theta_t := \log(R_t)$. 

Use Poisson likelihood.

$$
\begin{aligned}
\widehat{R} &= \exp(\widehat{\theta}) &\widehat{\theta} &= \argmin_\theta\; \eta^{\mathsf{T}}\exp(\theta) - 
\mathbf{y}^{\mathsf{T}}\theta + \lambda\Vert D^{(k+1)}\theta\Vert_1
\end{aligned}
$$

. . .

* Convex, has a global optimum
* $\lambda$ controls smoothness relative to data fidelity
* $\ell_1$ penalty produces adaptive piecewise polynomials of order $k+1$
* Near minimax optimal for functions with bounded total variation


## Local adaptivity --- $\ell_1$ vs. $\ell_2$


```{r}
#| label: adaptivity
#| cache: true
set.seed(12345)
n <- 101
x <- seq(-.2, .6, length = n)
fn <- function(x, a = 60) {
  g <- function(x) 3 * sin(3 * x) + 2 * sin(15 * x) + 5 * sin(2 * x)
  r <- function(x) sin(a * x)
  rev(- g(x) * (x > 0) - r(x) * (x <= 0))
}
y <- fn(x) + rnorm(n, 0, .5)

o3 <- trendfilter::cv_trendfilter(y, x, nfolds = 10)
lam_loc <- which(o3$lambda == o3$lambda_min)
tf <- predict(o3, which_lambda = "lambda_min")
tf_df <- o3$full_fit$dof[lam_loc]
s1 <- smooth.spline(x, y, df = tf_df)
s2 <- smooth.spline(x, y, df = tf_df + 10)
dat <- tibble(x = x + .2, y = y)
tib <- tibble(x = x + .2, `trend filter` = tf, truth = fn(.env$x), 
              `spline (same df)` = s1$y, `spline (df + 10)` = s2$y)
tib |>
  pivot_longer(-x) |>
  ggplot(aes(x)) +
  geom_point(data = dat, aes(y = y), shape = 16, colour = theme_black) +
  geom_line(aes(y = value, colour = fct_relevel(name, "truth"))) +
  scale_colour_brewer(palette = "Set1", name = "")
```

## Polynomial order, $k=0$

::: flex
::: w-40
<br><br>
$$
\begin{aligned}
D^{(1)} &= \begin{bmatrix} 
1 & -1 &  &  & & \\ 
 & 1 & -1 &  & & \\
  &   &    & \ddots && \\ 
 &   &   &  & 1 & -1 
\end{bmatrix} \\ \\
&\in \mathbb{R}^{(n-1)\times n}
\end{aligned}
$$
:::

::: w-60
```{r}
#| label: k0
#| cache: true
#| fig-width: 5
#| fig-height: 5
#| out-width: "800px"
#| out-height: "800px"
o0 <- trendfilter::cv_trendfilter(y, x, k = 0, nfolds = 10)
tib <- tibble(x = x + .2, y = predict(o0, which_lambda = "lambda_1se"))
ggplot(mapping = aes(x, y)) +
  geom_point(data = dat, shape = 16, colour = primary) +
  geom_line(data = tib, colour = tertiary, linewidth = 1.5)
```
:::
:::

## Polynomial order, $k=1$

::: flex
::: w-40
<br><br>
$$
\begin{aligned}
D^{(2)} &= \begin{bmatrix} 
1 & -2 & 1 &  & & \\ 
 & 1 & -2 & 1 & & \\
  &   &    & \ddots && \\ 
 &   &   & 1 & -2 & 1 
\end{bmatrix} \\ \\
&= D^{(1)}D^{(1)}\\ \\
&\in \mathbb{R}^{(n-k-1)\times n}
\end{aligned}
$$
:::

::: w-60
```{r}
#| label: k1
#| cache: true
#| fig-width: 5
#| fig-height: 5
#| out-width: "800px"
#| out-height: "800px"
o1 <- trendfilter::cv_trendfilter(y, x, k = 1, nfolds = 10)
tib <- tibble(x = x + .2, y = predict(o1, which_lambda = "lambda_1se"))
ggplot(mapping = aes(x, y)) +
  geom_point(data = dat, shape = 16, colour = primary) +
  geom_line(data = tib, colour = tertiary, linewidth = 1.5) 
```
:::
:::

## Polynomial order, $k=2$

::: flex
::: w-40
<br><br>
$$
\begin{aligned}
D^{(3)} &= \begin{bmatrix} 
-1 & 3 & -3 & 1  & & \\ 
 & -1 & 3 & -3 &1 & \\
  &   &    & \ddots && \\ 
 &   &  -1 & 3 & -3 & 1 
\end{bmatrix} \\ \\
&= D^{(1)}D^{(2)}\\ \\
&\in \mathbb{R}^{(n-k-1)\times n}
\end{aligned}
$$
:::

::: w-60
```{r}
#| label: k2
#| cache: true
#| fig-width: 5
#| fig-height: 5
#| out-width: "800px"
#| out-height: "800px"
o2 <- trendfilter::cv_trendfilter(y, x, k = 2, nfolds = 10)
tib <- tibble(x = x + .2, y = predict(o2, which_lambda = "lambda_1se"))
ggplot(mapping = aes(x, y)) +
  geom_point(data = dat, shape = 16, colour = primary) +
  geom_line(data = tib, colour = tertiary, linewidth = 1.5)
```
:::
:::


## Estimation algorithm

$$
\minimize_\theta\; \eta^{\mathsf{T}}\exp(\theta) - 
\mathbf{y}^{\mathsf{T}}\theta + \lambda\Vert D^{(k+1)}\theta\Vert_1
$$

## Estimation algorithm

$$
\minimize_{\theta,\ {\color{BurntOrange} \alpha}}\; 
\eta^{\mathsf{T}}\exp(\theta) - 
\mathbf{y}^{\mathsf{T}}\theta + 
\lambda\Vert D^{(1)}{\color{BurntOrange} \alpha}\Vert_1\quad
{\color{BurntOrange} \textrm{subject to}\quad \alpha = D^{(k)}\theta}
$$


## Estimation algorithm

$$
\minimize_{\theta,\ \alpha}\; 
\eta^{\mathsf{T}}\exp(\theta) - 
\mathbf{y}^{\mathsf{T}}\theta + 
\lambda\Vert D^{(1)} \alpha\Vert_1\quad
\textrm{subject to}\quad \alpha = D^{(k)}\theta
$$

<br>
<hr>
<br>

Alternating direction method of multipliers (ADMM)

$$
\begin{aligned}
\theta &\longleftarrow \argmin_\theta\ \eta^{\mathsf{T}}\exp(\theta) - 
\mathbf{y}^{\mathsf{T}}\theta + 
  \frac{\rho}{2}\Vert D^{(k)}\theta - \alpha + u \Vert_2^2 \\
\alpha &\longleftarrow \argmin_\alpha\ \lambda\Vert D^{(1)} \alpha \Vert_1 +
  \frac{\rho}{2}\Vert D^{(k)}\theta - \alpha + u \Vert_2^2 \\
u &\longleftarrow u + D^{(k)}\theta - \alpha
\end{aligned}
$$


## Estimation algorithm

$$
\minimize_{\theta,\ \alpha}\; 
\eta^{\mathsf{T}}\exp(\theta) - 
\mathbf{y}^{\mathsf{T}}\theta + 
\lambda\Vert D^{(1)} \alpha\Vert_1\quad
\textrm{subject to}\quad \alpha = D^{(k)}\theta
$$

<br>
<hr>
<br>

Alternating direction method of multipliers (ADMM)

$$
\begin{aligned}
\theta &\longleftarrow  {\color{Cerulean}\textrm{Proximal Newton / Fisher Scoring}} \\
\alpha &\longleftarrow  {\color{BurntOrange}\textrm{Fused Lasso Signal Approximator}} \\
u &\longleftarrow u + D^{(k)}\theta - \alpha
\end{aligned}
$$

. . .

Solve sequentially for $\Vert (D^{\dagger})^{\mathsf{T}}(\eta - y)\Vert_\infty = \lambda_1 > \cdots > \lambda_M=\epsilon \lambda_1$. 

# Results and features of [{rtestim}]{.monotype} {.inverse}

## Canadian Covid-19 cases

```{r}
#| label: cancovid
cancovid |>
  ggplot(aes(date)) +
  geom_ribbon(aes(ymin = 0, ymax = incident_cases / 1e3), fill = primary) +
  scale_x_date(expand = expansion()) +
  scale_y_continuous(expand = expansion(c(0, 0.05))) +
  ylab("Reported cases (1000s)") + xlab("Date")
```

## $R_t$ for Canadian Covid-19 cases

```{r}
#| label: cancovid-rt
#| message: false
plot(rto) +
  coord_cartesian(ylim = c(0.7, 2)) + 
  scale_x_date(expand = expansion()) + 
  scale_color_distiller(
    palette = "Set1", 
    direction = 1, 
    name = expression(lambda), 
    trans = "log10",
    labels = scales::label_log(), 
    breaks = c(10^6, 10^7)
  ) +
  theme(plot.background = element_blank())
```


## Reconvolved Canadian Covid-19 cases

```{r}
#| label: cancovid-reconvolved
tibble(
  `Predicted cases` = c(predict(rto)) / 1000, 
  lambda = rep(rto$lambda, each = length(rto$observed_counts)),
  Date = rep(rto$x, times = length(rto$lambda))
) |>
  ggplot() +
  geom_ribbon(
    data = cancovid, 
    aes(date, ymin = 0, ymax = incident_cases / 1000), 
    fill = primary, alpha = .5
  ) +
  geom_line(aes(Date, `Predicted cases`, colour = lambda, group = lambda)) +
  scale_x_date(expand = expansion()) + 
  scale_color_distiller(
    palette = "Set1", 
    direction = 1, 
    name = expression(lambda), 
    trans = "log10",
    labels = scales::label_log(), 
    breaks = c(10^6, 10^7)
  ) +
  scale_y_continuous(
    expand = expansion(c(0, .05)), 
    name = "Predicted / Reported Cases (1000s)"
  ) +
  theme(plot.background = element_blank())
```

## Example simulations for different methods

```{r}
#| label: simulated-realizations
#| fig-width: 10
#| fig-height: 5
#| cache: true
res_dat_long <- read_rds(here::here("_data", "data_example_estimates_long.rds"))
res_dat_long %>%
  filter(si_type == "measles", dist == "Poisson") %>% 
  group_by(Rt_case, method) %>%
  ggplot(aes(y = Rt_value, x = time)) + 
  geom_ribbon(aes(ymax = Upper_bound, ymin = Lower_bound, fill = method), alpha = 0.4) +
  geom_line(aes(col = method)) +
  geom_line(aes(y = trueRt)) + 
  facet_grid(Rt_case ~ method) +
  scale_colour_manual(values = ggsci::pal_uchicago()(8)) +
  scale_fill_manual(values = ggsci::pal_uchicago()(8)) +
  labs(y = "Rt estimates for measles epidemics\nwith Poisson incidence", x = "Time") + 
  scale_x_continuous(expand = expansion()) + 
  scale_y_continuous(expand = expansion(c(0, 0.05))) + 
  coord_cartesian(ylim = c(0, 4)) + 
  theme(legend.position = "none")
```

## `{rtestim}` software

![](gfx/rtestim.png){.absolute top="-8%" right="-15%" height="105%" style="max-height: unset;"}

* Guts are in [C++]{.monotype} for speed

* Lots of the usual S3 methods

* Approximate "confidence" bands

* $\widehat{R}$ is a member of a function space

* Arbitrary spacing of observations

* Built-in cross validation

* Time-varying delay distributions

## `{rtestim}` software

::: flex
::: w-40

* Guts are in [C++]{.monotype} for speed

* Lots of the usual S3 methods

* [Approximate "confidence" bands]{.secondary}

* $\widehat{R}$ is a member of a function space

* Arbitrary spacing of observations

* Built-in cross validation

* Time-varying delay distributions

:::



::: {.w-60 .fragment}

```{r}
#| label: confband
#| out-width: "800px"
#| out-height: "500px"
can_cb <- confband(rto, lambda = rto$lambda[20], level = c(.5, .8, .95))
plot(can_cb) + 
  coord_cartesian(ylim = c(0.6, 1.8)) +
  scale_x_date(expand = expansion()) + 
  theme(plot.background = element_blank())
```

:::
:::

::: {.fragment}
Approximation + Delta method gives

$$
\textrm{Var}(\widehat{R}) = \left(\textrm{diag}(\widehat{y}) + 
\lambda D^{\mathsf{T}}D\right)^{\dagger} \left(\frac{1}{\eta^2}\right)
$$

:::


## `{rtestim}` software

::: flex
::: w-40

* Guts are in [C++]{.monotype} for speed

* Lots of the usual S3 methods

* Approximate "confidence" bands

* [$\widehat{R}$ is a member of a function space]{.secondary}

* [Arbitrary spacing of observations]{.secondary}

* [Built-in cross validation]{.secondary}

* Time-varying delay distributions

:::



::: {.w-60 .fragment}

```{r}
#| label: cross-validation
#| out-width: "800px"
#| out-height: "500px"
# can_cv <- cv_estimate_rt(cancovid$incident_cases, x = cancovid$date, nfold = 10)
can_cv <- read_rds(here::here("_data", "can_cv_10fold.rds"))
plot(can_cv) + 
  theme(plot.background = element_blank()) +
  scale_x_continuous(transform = "log10", labels = scales::label_log())
```

:::
:::

::: {.fragment}
The solution is an element of the space of [_discrete splines of order $k$_]{.tertiary} 
(Tibshirani, 2020)

* Lets us interpolate (and extrapolate) to off-observation points
* Lets us handle uneven spacing
:::

## `{rtestim}` software

::: flex
::: w-40

* Guts are in [C++]{.monotype} for speed

* Lots of the usual S3 methods

* Approximate "confidence" bands

* $\widehat{R}$ is a member of a function space

* Arbitrary spacing of observations

* Built-in cross validation

* [Time-varying delay distributions]{.secondary}

:::



::: {.w-60}

::: {.r-stack}
::: {.fragment}
```{r}
#| label: duotang
#| out-width: "800px"
#| out-height: "600px"
#| fig-height: 4
#| fig-width: 5
props <- read_rds(here::here("_data", "duotang-counts.rds")) |>
  pivot_wider(names_from = pango_group, values_from = n, values_fill = 0) |>
  mutate(total = rowSums(across(-c(week, province)))) %>%
  mutate(across(-c(week, province, total), ~ .x / total)) %>%
  select(-total)

smooth_it <- function(props_group) {
  z <- props_group |> select(-week, -province)
  n <- names(z)
  nn <- gsub(" ", "_", n)
  names(z) <- nn
  form_resp <- paste0("cbind(", paste0(names(z), collapse = ",") ,") ~ ")
  z$time <- as.numeric(props_group$week)
  form <- as.formula(paste0(form_resp, "poly(time, degree = 3)"))
  fits <- nnet::multinom(form, z, trace = FALSE)
  rng <- range(props_group$week)
  alltime <- as.numeric(seq(rng[1], rng[2], by = 1))
  z <- as_tibble(predict(fits, data.frame(time = alltime), type = "probs")) %>%
    mutate(Date = as.Date(alltime))
  z
}

can_props_smoothed <- smooth_it(props %>% filter(province == "Canada"))

can_props_smoothed %>%
  pivot_longer(-Date) |>
  ggplot(aes(Date, y = value, fill = name)) +
  geom_area(position = "stack") +
  ylab("Variants circulating in Canada") +
  xlab("") + 
  theme(plot.background = element_blank()) +
  scale_x_date(name = "", date_breaks = "1 year", date_labels = "%Y", expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) + 
  scale_fill_viridis_d(name = "")
```
:::
::: {.fragment}
```{r}
#| label: show-delay-distns
#| out-width: "800px"
#| out-height: "600px"
#| fig-height: 4
#| fig-width: 5
can_pred_class <- read_rds(here::here("_data", "can_pred_class.rds"))
delay_distns_can <- read_rds(here::here("_data", "delay-distns-byvar.rds")) |> # in rtestim/vignettes
  filter(para == "SI", type %in% unique(can_pred_class$var))
delay_distns_can |>
  rowwise() |>
  mutate(probability = list(discretize_gamma(0:20, shape, scale))) |>
  ungroup() |>
  select(type, probability) |>
  unnest(probability) |>
  group_by(type) |>
  mutate(delay = row_number() - 1) |>
  ggplot(aes(delay, probability, colour = type)) +
  geom_line() + 
  scale_color_brewer(palette = "Set1", name = "") +
  scale_y_continuous(expand = expansion(c(0, 0.05))) +
  theme(plot.background = element_rect(fill = theme_white, colour = theme_white))
```
:::
:::
:::
:::

## `{rtestim}` software

```{r}
#| label: tv-delays
library(Matrix)
n <- nrow(cancovid)
delay_mat <- matrix(0, n, n)
delay_mat[1,1] <- 1
for (iter in 2:n) {
  current_var <- can_pred_class$var[iter]
  current_pars <- delay_distns_can |> filter(type == current_var)
  delay <- discretize_gamma(0:(iter - 1), current_pars$shape, current_pars$scale)
  delay_mat[iter, 1:iter] <- rev(delay)
}
delay_mat <- drop0(as(delay_mat, "CsparseMatrix")) # make it sparse, not necessary
delay_mat <- delay_mat / rowSums(delay_mat) # renormalize
can_tvar <- estimate_rt(
  cancovid$incident_cases, 
  x = cancovid$date, 
  delay_distn = delay_mat,
  lambda_min_ratio = 1e-6
)
can_tvar_ci <- confband(can_tvar, can_tvar$lambda[30], level = c(.5, .8, .95)) |>
  bind_cols(can_pred_class) |>
  mutate(variant = fct_relevel(as.factor(var), "Ancestral lineage"))
```

```{r}
#| label: plot-tvar
#| fig-width: 8
#| fig-height: 4
ggplot(can_tvar_ci, aes(x = Date)) +
  geom_ribbon(
    data = cancovid |> rename(Date = date) |> 
      mutate(incident_cases = incident_cases * 3 / 100000 + .5),
    aes(ymin = 0, ymax = incident_cases), fill = "grey", alpha = .5
  ) +
  geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`, fill = variant), alpha = .2) +
  geom_ribbon(aes(ymin = `10.0%`, ymax = `90.0%`, fill = variant), alpha = .3) +
  geom_ribbon(aes(ymin = `25.0%`, ymax = `75.0%`, fill = variant), alpha = .5) +
  geom_line(aes(y = fit), color = primary) +
  geom_hline(yintercept = 1, color = theme_black) +
  ylab("Estimated Rt with 50%, 80%, and 95%\nconfidence bands") +
  scale_y_continuous(
    expand = expansion(0),
    sec.axis = sec_axis(~ (. - 0.5) * 100 / 3, name = "Observed cases (1000s)")
  ) + 
  scale_x_date(expand = expansion()) +
  coord_cartesian(ylim = c(0.5, 1.75), xlim = ymd(c("2020-04-01", "2023-03-01"))) + 
  scale_fill_brewer(palette = "Set1", name = "") +
  theme(legend.position = "bottom")
```

