---
org: "CANSSI Prairies &mdash; Epi Forecasting Workshop 2025"
title: "Introduction to Panel Data in Epidemiology"
subtitle: "Lecture 1"
short-title: "Understanding Data"
format: revealjs
---


```{r setup}
#| cache: false
#| include: false
source(here::here("slides", "_code", "setup.R"))
library(tidymodels)
library(epidatasets)
library(epipredict)
library(epidatr)
load(here::here("_data", "day1m_queries.rda"))
```


## Outline

1. About

1. Workshop Overview and System Setup

1. Panel Data

1. Versioned Data

1. Epidata Repository and API

1. `{epidatr}` and Other Data

1. Versioning in `{epidatr}`


# About {.inverse}

## Daniel J. McDonald

* PhD in Statistics from Carnegie Mellon University
* Risk Bounds for Time series, esp. macroeconomic forecasting
* Subsequent research and teaching on machine learning (model selection, optimization, regularization, nonparametrics)
* Began working on epidemiology in mid-2020 with Delphi Research Group
* Focus is largely on forecasting and nowcasting epidemic aggregates
* Funding from US CDC (ongoing?), CSTE, NSERC, CANSSI

## About Delphi

* Founded in 2012 at Carnegie Mellon University, now expanded to UC Berkeley, and University of British Columbia.

* Currently 5 faculty, ~10 PhD students, ~15 staff (mostly software engineers).

* Easy to join us from anywhere (lots of volunteers during Covid-19 pandemic).

* We are:
    + CDC Center of Excellence for Influenza and Covid-19 Forecasting (2019-24).
    + CDC Innovation Center for Outbreak Analytics and Disease Modeling (2024-29).

[**Our mission:**]{.primary} To develop the theory and practice of [epidemic detection, tracking and forecasting]{.primary}, and their use in decision making, both public and private.

## What does Delphi do?

* Procure [real-time, aggregated data streams]{.primary} informative of infectious diseases and syndromes, in collaboration with partners in industry and government.

* Extract signals and make them widely available via the [Epidata platform & API]{.primary}.

* Develop and deploy algorithms for [epidemic detection, tracking, forecasting]{.primary}.

* Develop and maintain statistical software packages for these tasks.

* Make it all production-grade, maximally-accessible, and open-source (to serve CDC, state and local public health agencies, epi-forecasting researchers, data journalists, the public)

## What we provide

![](gfx/web_of_parts.svg){fig-align=center}

## Acknowledgements

Most of this material was developed for the [InsightNet Tooling Workshop in December 2024](https://cmu-delphi.github.io/insightnet-workshop-2024/).

::: flex

::: w-45
Thanks to major contributors
: Ryan J. Tibshirani
: Rachel Lobay
: Alice Clima
: Logan Brooks
: Delphi Tooling and Forecasting Team
: Elvis Cai, Olivia Liu, Paul Gustafson
:::

::: w-55
Funding
: Center for Forecasting and Outbreak Analytics 
: Council of State and Territorial Epidemiologists
: NSERC
: CANSSI
:::
:::

::: {layout-ncol=4}
![](/assets/img/delphi.jpg)

![](/assets/img/cmu.png)

![](/assets/img/berkeley.jpg)

![](/assets/img/ubc.jpg)
:::

# Workshop Overview and System Setup {.inverse}

## What we will cover

- Characteristics of panel data in epidemiology
- Tools for processing and plotting panel data
- Statistical background on nowcasting and forecasting
- Tools for building nowcasting and forecasting models 
- Plenty of examples throughout of real case studies

## Goals part I

Present a statistical way of thinking about now/forecasting

Basic mindsets
: - data versioning and structure
: - the importance of empirical validation using techniques like time series cross-validation are ubiquitous

Certain basic modeling considerations
: - starting simple and building up complexity
: - taming variance through regularization,
: - addressing nonstationarity with trailing training windows

## Goals part II

- Present software whichs aid processing, tracking,
  nowcasting, and forecasting with panel data 
- These tools are still in development and we welcome your feedback
- We have tried hard to get the framework right; but many individual
  pieces themselves could still be improved 
- If these aren't working for you, then we want to hear from you!
- We welcome collaboration, and everything we do is open source

## A disclaimer

- My background is primarily in statistics and computer science
- This obviously influences my way of thinking and my approach to 
  nowcasting and forecasting
- I don't have nearly as much experience with traditional epi models,
  but I do have opinions about the pros/cons. 
- Ask me at any point if
  you have a question about why I'm doing things a certain way
  
::: {.fragment .box-text .absolute top=10%}
- This workshop is supposed to be useful for YOU. Ask questions if
  you have them, don't be shy
- We may not (likely won't?) cover everything. Hopefully the materials
  will be a resource for you beyond this workshop
:::

## System setup

```{r}
#| eval: false
#| echo: true
install.packages("remotes")
install.packages("tidyverse")
install.packages("tidymodels")
install.packages("glmnet")
remotes::install_github("cmu-delphi/epidatr")
remotes::install_github("cmu-delphi/epidatasets")
remotes::install_github("cmu-delphi/epiprocess@dev")
remotes::install_github("cmu-delphi/epipredict@dev")
remotes::install_github("dajmcdon/rtestim")
```

* Let's take a few moments here.
* You may also navigate to the GitHub repo and Clone/Fork the entire thing.

```{r qr-to-github}
#| dev: png
ggplot(qrdat("https://dajmcdon.github.io/canssip-epidata-2025"), aes(x, y, fill = z, alpha = z)) +
  geom_raster() +
  coord_equal(expand = FALSE) +
  scale_fill_manual(values = c("white", primary), guide = "none") +
  scale_alpha_manual(values = c(0, 1), guide = "none") + 
  theme_void() +
  theme(
    text = element_text(
      color = primary, size = 36,
      margin = margin(3,0,3,0))
  )
```


# Panel Data {.inverse}

## Panel data

* [Panel data]{.secondary} is cross-sectional measurements of subjects over time.

* With aggregated data, the subjects are geographic units (e.g. provinces, states). 

* Time index + one or more locations/keys.

```{r panel-ca-ex}
dv_versioned_panel_final |> filter(geo_value == "ca") |> select(-version)
```

[The % of outpatient doctor visits that are COVID-related in CA, between June 2020 to Dec. 2021]{.small .grey}

## Examples of panel data

[JHU CSSE COVID-19 cases per 100k]{.secondary}

```{r examples-panel-covid2}
#| echo: false
#| fig-width: 7
names <- c("COVID-19 cases", "CHNG-CLI", "CHNG-COVID", "COVID-19 hospital admissions")
units <- c("Reported cases per 100k people",
           "% doctor's visits due to CLI",
           "% doctor's visits due to COVID-19",
           "Hospital admissions per 100k people")

as_epi_df(panel_data[[1]]) %>%
  autoplot("value") +
  scale_color_delphi() +
  theme(legend.title = element_blank()) +
  xlab("Date") + ylab(units[1]) +
  geom_hline(yintercept = 0)
```

::: {.notes}

* WA switch to weekly reporting in 2022
* FL reports "whenever" (weekly, biweekly, three days in a row, then 4 zeros, etc.)
* API calculates change from cumulative, so no-report becomes a 0.
* If state decreases total, then we see a negative.

:::


## Examples of panel data

[Confirmed COVID-19 Hospital Admissions per 100k, 7day average]{.secondary}

```{r examples-hhs-admissions}
#| echo: false
#| fig-width: 7
as_epi_df(panel_data[[4]]) %>%
  autoplot("value") +
  scale_color_delphi() +
  theme(legend.title = element_blank()) +
  xlab("Date") + ylab(units[4]) +
  scale_y_continuous(expand = expansion(c(0, 0.05)))
```

::: {.fragment .box-text .absolute top=20% left=30%}

The $x$-axis is

[Date of report]{.secondary}
 
Not "date of event"

:::

## More disclaimers...

* Most of this workshop will focus on panel data

* Typical for the tasks my group has focused on

* Typically analyze [aggregate signals]{.secondary}

* [Simultaneously across geographies]{.secondary}

* Contrasts with "single geo models"

* Not working with "line list data"



# Versioned Data {.inverse}

## Intro to versioned data

::: {.fragment .fade-in-then-out .box-text .absolute top=20% left=10%}
::: {.secondary}
→ Person comes to ER  
→ Admitted  
→ Has some tests  
→ Tests come back  
→ Entered into the system  
→ ...
:::
:::

* Epidemic aggregates are subject to [reporting delays and revisions]{.secondary}

<br>

* A "Hospital admission" may not attributable to a particular condition
until a few days have passed

<br>

* Additionally, various mistakes lead to revisions

<br>

* Track both: when the event occurred and when it was reported

## Intro to versioned data


* Epidemic aggregates are subject to [reporting delays and revisions]{.secondary}

<br>

* A "Hospital admission" may not attributable to a particular condition
until a few days have passed

<br>

* Additionally, various mistakes lead to revisions

<br>

* Track both: [when the event occurred]{.fragment .hl-green} and 
[when it was reported]{.fragment .hl-green}




## Versioned data

* The event time is indicated by `time_value` (or `reference_date`)

* Second time index indicates the data `version` (or `reporting_date`)

`version` = the time at which we saw a `value` associated to a `time_value`

```{r versioned-ca-ex}
#| echo: false
dv_archive <- dv_versioned_panel |>
  as_epi_archive(compactify = TRUE)
dv_ca <- dv_archive$DT |> filter(geo_value == "ca")
head(dv_ca) |> as_tibble()
```


## Versioned panel data

Estimated percentage of outpatient visits due to CLI across multiple versions.


```{r versioned-panel-multi-states-ex-2}
#| fig-width: 7
dv_final <- dv_versioned_panel_final
max_version <- max(dv_archive$DT$version)
versions <- seq(as.Date("2020-06-01"), max_version - 1, by = "1 week")
weekly_snapshots <- map(versions, function(v) {
  epix_as_of(dv_archive, v) %>% mutate(version = v)
}) |> list_rbind()
weekly_snapshots |>
  filter(geo_value %in% c("ca", "fl")) |>
  ggplot(aes(x = time_value, y = percent_cli)) +
  facet_wrap(~geo_value) +
  geom_line(aes(color = version, group = factor(version))) +
  # geom_vline(aes(color = factor(version), xintercept = version), lty = 3) +
  geom_line(data = dv_final |> filter(geo_value %in% c("fl", "ca")), color = "black") +
  labs(x = "Reference date", y = "% doctor's visits with CLI") +
  expand_limits(y = 0) +
  scale_x_date(date_labels = "%m/%Y", expand = expansion(0)) +
  scale_y_continuous(expand = expansion(c(0, 0.05))) +
  theme_bw() +
  scale_color_viridis_c(trans = "date", labels = label_date(format = "%m/%Y"), name = "Version") 
```

## Latency and revision in signals

* [Latency]{.secondary} the delay between data collection and availability

::: {.callout-tip icon=false}
## Example
A signal based on insurance claims may take several days to appear as claims are processed
:::

<br>

. . .

* [Revision]{.secondary} data is updated or corrected after initial publication

::: {.callout-tip icon=false}
## Example
COVID-19 case reports are revised as reporting backlogs are cleared
:::


## Latency and revision in signals - Example

* Recall the first example of panel & versioned data we've seen... 

```{r latency-ca-june}
#| echo: false
dv_latency <- dv_ca |> 
  filter(month(time_value) == 6, year(time_value) == 2020) |> 
  mutate(latency = version - time_value) |>
  group_by(time_value) |>
  filter(version == min(version)) |>
  ungroup() |>
  as_tibble()

lat_ca_june <- median(dv_latency$latency)
```

* In June 2020, this signal is typically `r lat_ca_june` days [latent]{.secondary}

```{r latency-ca-ex}
#| echo: false
head(dv_latency, 5) 
```
. . .

and subject to [revision]{.secondary}

```{r revision-ca-ex}
#| echo: false
dv_ca |> 
  filter(time_value == "2020-06-01") |> 
  mutate(latency = version - time_value) |>
  as_tibble() |> 
  head(5)
```

## Revision triangle, Insurance Claims WA January 2022 

* 7-day trailing average to smooth day-of-week effects

```{r revision-triangle}
#| echo: false
#| dpi: 300
#| fig-format: png
#| fig-width: 7
dv_cli <- left_join(dv_wa_versioned, dv_wa_finalized, by = join_by(time_value)) |>
  mutate(value = zoo::rollmeanr(value, k = 7, na.pad = TRUE), 
         final_value = zoo::rollmeanr(final_value, k = 7, na.pad = TRUE),
         .by = issue)

p1 <- dv_cli |>
  filter(time_value > ymd("2021-12-31"), issue >= "2021-12-31") |>
  ggplot(aes(time_value, issue, fill = value / final_value * 100)) +
  geom_tile() +
  scale_x_date(
    limits = ymd(c("2022-01-01", "2022-01-31")),
    date_breaks = "10 days",
    date_labels = "%d",
    expand = expansion(), 
    name = "Reference date"
  ) +
  scale_y_date(
    limits = ymd(c("2022-01-01", "2022-01-31")),
    date_breaks = "10 days",
    date_labels = "%d",
    expand = expansion(), 
    name = "Report date",
    
  ) +
  scale_fill_viridis_c(
    name = "% final", 
    option = "B",
    direction = -1
  ) +
  theme_bw() +
  theme(legend.position = "bottom", legend.key.width = unit(1, "cm")) 

p2 <- dv_cli |>
  filter(issue > "2021-12-31", time_value > ymd("2021-12-31")) |>
  ggplot(aes(time_value)) +
  geom_line(aes(y = value, colour = issue, group = issue)) +
  scale_x_date(
    limits = ymd(c("2022-01-01", "2022-01-31")), 
    date_breaks = "10 days",
    date_labels = "%d",
    expand = expansion(), 
    name = "Reference date"
  ) +
  scale_y_continuous(
    expand = expansion(), 
    name = "% Outpatient visits w/ CLI",
  ) +
  scale_colour_viridis_c(
    name = "Report date", 
    direction = -1,
    trans = "date",
    labels = scales::label_date("%d"),
    option = "B"
  ) +
  geom_line(aes(y = final_value), color = "black") +
  theme_bw() +
  theme(legend.position = "bottom", legend.key.width = unit(1, "cm")) 

cowplot::plot_grid(p1, p2)
```

## Revisions
Many data sources are subject to revisions:

<br>

* Case and death counts are corrected or adjusted by authorities

* Medical claims can take weeks to be submitted and processed

* Surveys are not completed promptly

. . .

<br>

[An accurate revision log is crucial for researchers building nowcasts and forecasts]{.secondary}

<br>

::: {.fragment .callout-important}
## Obvious but crucial

A forecast that is made today can only use data available "as of" today
:::

## Three types of revisions


1. [Sources that don't revise]{.fourth-colour} (provisional and final are the same) 

Facebook Survey and Google symptoms

. . .

2. [Predictable revisions]{.secondary} 

Claims data and public health reports aligned by test, hospitalization, 
or death date

Almost always revised upward as additional claims enter the pipeline

. . .

3. [Revisions that are large and erratic to predict]{.tertiary} 

COVID cases and deaths

These are aligned by report date 


## Types of revisions - Comparison between 2. and 3.

* Revision behavior for two indicators in the HRR containing Charlotte, NC.


* [DV-CLI signal (left)]{.secondary}: regularly revised, but effects fade

* [JHU CSSE cases (right)]{.tertiary} remain "as first reported" until a major correction is made on Oct. 19


```{r fig1-McDonald}
#| echo: false
#| fig-width: 7
# This is Figure 1 from https://www.pnas.org/doi/pdf/10.1073/pnas.2111453118
p1 <- dv_311_as_of |>
  mutate(as_of = fct_relabel(factor(as_of), function(x) strftime(x, "%b %d"))) |>
  ggplot(aes(x = time_value, y = value)) + 
  geom_line(aes(color = factor(as_of))) + 
  labs(title = "DV-CLI", x = "", y = "% doctor's visits due to CLI",
       color = "As of:") +
  theme_bw() + 
  scale_color_delphi()

p2 <- cases_311_as_of |>
  mutate(as_of = fct_relabel(factor(as_of), function(x) strftime(x, "%b %d"))) |>
  ggplot(aes(x = time_value, y = value)) + 
  geom_line(aes(color = factor(as_of))) + 
  labs(title = "Cases", x = "", y = "Cases per 100,000 people",
       color = "As of:") +
  theme_bw() + 
  scale_color_delphi()

suppressWarnings(leg <- cowplot::get_legend(
  p1 + 
    theme(legend.margin = margin(0,12,0,12), legend.background = element_blank())
))

cowplot::plot_grid(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"),
  leg,
  nrow = 1,
  rel_widths = c(1, 1, .3)
)
```


## Reporting backlogs - Example

Bexar County, Texas, summer of 2020...

* Large backlog of case reports results in a spike
* Auxilliary signals show continued decline
* Reports are not be trustworthy without context


```{r fig4-Reinhart}
#| echo: false
#| message: false
#| fig-width: 7.5
#| fig-height: 3

# Similar to Figure 4 from https://www.pnas.org/doi/10.1073/pnas.2111452118

sa_anomaly_date <- as.Date("2020-07-16")
reinhart[-1] <- map(
  reinhart[-1], 
  ~ mutate(.x, value = scale(value))
)
reinhart <- list_rbind(reinhart)

g1 <- ggplot(reinhart |> filter(source == "jhu-csse"), aes(x = time_value, y = value)) +
  geom_vline(xintercept = sa_anomaly_date, 
             color = fifth_colour,
             alpha = 0.75) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b %Y") +
  geom_line(color = ubclblue) +
  labs(x = "", y = "Cases", title = "Cases") +
  theme_bw()

labels <- c(
  `fb-survey` = "Survey-based CLI",
  chng = "Outpatient CLI",
  `google-symptoms` = "Google searches",
  `doctor-visits` = "Insurance claims"
)


g2 <- reinhart |>
  filter(source != "jhu-csse") |>
  ggplot(aes(x = time_value, y = value, color = source)) +
  geom_vline(xintercept = sa_anomaly_date, color = fifth_colour,
             alpha = 0.75) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b %Y") +
  geom_line() +
  scale_color_delphi(labels = labels, name = "") +
  labs(x = "", y = "Signal value (rescaled)", title = "Auxiliary signals") +
  theme_bw()

cowplot::plot_grid(g1, g2, ncol = 2, rel_widths = c(.38, .62))
```


# Epidata Repository and API {.inverse}

## What is the Epidata repository

[Epidata:]{.secondary} repository of aggregated epi-surveillance time series

Signals can be either public or restricted.

* Currently contains over 5 billion records

* During pandemic, handled millions of API queries per day

* Many signals aren't available elsewhere


::: {.callout-important appearance="simple"}
Make epi-surveillance more nimble, complete, standardized, robust, and real-time
:::




## Features of Delphi Epidata

* Built-in support for:
    1. Data revisions ("backfill"), including reporting dates and changes
    1. Geo levels w/ auto-aggregation (e.g. county, state, and nation) and specialized levels (e.g., DMA, sewer sheds)
    1. Demographic breakdown
    1. Representation for missingness and censoring
    1. Population sizes and fine-grained population density
    
* Customized smoothing and normalization

* Access control

* Code is Open Source.  

* Signals are as accessible (w/ API, SDK) as allowed by DUAs


## Severity pyramid

![](gfx/severity-pyramid.svg){fig-align=center}


::: {.fragment .box-text .absolute top=40% left=20%}
<https://delphi.cmu.edu/epiportal/>
:::

# [{epidatr}]{.monotype} {.inverse}


## Installing `{epidatr}`

(you already did this, but just for posterity...)

Install the CRAN version

```{r install-epidatr-cran}
#| echo: true
#| eval: false
# Install the CRAN version
install.packages("epidatr")
```

<br>

or the development version

```{r install-epidatr-dev}
#| eval: false
#| echo: true
# Install the development version from the GitHub dev branch
remotes::install_github("cmu-delphi/epidatr@dev")
```

The CRAN listing is [here](https://cran.r-project.org/package=epidatr/index.html).

## Python

In Python, install [`delphi-epidata` from PyPI](https://pypi.org/project/delphi-epidata/) with 

``` sh
pip install delphi-epidata
```

<br>

`delphi-epidata` is soon to be replaced with `epidatpy`.

``` sh
# Latest dev version
pip install -e "git+https://github.com/cmu-delphi/epidatpy.git#egg=epidatpy"

# PyPI version (not yet available)
pip install epidatpy
```




## Using `{epidatr}` and `{epidatpy}`

```{r hhs-influenza-pub-covidcast}
#| echo: true
#| eval: false
library(epidatr)
hhs_flu_nc <- pub_covidcast(
  source = 'hhs', 
  signals = 'confirmed_admissions_influenza_1d', 
  geo_type = 'state', 
  time_type = 'day', 
  geo_values = 'nc',
  time_values = c(20240401, 20240405:20240414)
)
head(hhs_flu_nc, n = 3)
```

```{r hhs-influenza-pub-covidcast-2}
#| echo: false
head(hhs_flu_nc, n = 3)
```

<br>

Python equivalent:
``` python
res = Epidata.covidcast('hhs', 'confirmed_admissions_influenza_1d', 'day', 
  'state', [20240401, Epidata.range(20240405, 20240414)], 'nc')
```



## API keys

* [Anyone may access the Epidata API anonymously without providing any personal data!!]{.fragment .hl-claret}

* Anonymous API access is subject to some restrictions:
  <small>public datasets only; 60 requests per hour; only two parameters may have multiple selections</small>

* API key grants privileged access; can be obtained by [registering with us](https://api.delphi.cmu.edu/epidata/admin/registration_form) 

* Privileges of registration: no rate limit; no limit on multiple selections

* We just want to know which signals people care about to ensure we're providing benefit

<!-- rate limited to 60 requests per hour;  -->
::: {.callout-tip}
* The `{epidatr}` client automatically searches for the key in the `DELPHI_EPIDATA_KEY` environment variable. 
* We recommend storing it in your `.Renviron` file, which R reads by default. 
* More on setting your API key [here](https://rdrr.io/cran/epidatr/man/get_api_key.html).
:::




## Interactive tooling in R 


```{r avail-endpoints-fun}
#| echo: true
avail_endpoints()
```


## Fetching data - COVIDcast main endpoint 

```{r us-jhu-pub-covidcast}
#| echo: true
#| eval: false
jhu_us_cases <- pub_covidcast(
  source = "jhu-csse",                        # this endpoint contains many different sources
  signals = "confirmed_7dav_incidence_prop",  # other signals: deaths, cumulative, etc.
  geo_type = "nation",                        # the geographic resolution (nation, state, hrr, msa, etc.)
  time_type = "day",                          # or week or year
  geo_values = "us",                          # optional
  time_values = epirange(20210101, 20210401), # optional
  ...                                         # additional arguments
)
```

```{r head-us-jhu-pub-covidcast}
#| echo: false 
head(jhu_us_cases, n = 3) |> 
  select(geo_value, signal, source, geo_type, time_value, issue, lag, value)
```

`value` is the requested signal

[There are some other columns in the usual output that I've hidden]{.small .grey}


## Get everything for a `source + signal`

```{r us-jhu-pub-covidcast-2}
#| echo: true
#| eval: false
jhu_us_cases <- pub_covidcast(
  source = "jhu-csse",                  # this endpoint contains many different sources
  signals = "confirmed_incidence_num",  # raw cases during the entire pandemic reporting until ~ April 2024
  geo_type = "county",                  # the geographic resolution (nation, state, hrr, msa, etc.)
  time_type = "day",                    # lowest resolution
  geo_values = "*",                     # (default) 
  time_values = "*",                    # (default) 
  ...                                   # additional arguments
)
```

<br><br>

* This query takes a few minutes to run, so I don't recommend it. 

* But there is support for automatic caching,

* and using `"*"` speeds things up relative to specifying many specific ranges.

::: {.fragment .box-text .absolute top=30% left=10%}
The result has about 3.75M rows and occupies 400Mb.
:::


# Versioning in [{epidatr}]{.monotype} {.inverse}

## Versioned data in `{epidatr}`

<br>

Two important, mutually exclusive parameters

<br>

### `issues = c(mdy1, mdy2, ..., )` 
* fetches the data that the source made available on the requested dates
* Database stores only the diffs, so that's typically what you get
* Even if the source republishes the entire history every time they make an update


### `as_of = mdy`
* fetches the all available data as it would have looked on `mdy`
* Think of it as [winding back the clock]{.secondary} to the date `mdy`
* API only accepts a single date here

## Example `issues` query

* I wanted to display a major reporting error. 

```{r version-query-wrong}
#| echo: true
versions <- as.Date(c("2021-02-15", "2021-02-20", "2021-02-25", "2021-03-01", "2023-01-01")) 
pub_covidcast(
  "jhu-csse", "deaths_7dav_incidence_num", 
  geo_type = "state", 
  geo_values = "oh",
  time_type = "day",
  time_values = epirange(20210101,20210301),
  issues = versions
) |>
  select(geo_value, time_value, version = issue, deaths = value)
```

::: {.fragment .box-text .absolute top=20% left=10%}
* Not what I wanted.

* Got only the diff on each issue.

* I wanted to view the whole history on each of those dates.
:::

## Correct `as_of` query

```{r version-query-asof}
#| echo: true
#| code-line-numbers: "|1,2,9"
res <- map(versions, # same set as before
           .f = \(v) { 
             pub_covidcast(
               "jhu-csse", "deaths_7dav_incidence_num", 
               geo_type = "state", 
               geo_values = "oh",
               time_type = "day",
               time_values = epirange(20210101,20210301),
               as_of = v 
             ) |>
               select(geo_value, time_value, deaths = value) |>
               mutate(version = v)
           }) |>
  list_rbind()
res |> head(7)
```

::: {.fragment .box-text .absolute top=20% left=5%}
* Got the data [as it would have appeared]{.tertiary} for each of the 4 dates.
* But `as_of` can only accept a scalar, not vector of dates.
* Had to "loop" over them.
* We'll see a more efficient way to do this later this morning.
:::

## Now I can show you why I wanted that query

```{r oh-death-spike}
res |>
  ggplot(aes(time_value, deaths, color = factor(version))) +
  geom_vline(
    aes(xintercept = version, color = factor(version)), 
    show.legend = FALSE, alpha = .3) +
  geom_line() +
  scale_color_delphi() +
  xlab("Date") + ylab("Reported COVID-19 deaths in\nOhio 2021") +
  theme(legend.position = "right")
```



## Versioning in nowcasting and forecasting

* Revision patterns can be used to inform understanding of current situation

* Often, predicting "today" is more about predicting the revisions than the process

* Forecasting often requires adjustments for revision/reporting patterns

* Backtesting requires using data that would have been available at the time, not current data

* Only looking at the most recent data is a huge blunder


## Wrapup and worksheet discussion

* [Versioned Data and Latency]{.primary}
    1. `as_of`:  One version; the specific date when the data was last updated 
    1. `issues`: Multiple versions; with different `as_of` dates

* [Epidata API:]{.primary} delivers up-to-date, granular epidemiological data + historical versions.

* `{epidatr}`: Client package for R

* Versioning and panel structure are key first steps for analysis

    
```{r wksheet}
#| dev: png
wkshtqr
```


