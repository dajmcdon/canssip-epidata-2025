---
org: "CANSSI Prairies &mdash; Epi Forecasting Workshop 2025"
title: "Introduction to Panel Data in Epidemiology"
subtitle: "Lecture 1"
short-title: "Understanding Data"
format: revealjs
---


```{r setup}
#| cache: false
#| include: false
source(here::here("slides", "_code", "setup.R"))
library(tidymodels)
library(epidatasets)
library(epipredict)
library(epidatr)
load(here::here("_data", "day1m_queries.rda"))
```


## Outline

\DeclareMathOperator*{\minimize}{minimize}

1. About Me

1. Workshop Overview and System Setup

1. Panel Data

1. Versioned Data

1. Epidata Repository and API

1. `{epidatr}` and Other Data

1. Versioning in `{epidatr}`


# About Me {.inverse}

## Daniel J. McDonald



## About Delphi

* Founded in 2012 at Carnegie Mellon University, now expanded to UC Berkeley, and University of British Columbia.

* Currently 5 faculty, ~10 PhD students, ~15 staff (mostly software engineers).

* Easy to join us from anywhere (lots of volunteers during Covid-19 pandemic).

* We are:
    + CDC Center of Excellence for Influenza and Covid-19 Forecasting (2019-24).
    + CDC Innovation Center for Outbreak Analytics and Disease Modeling (2024-29).

[**Our mission:**]{.primary} To develop the theory and practice of [epidemic detection, tracking and forecasting]{.primary}, and their use in decision making, both public and private.

## What does Delphi do?

* Procure [real-time, aggregated data streams]{.primary} informative of infectious diseases and syndromes, in collaboration with partners in industry and government.

* Extract signals and make them widely available via the [Epidata platform & API]{.primary}.

* Develop and deploy algorithms for [epidemic detection, tracking, forecasting]{.primary}.

* Develop and maintain statistical software packages for these tasks.

* Make it all production-grade, maximally-accessible, and open-source (to serve CDC, state and local public health agencies, epi-forecasting researchers, data journalists, the public)

## What we provide

![](gfx/web_of_parts.svg){fig-align=center}

# Workshop Overview and System Setup {.inverse}

## What we will cover

- Characteristics of panel data in epidemiology
- Tools for processing and plotting panel data
- Statistical background on nowcasting and forecasting
- Tools for building nowcasting and forecasting models 
- Plenty of examples throughout of real case studies

## Goals part I

- Expose you to a statistical way of thinking about now/forecasting
- Certain basic mindsets (e.g., the importance of empirical validation
  using techniques like time series cross-validation) are ubiquitous
- Certain basic modeling considerations (e.g., starting simple and
  building up complexity, taming variance through regularization,
  addressing nonstationarity with trailing training windows) are also
  ubiquitous

## Goals part II

- Expose you to software packages which aid processing, tracking,
  nowcasting, and forecasting with panel data 
- These tools are still in development and we welcome your feedback
- We have tried hard to get the framework right; but many individual
  pieces themselves could still be improved 
- If these aren't working for you, then we want to hear from you!
- We welcome collaboration, and everything we do is open source

## A disclaimer

- My background is primarily in statistics and computer science
- This obviously influences my way of thinking and my approach to 
  nowcasting and forecasting
- I don't have nearly as much experience with traditional epi models,
  but I do have opinions about the pros/cons. 
- Ask me at any point if
  you have a question about why I'm doing things a certain way
  
## One last slide

- This workshop is supposed to be useful for YOU. Ask questions if
  you have them, don't be shy
- We may not (likely won't?) cover everything. Hopefully the materials
  will be a resource for you beyond this workshop
  

## System setup -- Passive viewing

<br>

### All of the slides are at 

<br>

[<https://cmu-delphi.github.io/insightnet-workshop-2024>]{style="text-align: center;"}

<br>

### The source code is in the Repo 

<br>

<https://github.com/cmu-delphi/insightnet-workshop-2024>

<br>

This is enough, but we hope you'll want to work through the code as we go along.

<br>

[Detailed versions of the next few slides are shown at the Repo Link above.]{.secondary}

## System setup -- Required software 

<br>

### We assume you already have

<br>

1. `R`

<br>

2. An IDE. We'll use RStudio, but you can use VSCode or Emacs or Whatnot

## System setup -- Downloading the materials

### Easy way: 

1. Click the [Big Green Button]{.fourth-colour} that says <kbd>< > Code â–¾</kbd>
2. Choose [Download Zip]{.primary}
3. Open the Zip directory and then Open `insightnet-workshop-2024.Rproj`

### More expert (local `git` user):

1. Click the [Big Green Button]{.fourth-colour} that says <kbd>< > Code â–¾</kbd>
2. Copy the URL.
3. Open RStudio, select File > New Project > Version Control. Paste there and proceed.

### Even more expert (wants `github` remote):

1. Click the Grey Button that says <kbd>â‘‚ Fork â–¾</kbd>
2. Proceed along the same lines as above.

## System setup -- Installing required packages

We will use a [lot]{.tertiary} of packages.

### We've tried to make it so you can get them all at once (with the right versions)

ðŸ¤ž We hope this works... ðŸ¤ž
[Note that you can "Copy to Clipboard"]{.fourth-colour}

<br>

In RStudio:

``` r
install.packages("pak") # good for installing from non-CRAN sources
pak::pkg_install("cmu-delphi/InsightNetFcast24", dependencies = TRUE)
InsightNetFcast24::verify_setup()
```

<br>

Hopefully, you see:
```{r}
#| message: true
cli::cli_alert_success("You should be good to go!")
```

Ask for help if you see something like:
```{r}
#| error: true
verify_setup <- function() {
  cli::cli_abort(c(
    "The following packages do not have the correct version:",
    "i" = "Installed: {.pkg epipredict 0.2.0}.",
    "i" = "Required: {.pkg epipredict == 0.1.5}."
  ))
}
verify_setup()
```

# Panel Data {.inverse}

## Panel data

* [Panel data]{.secondary} is cross-sectional measurements of subjects over time.

* With aggregated data, the subjects are geographic units (e.g. provinces, states). 

* Time index + one or more locations/keys.

```{r panel-ca-ex}
dv_versioned_panel_final |> filter(geo_value == "ca") |> select(-version)
```

[The % of outpatient doctor visits that are COVID-related in CA, between June 2020 to Dec. 2021]{.small .grey}

## Examples of panel data

[JHU CSSE COVID-19 cases per 100k]{.secondary}

```{r examples-panel-covid2}
#| echo: false
#| fig-width: 7
names <- c("COVID-19 cases", "CHNG-CLI", "CHNG-COVID", "COVID-19 hospital admissions")
units <- c("Reported cases per 100k people",
           "% doctor's visits due to CLI",
           "% doctor's visits due to COVID-19",
           "Hospital admissions per 100k people")

as_epi_df(panel_data[[1]]) %>%
  autoplot("value") +
  scale_color_delphi() +
  theme(legend.title = element_blank()) +
  xlab("Date") + ylab(units[1]) +
  scale_y_continuous(expand = expansion(c(0, 0.05)))
```

::: {.notes}

* WA switch to weekly reporting in 2022
* FL reports "whenever" (weekly, biweekly, three days in a row, then 4 zeros, etc.)
* API calculates change from cumulative, so no-report becomes a 0.
* If state decreases total, then we see a negative.

:::


## Examples of panel data

[Confirmed COVID-19 Hospital Admissions per 100k, 7day average]{.secondary}

```{r examples-hhs-admissions}
#| echo: false
#| fig-width: 7
as_epi_df(panel_data[[4]]) %>%
  autoplot("value") +
  scale_color_delphi() +
  theme(legend.title = element_blank()) +
  xlab("Date") + ylab(units[4]) +
  scale_y_continuous(expand = expansion(c(0, 0.05)))
```

::: {.fragment .box-text .absolute top=20% left=30%}

The $x$-axis is

[Date of report]{.secondary}
 
Not "date of event"

:::



# Versioned Data {.inverse}

## Intro to versioned data

::: {.fragment .fade-in-then-out .box-text .absolute top=20% left=10%}
::: {.secondary}
â†’ Person comes to ER  
â†’ Admitted  
â†’ Has some tests  
â†’ Tests come back  
â†’ Entered into the system  
â†’ ...
:::
:::

* Epidemic aggregates are subject to [reporting delays and revisions]{.secondary}

<br>

* A "Hospital admission" may not attributable to a particular condition
until a few days have passed

<br>

* Additionally, various mistakes lead to revisions

<br>

* Track both: when the event occurred and when it was reported

## Intro to versioned data


* Epidemic aggregates are subject to [reporting delays and revisions]{.secondary}

<br>

* A "Hospital admission" may not attributable to a particular condition
until a few days have passed

<br>

* Additionally, various mistakes lead to revisions

<br>

* Track both: [when the event occurred]{.fragment .hl-green} and 
[when it was reported]{.fragment .hl-green}




## Versioned data

* The event time is indicated by `time_value` (or `reference_date`)

* Second time index indicates the data `version` (or `reporting_date`)

`version` = the time at which we saw a `value` associated to a `time_value`

```{r versioned-ca-ex}
#| echo: false
dv_archive <- dv_versioned_panel |>
  as_epi_archive(compactify = TRUE)
dv_ca <- dv_archive$DT |> filter(geo_value == "ca")
head(dv_ca) |> as_tibble()
```


## Versioned panel data

Estimated percentage of outpatient visits due to CLI across multiple versions.


```{r versioned-panel-multi-states-ex-2}
#| fig-width: 7
dv_final <- dv_versioned_panel_final
max_version <- max(dv_archive$DT$version)
versions <- seq(as.Date("2020-06-01"), max_version - 1, by = "1 week")
weekly_snapshots <- map(versions, function(v) {
  epix_as_of(dv_archive, v) %>% mutate(version = v)
}) |> list_rbind()
weekly_snapshots |>
  filter(geo_value %in% c("ca", "fl")) |>
  ggplot(aes(x = time_value, y = percent_cli)) +
  facet_wrap(~geo_value) +
  geom_line(aes(color = version, group = factor(version))) +
  # geom_vline(aes(color = factor(version), xintercept = version), lty = 3) +
  geom_line(data = dv_final |> filter(geo_value %in% c("fl", "ca")), color = "black") +
  labs(x = "Reference date", y = "% doctor's visits with CLI") +
  expand_limits(y = 0) +
  scale_x_date(date_labels = "%m/%Y", expand = expansion(0)) +
  scale_y_continuous(expand = expansion(c(0, 0.05))) +
  theme_bw() +
  scale_color_viridis_c(trans = "date", labels = label_date(format = "%m/%Y"), name = "Version") 
```

## Latency and revision in signals

* [Latency]{.secondary} the delay between data collection and availability

::: {.callout-tip icon=false}
## Example
A signal based on insurance claims may take several days to appear as claims are processed
:::

<br>

. . .

* [Revision]{.secondary} data is updated or corrected after initial publication

::: {.callout-tip icon=false}
## Example
COVID-19 case reports are revised as reporting backlogs are cleared
:::


## Latency and revision in signals - Example

* Recall the first example of panel & versioned data we've seen... 

```{r latency-ca-june}
#| echo: false
dv_latency <- dv_ca |> 
  filter(month(time_value) == 6, year(time_value) == 2020) |> 
  mutate(latency = version - time_value) |>
  group_by(time_value) |>
  filter(version == min(version)) |>
  ungroup() |>
  as_tibble()

lat_ca_june <- median(dv_latency$latency)
```

* In June 2020, this signal is typically `r lat_ca_june` days [latent]{.secondary}

```{r latency-ca-ex}
#| echo: false
head(dv_latency, 5) 
```
. . .

and subject to [revision]{.secondary}

```{r revision-ca-ex}
#| echo: false
dv_ca |> 
  filter(time_value == "2020-06-01") |> 
  mutate(latency = version - time_value) |>
  as_tibble() |> 
  head(5)
```

## Revision triangle, Insurance Claims WA January 2022 

* 7-day trailing average to smooth day-of-week effects

```{r revision-triangle}
#| echo: false
#| dpi: 300
#| fig-format: png
#| fig-width: 7
dv_cli <- left_join(dv_wa_versioned, dv_wa_finalized, by = join_by(time_value)) |>
  mutate(value = zoo::rollmeanr(value, k = 7, na.pad = TRUE), 
         final_value = zoo::rollmeanr(final_value, k = 7, na.pad = TRUE),
         .by = issue)

p1 <- dv_cli |>
  filter(time_value > ymd("2021-12-31"), issue >= "2021-12-31") |>
  ggplot(aes(time_value, issue, fill = value / final_value * 100)) +
  geom_tile() +
  scale_x_date(
    limits = ymd(c("2022-01-01", "2022-01-31")),
    date_breaks = "10 days",
    date_labels = "%d",
    expand = expansion(), 
    name = "Reference date"
  ) +
  scale_y_date(
    limits = ymd(c("2022-01-01", "2022-01-31")),
    date_breaks = "10 days",
    date_labels = "%d",
    expand = expansion(), 
    name = "Report date",
    
  ) +
  scale_fill_viridis_c(
    name = "% final", 
    option = "B",
    direction = -1
  ) +
  theme_bw() +
  theme(legend.position = "bottom", legend.key.width = unit(1, "cm")) 

p2 <- dv_cli |>
  filter(issue > "2021-12-31", time_value > ymd("2021-12-31")) |>
  ggplot(aes(time_value)) +
  geom_line(aes(y = value, colour = issue, group = issue)) +
  scale_x_date(
    limits = ymd(c("2022-01-01", "2022-01-31")), 
    date_breaks = "10 days",
    date_labels = "%d",
    expand = expansion(), 
    name = "Reference date"
  ) +
  scale_y_continuous(
    expand = expansion(), 
    name = "% Outpatient visits w/ CLI",
  ) +
  scale_colour_viridis_c(
    name = "Report date", 
    direction = -1,
    trans = "date",
    labels = scales::label_date("%d"),
    option = "B"
  ) +
  geom_line(aes(y = final_value), color = "black") +
  theme_bw() +
  theme(legend.position = "bottom", legend.key.width = unit(1, "cm")) 

cowplot::plot_grid(p1, p2)
```

## Revisions
Many data sources are subject to revisions:

<br>

* Case and death counts are corrected or adjusted by authorities

* Medical claims can take weeks to be submitted and processed

* Surveys are not completed promptly

. . .

<br>

[An accurate revision log is crucial for researchers building forecasts]{.secondary}

<br>

::: {.fragment .callout-important}
## Obvious but crucial

A forecast that is made today can only use data available "as of" today
:::

## Three types of revisions


1. [Sources that don't revise]{.fourth-colour} (provisional and final are the same) 

Facebook Survey and Google symptoms

. . .

2. [Predictable revisions]{.secondary} 

Claims data and public health reports aligned by test, hospitalization, 
or death date

Almost always revised upward as additional claims enter the pipeline

. . .

3. [Revisions that are large and erratic to predict]{.tertiary} 

COVID cases and deaths

These are aligned by report date 


## Types of revisions - Comparison between 2. and 3.

* Revision behavior for two indicators in the HRR containing Charlotte, NC.


* [DV-CLI signal (left)]{.secondary}: regularly revised, but effects fade

* [JHU CSSE cases (right)]{.tertiary} remain "as first reported" until a major correction is made on Oct. 19


```{r fig1-McDonald}
#| echo: false
#| fig-width: 7
# This is Figure 1 from https://www.pnas.org/doi/pdf/10.1073/pnas.2111453118
p1 <- dv_311_as_of |>
  mutate(as_of = fct_relabel(factor(as_of), function(x) strftime(x, "%b %d"))) |>
  ggplot(aes(x = time_value, y = value)) + 
  geom_line(aes(color = factor(as_of))) + 
  labs(title = "DV-CLI", x = "", y = "% doctor's visits due to CLI",
       color = "As of:") +
  theme_bw() + 
  scale_color_viridis_d(end = .9, begin = .1)

p2 <- cases_311_as_of |>
  mutate(as_of = fct_relabel(factor(as_of), function(x) strftime(x, "%b %d"))) |>
  ggplot(aes(x = time_value, y = value)) + 
  geom_line(aes(color = factor(as_of))) + 
  labs(title = "Cases", x = "", y = "Cases per 100,000 people",
       color = "As of:") +
  theme_bw() + 
  scale_color_viridis_d(end = .9, begin = .1)

suppressWarnings(leg <- cowplot::get_legend(
  p1 + 
    theme(legend.margin = margin(0,12,0,12), legend.background = element_blank())
))

cowplot::plot_grid(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"),
  leg,
  nrow = 1,
  rel_widths = c(1, 1, .3)
)
```


## Reporting backlogs - Example

Bexar County, Texas, summer of 2020...

* Large backlog of case reports results in a spike
* Auxilliary signals show continued decline
* Reports are not be trustworthy without context


```{r fig4-Reinhart}
#| echo: false
#| message: false
#| fig-width: 7.5
#| fig-height: 3

# Similar to Figure 4 from https://www.pnas.org/doi/10.1073/pnas.2111452118

sa_anomaly_date <- as.Date("2020-07-16")
reinhart[-1] <- map(
  reinhart[-1], 
  ~ mutate(.x, value = scale(value))
)
reinhart <- list_rbind(reinhart)

g1 <- ggplot(reinhart |> filter(source == "jhu-csse"), aes(x = time_value, y = value)) +
  geom_vline(xintercept = sa_anomaly_date, color = "gray", linetype = "dashed",
             alpha = 0.75) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b %Y") +
  geom_line() +
  labs(x = "", y = "Cases", title = "Cases") +
  theme_bw()

labels <- c(
  `fb-survey` = "Survey-based CLI",
  chng = "Outpatient CLI",
  `google-symptoms` = "Google searches",
  `doctor-visits` = "Insurance claims"
)


g2 <- reinhart |>
  filter(source != "jhu-csse") |>
  ggplot(aes(x = time_value, y = value, color = source)) +
  geom_vline(xintercept = sa_anomaly_date, color = "gray", linetype = "dashed",
             alpha = 0.75) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b %Y") +
  geom_line() +
  scale_color_delphi(labels = labels, name = "") +
  labs(x = "", y = "Signal value (rescaled)", title = "Auxiliary signals") +
  theme_bw()

cowplot::plot_grid(g1, g2, ncol = 2, rel_widths = c(.38, .62))
```


# Epidata Repository and API {.inverse}

## What is the Epidata repository

[Epidata:]{.secondary} repository of aggregated epi-surveillance time series

Signals can be either public or restricted.

* Currently contains over 5 billion records

* During pandemic, handled millions of API queries per day

* Many signals aren't available elsewhere


::: {.callout-important appearance="simple"}
Make epi-surveillance more nimble, complete, standardized, robust, and real-time
:::




## Features of Delphi Epidata

* Built-in support for:
    1. Data revisions ("backfill"), including reporting dates and changes
    1. Geo levels w/ auto-aggregation (e.g. county, state, and nation) and specialized levels (e.g., DMA, sewer sheds)
    1. Demographic breakdown
    1. Representation for missingness and censoring
    1. Population sizes and fine-grained population density
    
* Customized smoothing and normalization

* Access control

* Code is Open Source.  

* Signals are as accessible (w/ API, SDK) as allowed by DUAs


## Severity pyramid

![](gfx/severity-pyramid.svg){fig-align=center}


::: {.fragment .box-text .absolute top=40% left=20%}
<https://delphi.cmu.edu/epiportal/>
:::

# [{epidatr}]{.monotype} {.inverse}


## Installing `{epidatr}`

(you already did this, but just for posterity...)

Install the CRAN version

```{r install-epidatr-cran}
#| echo: true
#| eval: false
# Install the CRAN version
install.packages("epidatr")
```

<br>

or the development version

```{r install-epidatr-dev}
#| eval: false
#| echo: true
# Install the development version from the GitHub dev branch
remotes::install_github("cmu-delphi/epidatr@dev")
```

The CRAN listing is [here](https://cran.r-project.org/package=epidatr/index.html).

## Python

In Python, install [`delphi-epidata` from PyPI](https://pypi.org/project/delphi-epidata/) with 

``` sh
pip install delphi-epidata
```

<br>

`delphi-epidata` is soon to be replaced with `epidatpy`.

``` sh
# Latest dev version
pip install -e "git+https://github.com/cmu-delphi/epidatpy.git#egg=epidatpy"

# PyPI version (not yet available)
pip install epidatpy
```




## Using `{epidatr}` and `{epidatpy}`

```{r hhs-influenza-pub-covidcast}
#| echo: true
#| eval: false
library(epidatr)
hhs_flu_nc <- pub_covidcast(
  source = 'hhs', 
  signals = 'confirmed_admissions_influenza_1d', 
  geo_type = 'state', 
  time_type = 'day', 
  geo_values = 'nc',
  time_values = c(20240401, 20240405:20240414)
)
head(hhs_flu_nc, n = 3)
```

```{r hhs-influenza-pub-covidcast-2}
#| echo: false
head(hhs_flu_nc, n = 3)
```

<br>

Python equivalent:
``` python
res = Epidata.covidcast('hhs', 'confirmed_admissions_influenza_1d', 'day', 
  'state', [20240401, Epidata.range(20240405, 20240414)], 'nc')
```



## API keys

* [Anyone may access the Epidata API anonymously without providing any personal data!!]{.fragment .hl-claret}

* Anonymous API access is subject to some restrictions:
  <small>public datasets only; 60 requests per hour; only two parameters may have multiple selections</small>

* API key grants privileged access; can be obtained by [registering with us](https://api.delphi.cmu.edu/epidata/admin/registration_form) 

* Privileges of registration: no rate limit; no limit on multiple selections

* We just want to know which signals people care about to ensure we're providing benefit

<!-- rate limited to 60 requests per hour;  -->
::: {.callout-tip}
* The `{epidatr}` client automatically searches for the key in the `DELPHI_EPIDATA_KEY` environment variable. 
* We recommend storing it in your `.Renviron` file, which R reads by default. 
* More on setting your API key [here](https://rdrr.io/cran/epidatr/man/get_api_key.html).
:::




## Interactive tooling in R 


```{r avail-endpoints-fun}
#| echo: true
avail_endpoints()
```


## Fetching data - COVIDcast main endpoint 

```{r us-jhu-pub-covidcast}
#| echo: true
#| eval: false
jhu_us_cases <- pub_covidcast(
  source = "jhu-csse",                        # this endpoint contains many different sources
  signals = "confirmed_7dav_incidence_prop",  # other signals: deaths, cumulative, etc.
  geo_type = "nation",                        # the geographic resolution (nation, state, hrr, msa, etc.)
  time_type = "day",                          # or week or year
  geo_values = "us",                          # optional
  time_values = epirange(20210101, 20210401), # optional
  ...                                         # additional arguments
)
```

```{r head-us-jhu-pub-covidcast}
#| echo: false 
head(jhu_us_cases, n = 3) |> 
  select(geo_value, signal, source, geo_type, time_value, issue, lag, value)
```

`value` is the requested signal

[There are some other columns in the usual output that I've hidden]{.small .grey}



# Versioning in [{epidatr}]{.monotype} {.inverse}

## Versioned data in `{epidatr}`

Epidata API contains each signal's estimate, location, date, and update timeline

Requesting Specific Data Versions:

* Use `as_of` or `issues` to specify data availability
* `as_of` always fetches one version
* `issues` can fetch multiple
* Only one may be used at a time
* Not all endpoints support both


## Obtaining data "as of" a specific date

[Doctor Visits](https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/doctor-visits.html) (from the `covidcast` endpoint)

* The percentage of outpatient visits w/ Covid-like illness
* Pennsylvania on May 1, 2020:

```{r pa-may-1st-as-of}
#| echo: true
#| eval: false
dv_pa_as_of <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  time_values = "2020-05-01",
  geo_type = "state",
  geo_values = "pa",
  as_of = "2020-05-07"
)
```

```{r head-pa-may-1st-as-of}
#| echo: false
head(dv_pa_as_of) |> 
  select(geo_value, signal, source, time_value, issue, lag, value)
```

* Initial estimate *issued* on May 7, 2020 
* Due to delay from reporting and ingestion by the API

## Obtaining data "as of" a specific date

[Default behaviour:]{.primary} unspecified `as_of`, get the most recent data

```{r pa-may-1st-most-recent}
#| echo: true
#| eval: false
dv_pa_final <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  time_values = "2020-05-01",
  geo_type = "state",
  geo_values = "pa"
)
```

```{r head-pa-may-1st-most-recent}
#| echo: false
head(dv_pa_final) |> 
  select(geo_value, signal, source, time_value, issue, lag, value, stderr)
```

[**Estimate changed substantially**]{.primary}:

* Increased to ~6% from <3%
    
## Versioning is important for forecasting

<br>

* Backtesting requires using data that would have been available at the time

<br>

* Not later updates

<br>

* Overly optimistic


## Obtaining multiple specific issues for one state
Request all issues in a certain time period

```{r pa-multiple-issues}
#| echo: true
#| eval: false
#| code-line-numbers: "1,8,9"
dv_pa_issues <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  time_values = "2020-05-01",
  geo_type = "state",
  geo_values = "pa",
  issues = epirange("2020-05-01", "2020-05-15")
)
```

```{r head-pa-multiple-issues}
#| echo: false 
head(dv_pa_issues) |> 
  select(geo_value, signal, source, time_value, issue, lag, value, stderr)
```

## Obtaining multiple issues for one state

To get all issues up to a specific date, set an extreme lower bound

```{r extreme-lb-multiple-issues}
#| echo: true
#| eval: false
#| code-line-numbers: "1,8,9"
dv_pa_issues_sub <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  time_values = "2020-05-01",
  geo_type = "state",
  geo_values = "pa",
  issues = epirange("1900-01-01", "2020-05-15")
)
```

```{r head-extreme-lb-multiple-issues}
#| echo: false
head(dv_pa_issues_sub) |> 
  select(geo_value, signal, source, time_value, issue, lag, value, stderr)
```

No change here â€¢ Can matter if the latency or reporting lag is unknown

[API docs](https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/doctor-visits.html)
show the earliest date available.

## Obtaining multiple issues for one state

At some point, nothing changes â€¢ It is [finalized]{.primary} â€¢ That will be the "last" issue

```{r extreme-lb-ub-multiple-issues}
#| echo: true
#| eval: false
#| code-line-numbers: "1,8,9"
dv_pa_issues_all <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  time_values = "2020-05-01",
  geo_type = "state",
  geo_values = "pa",
  issues = epirange("1900-01-01", "2024-12-11") # From the 1900s to today
)
```

```{r head-extreme-lb-ub-multiple-issues}
#| echo: false
tail(dv_pa_issues_all) |> 
  select(geo_value, signal, source, time_value, issue, lag, value, stderr)
```

* Avoid queries with too-late minimum too-early maximum issue
* Could be misleading results

<!-- * We caution against starting queries with too late a minimum issue or too early maximum issue, as it could lead to incorrect or misleading results. You're safest bet to capture all issues is on the next slide... -->

## Obtaining all issues for one state


```{r pa-all-issues}
#| echo: true
#| eval: false
#| code-line-numbers: "1,5,8,9"
dv_pa_issues_star <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  time_values = epirange("2020-05-01", "2020-05-07"),
  geo_type = "state",
  geo_values = "pa",
  issues = "*"
)
```

```{r head-pa-all-issues}
#| echo: false
head(dv_pa_issues_star, n = 8) |> 
  select(geo_value, signal, source, time_value, issue, lag, value, stderr)
```

## Obtaining all issues for all states

Using `*` gives all available

```{r all-the-states-and-issues}
#| echo: true
#| eval: false
#| code-line-numbers: "1,7-9"
dv_state_issues_star <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  time_values = epirange("2020-05-01", "2020-05-07"),
  geo_type = "state",
  geo_values = "*",
  issues = "*"
)
```

```{r head-all-the-states-and-issues}
#| echo: false
head(dv_state_issues_star) |> 
  select(geo_value, signal, source, time_value, issue, lag, value, stderr)
```


<!--

## Last but not least... The do nothing approach 

![](gfx/do-nothing.jpg){height="550px"}



## The do nothing approach 

* `geo_values = *` is the default
* `issues` defaults to "most recent"


```{r do-nothing}
#| echo: true
#| eval: false
dv_state_default <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  time_values = epirange("2020-05-01", "2020-05-07"),
  geo_type = "state"
)
```

-->

## Obtaining one issue for all states

<!--
[**Final question:**]{.primary}  What do you think happens when we adopt a "do nothing" approach to `geo_values` and `issues` (take both of them out)?

<small> [**Hint**]{.primary}: remember a couple slides ago, when we removed `as_of`, we got the most recent estimate for PA. </small>
-->

Defaults are intended to be "what you would expect"

```{r one-issue-all-states}
#| echo: true
#| eval: false
dv_state_default <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  time_values = epirange("2020-05-01", "2020-05-07"),
  geo_type = "state"
)
```

```{r head-one-issue-all-states}
#| echo: false
head(dv_state_default) |> 
  select(geo_value, signal, source, time_value, issue, lag, value, stderr)
```

<hr>

* most recent issue
* all states


<!-- ## Observations issued with a specific lag
<div style="font-size: 0.8em;">
* We can use the `lag` argument to request only data reported with a certain lag. 

* **Example**: Request  a lag of 7 days fetches only data issued exactly 7 days after the corresponding `time_value`:

```{r specific lag}
#| echo: false
#| eval: false
epidata <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  time_values = epirange("2020-05-01", "2020-05-07"),
  geo_type = "state",
  geo_values = "pa",
  lag = 7
)
```

```{r}
#| echo: false
#| eval: false
head(epidata) |> select(geo_value, signal, source, time_value, issue, lag, value, stderr)
```
</div>

## Query results exclusion
<div style="font-size: 0.8em;">
* Although the query we ran on the previous slide requested values from May 1 to May 7, May 3 and May 4 were not included due to a 7-day lag.

* Results for those dates appear only if updates are issued on the corresponding lag day (e.g., May 10).

```{r query results exclusion}
#| echo: false
#| eval: false
epidata <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  time_values = epirange("2020-05-03", "2020-05-03"),
  geo_type = "state",
  geo_values = "pa",
  issues = epirange("2020-05-09", "2020-05-15")
)
```

```{r}
#| echo: false
#| eval: false
head(epidata) |> select(geo_value, signal, source, time_value, issue, lag, value, stderr)
```
</div>
-->

## Main takeaways

* [**Delphi Epidata:**]{.primary} platform for real-time epidemic data
    * provides (aggregated) signals for tracking and forecasting
    * sources like [**health records**]{.secondary}, [**mobility patterns**]{.tertiary}, and [**more**]{.fourth-colour}.

<!-- * [**Delphi Epidata:**]{.primary} A one-stop platform for real-time epidemic data, providing aggregated signals for disease tracking and forecasting from diverse sources like health records, mobility patterns, and more. -->

* [**Epidata API:**]{.primary} delivers up-to-date, granular epidemiological data + historical versions.

<!-- Open-access API delivering up-to-date, granular epidemiological data + makes all historical versions available. -->

* `{epidatr}`: Client package for R

<!-- and interactive tools for discovering and analyzing health signals. -->

* [**Versioned Data and Latency:**]{.primary}
    1. `as_of`:  One version; the specific date when the data was last updated 
    1. `issues`: Multiple versions; with different `as_of` dates
    
Manages the record of revisions for transparency and accuracy in data analysis.

<!-- Panel data captures time-series trends, which are often subject to revision.  A standout feature of this API is its inclusion of two critical fields... -->

<!-- 1. as_of:  One version of the data, and referring to the specific date when the data was last updated (i.e. the data was updated `as_of` this date) -->

<!-- 2. `issues`: Multiple versions of the data, each corresponding to different `as_of` dates, capturing revisions over time. --> 
