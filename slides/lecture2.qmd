---
org: "CANSSI Prairies &mdash; Epi Forecasting Workshop 2025"
title: "Data Cleaning, Versioning, and Nowcasting"
subtitle: "Lecture 2"
short-title: "Nowcasting"
format: revealjs
---

```{r setup}
#| cache: false
source(here::here("slides", "_code", "setup.R"))
library(epidatr)
library(epipredict)
library(epidatasets)
```


## Outline

1. Warmup: Examining Snapshots
1. Signal processing with snapshots
1. Tracking Revisions
1. Nowcasting Using `{epiprocess}`
1. Nowcasting with Regression



## Now that you have data, what do you do with it? {.nostretch}


![R4DS by Wickham, Ã‡etinkaya-Rundel, and Grolemund](https://r4ds.hadley.nz/diagrams/data-science/base.png){width=600}

::: {.fragment}
### Complications

* Usually panel data (multiple locations at once)

* Usually accessing in real time

* Data have revisions

* Data are reported irregularly, `NA`'s are frequent

* Individual streams have high signal-to-noise ratio
:::

::: {.fragment .box-text .absolute top=50%}
[Result:]{.secondary} spend lots of time doing processing and dealing with corner behaviour
:::

```{r download-api-data}
#| eval: false
#| include: false
cases_df_api <- pub_covidcast(
  source = "jhu-csse",
  signals = "confirmed_incidence_num",
  geo_type = "state",
  time_type = "day",
  geo_values = "ca,nc,ny",
  time_values = epirange(20220301, 20220331),
  as_of = as.Date("2024-01-01")
)
write_rds(cases_df_api, here::here("_data", "cases_df_api.rds"))

bc_linelist <- CanCovidData::get_british_columbia_case_data()
write_rds(bc_linelist, here::here("_data", "bc_linelist.rds"))
can_deaths <- read_csv("https://raw.githubusercontent.com/ccodwg/CovidTimelineCanada/refs/heads/main/data/hr/deaths_hr.csv")
can_cases <- read_csv("https://raw.githubusercontent.com/ccodwg/CovidTimelineCanada/refs/heads/main/data/hr/cases_hr.csv")
hr_crosswalk <- read_csv("https://raw.githubusercontent.com/ccodwg/CovidTimelineCanada/refs/heads/main/geo/hr.csv") |>
  select(region, hruid, name_short, pop)
can_cases_deaths <- full_join(
  can_cases |> select(region, sub_region_1, time_value = date, cases = value_daily),
  can_deaths |> select(region, sub_region_1, time_value = date, deaths = value_daily)
) |> left_join(hr_crosswalk |> select(hruid, name_short), by = join_by(sub_region_1 == hruid)) |>
  select(region, hr = name_short, time_value, cases, deaths) |>
  replace_na(list(hr = "Other", deaths = 0))
write_rds(can_cases_deaths, here::here("_data", "can_cases_deaths.rds"))
write_rds(hr_crosswalk, here::here("_data", "can_hr_crosswalk.rds"))
```

```{r load-data}
#| echo: false
# bc_linelist <- read_rds(here::here("_data", "bc_linelist.rds"))
cases_df_api <- read_rds(here::here("_data", "cases_df_api.rds"))
can_cases_deaths <- read_rds(here::here("_data", "can_cases_deaths.rds"))
hr_crosswalk <- read_rds(here::here("_data", "can_hr_crosswalk.rds"))
nchs_archive <- read_rds(here::here("_data", "nchs_archive_dt.rds")) |> as_epi_archive()
hhs_hosp_archive <- read_rds(here::here("_data", "hhs_hosp_archive_dt.rds")) |> as_epi_archive()
corrections_df <- read_rds(here::here("_data", "corrections_df.rds"))
prov_pop <- summarise(hr_crosswalk, pop = sum(pop), .by = region)
cases_df <- cases_df_api |> 
  select(time_value, geo_value, raw_cases = value)

case_rates_df <- cases_df |>
  left_join(state_census |> select(abbr, pop), join_by(geo_value == abbr)) |>
  mutate(scaled_cases = raw_cases / pop * 1e5, pop = NULL)
```

## R packages we maintain to facilitate typical analyses


![](gfx/epiverse_packages_flow.jpg)

# Warmup: Examining Snapshots {.inverse}
  

## `epi_df`: snapshot of a data set

* a tibble with a couple of required columns, `geo_value` and `time_value`.
* arbitrary additional columns containing measured values, called [signals]{.secondary}
* additional [keys]{.secondary} that index subsets (health region, `age_group`, `ethnicity`, etc.)

::: {.callout-note}
## `epi_df`

Represents a [snapshot]{.secondary} that
contains the most [up-to-date values]{.secondary} of the signal variables, 
[as of]{.secondary} a given time.
:::

<!--

## Example, linelist to `epi_df`

```{r bc-linelist}
#| eval: false
bc_linelist |> head(3)
```

```{r bc-linelist-edf}
#| echo: true
#| eval: false
bc_linelist |>
  rename(time_value = `Reported Date`, geo_value = `Health Authority`) |>
  count(time_value, geo_value, name = "cases") |>
  as_epi_df()
```

-->
  

## `epi_df`: Snapshot of a dataset

```{r extra-junk}
#| include: false
#| eval: false
base_url <- "https://raw.githubusercontent.com/dajmcdon/rvdss-canada/main/data/"
one_season <- "season_2024_2025/positive_tests.csv"
positive_tests <- read_csv(paste0(base_url, one_season), col_types = cols_only(
  geo_value = col_character(), time_value = col_date(), issue = col_date(), flu_pct_positive = col_double()
)) |> filter(
  geo_value %in% c("atlantic", "bc", "on", "qc", "prairies", "territories")
)
earch1 <- as_epi_archive(positive_tests)
edf1 <- earch1 |> epix_as_of(version = max(positive_tests$issue))
```

```{r can-edf}
#| echo: true
#| code-line-numbers: 1-3
can_edf <- can_cases_deaths |>
  rename(geo_value = region) |>
  as_epi_df(as_of = "2024-04-13", other_keys = "hr")
can_edf
```



## Warm up: plotting

```{r plot-edf}
#| echo: true
#| fig-width: 7
#| code-line-numbers: 1-3
can_edf |>
  filter(geo_value == "MB") |>
  autoplot(cases, deaths) +
  scale_y_continuous(name = "", expand = expansion(c(0, .05))) + xlab("") + scale_color_delphi(name = "")
```

::: {.fragment .box-text .absolute top=20% left=20%}
Weird reporting behaviour.

* MB stopped reporting deaths by HR.
* Put them all in "Other"
* Lots of missing values
:::

## Warm up: handling missingness

Two types of missing data

1. Explicit missingness means that there's an `NA`
2. Implicit missingness means that a combination of `time_value` and `geo_value` is not in the data.

```{r fix-missingness}
#| code-line-numbers: 2
#| echo: true
can_edf <- can_edf |>
  complete(time_value = full_seq(time_value, period = 1), fill = list(cases = 0, deaths = 0)) 
can_edf
```

## Warm up: aggregating

```{r can-agg}
#| echo: true
#| code-line-numbers: 2
can_prov <- can_edf |>
  sum_groups_epi_df(c(cases, deaths), group_cols = "geo_value")
can_prov
```



## Warm up: per capita scaling

```{r edf-rates}
#| echo: true
#| code-line-numbers: 2|3
can_prov <- can_prov |>
  inner_join(prov_pop, by = join_by(geo_value == region)) |>
  mutate(case_rate = cases / pop * 1e5, death_rate = deaths / pop * 1e6) |>
  select(-pop)
```

```{r plot-can-prov}
#| fig-width: 7
can_prov |>
  autoplot(case_rate, death_rate) +
  scale_y_continuous(name = "", expand = expansion(c(0, .05))) +
  xlab("") +
  guides(colour = guide_legend(ncol = 2))
```

::: {.fragment .absolute .box-text top=5% left=5%}
* Negative incidence is often due to cummulatives being differenced
* But sometimes due to correcting an error. 
* With luck, the source would make an adjustment.

```{r negative-deaths}
can_prov |> select(geo_value:deaths) |> filter(deaths < 0)
```

:::

# Signal Processing with Snapshots {.inverse}

## Examples of signal processing

<br><br>

1. Correlating signals across location or time 
1. Computing growth rates
1. Detecting and removing outliers
1. Calculating summaries with rolling windows


## Correlations at different lags (province-level)

```{r corr-lags-ex}
#| echo: true
cor0 <- epi_cor(can_prov, case_rate, death_rate, cor_by = geo_value)
cor21 <- epi_cor(can_prov, case_rate, death_rate, cor_by = geo_value, dt1 = -21)
```

```{r plot-corr-lags-ex}
#| fig-width: 7
bind_rows(`0` = cor0, `21` = cor21, .id = "lag") |>
  mutate(lag = fct_relabel(factor(lag), ~ paste(.x, "days"))) |>
  ggplot(aes(lag, cor, fill = geo_value)) +
  coord_flip() +
  geom_dotplot(
    binaxis = "y", stackdir = "center", binwidth = 1 / 20, 
    stackgroups = TRUE, method = "histodot") +
  labs(y = "Correlation", x = "Lag") +
  scale_fill_viridis_d(option = "B", guide = guide_legend(ncol = 2)) +
  theme(legend.position = "right")
```

::: {.fragment .box-text .absolute top=20% left=5%}
* Are case and death rates linearly associated across all days for each geography?

* Deaths and cases likely not contemporaneous, expect cases to precede deaths.
:::

## Lag analysis more systematically


```{r can-cors}
#| echo: true
#| code-line-numbers: 1|2|5
lags <- 0:35
can_lag_cors <- map(lags, \(l) epi_cor(can_prov, case_rate, death_rate, cor_by = geo_value, dt1 = -l)) |>
  set_names(lags) |> 
  list_rbind(names_to = "lag") |>
  summarize(mean_cor = mean(cor, na.rm = TRUE), .by = lag) |>
  mutate(lag = as.numeric(lag))
```

```{r plot-can-cors}
#| fig-width: 7
can_lag_cors |> 
  ggplot(aes(x = lag, y = mean_cor)) +
  geom_vline(xintercept = can_lag_cors$lag[which.max(can_lag_cors$mean_cor)], color = primary) + 
  geom_line(color = secondary) +
  geom_point(color = secondary) +
  scale_x_continuous(expand = expansion()) +
  labs(x = "Lag", y = "Correlation over time\naveraged over space")
```

::: {.fragment .absolute .box-text left=10% top=20%}
* Really strong weekly pattern. 

* But we can fix that. 
:::

## Quickly compute rolling functions by group

```{r can-7dav}
#| echo: true
#| code-line-numbers: 1-2|5
# trailing by default, new names are automatically created
can_prov <- epi_slide_mean(can_prov, c(case_rate, death_rate), .window_size = 7L)
can_lag_cors <- map(
  lags, 
  \(l) epi_cor(can_prov, case_rate_7dav, death_rate_7dav, cor_by = geo_value, dt1 = -l)
)
```

```{r can-cors-again}
#| fig-width: 7
can_lag_cors |> 
  set_names(lags) |> 
  list_rbind(names_to = "lag") |>
  summarize(mean_cor = mean(cor, na.rm = TRUE), .by = lag) |>
  mutate(lag = as.numeric(lag)) |>
  ggplot(aes(x = lag, y = mean_cor)) +
  geom_vline(aes(xintercept = lag[which.max(mean_cor)]), color = primary) + 
  geom_line(color = secondary) +
  geom_point(color = secondary) +
  scale_x_continuous(expand = expansion()) +
  labs(x = "Lag", y = "Correlation over time\naveraged over space")
```


## Notes on lagged correlations

<br>
Trailing average pushes the correlation [backward]{.secondary}

<br>
But weekly reporting aggregates incidence [forward]{.primary}

<br>
These may roughly offset, but better if you know the probability of reports and
deconvolve  
[more on this later]{.small .grey}

<br>
We were only averaging over 13 provinces + territories

<br>
Implicitly assuming that reporting / testing / disease behaviour is stable over
4 years

## Compare to the US


```{r us-cor}
#| echo: true
#| code-line-numbers: 1-2
# Only consider the 50 US states (no territories)
us_edf <- covid_case_death_rates |> filter(geo_value %in% tolower(state.abb)) 
cor0 <- epi_cor(us_edf, case_rate, death_rate, cor_by = geo_value)
cor21 <- epi_cor(us_edf, case_rate, death_rate, cor_by = geo_value, dt1 = -21)
```

```{r us-cor-plot}
#| fig-width: 7
bind_rows(`0` = cor0, `21` = cor21, .id = "lag") |>
  mutate(lag = fct_relabel(factor(lag), ~ paste(.x, "days"))) |>
  ggplot(aes(cor)) +
  geom_density(aes(fill = lag, col = lag), alpha = 0.5) +
  scale_fill_delphi() +
  scale_color_delphi() +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  scale_x_continuous(expand = expansion(0), limits = c(0, 1)) +
  labs(x = "Correlation", y = "Density", fill = "Lag", col = "Lag") +
  theme(legend.position = "right")
```


## Compare to the US

```{r sys-lag-ex}
#| fig-width: 7
#| fig-height: 3.5
map(lags, \(lag) epi_cor(us_edf, case_rate, death_rate, cor_by = geo_value, dt1 = -lag)) |>
  set_names(lags) |> 
  list_rbind(names_to = "lag") |>
  summarize(mean_cor = mean(cor, na.rm = TRUE), .by = lag) |>
  mutate(lag = as.numeric(lag)) |>
  ggplot(aes(x = lag, y = mean_cor)) +
  geom_vline(aes(xintercept = lag[which.max(mean_cor)]), color = primary) + 
  geom_line(color = secondary) +
  geom_point(color = secondary) +
  labs(x = "Lag", y = "Correlation over time\naveraged over space")
```

::: {.fragment .box-text .absolute top=20% left=10%}
Aggregate cases tend to lead deaths by &#8776;23 days (arg max $\rho$)

<br>
Same was true for Canada after smoothing, but lower correlation
:::

## Examining how correlations change over time

```{r cor-by-time-value}
#| echo: true
cor0 <- epi_cor(us_edf, case_rate, death_rate, cor_by = time_value, method = "kendall")
cor21 <- epi_cor(us_edf, case_rate, death_rate, cor_by = time_value, method = "kendall", dt1 = -21)
```

```{r plot-cor-by-time-value}
#| fig-width: 7
bind_rows(`0` = cor0, `21` = cor21, .id = "lag") |>
  mutate(lag = paste(lag, "days")) |>
  ggplot(aes(x = time_value, y = cor, color = lag)) +
  geom_line(key_glyph = "timeseries") +
  geom_hline(yintercept = 0) +
  scale_color_delphi() +
  scale_x_date(expand = expansion()) +
  theme(legend.position = "right") +
  labs(x = "Date", y = "Correlation over space")
```


## Compute growth rates


```{r growth-rates-ex}
#| echo: true
#| code-line-numbers: 2
edfg <- filter(can_prov, geo_value %in% c("MB", "BC"), !is.na(case_rate_7dav)) |>
  mutate(gr_cases = growth_rate(case_rate_7dav, time_value, method = "linear_reg", h = 21L), .by = geo_value)
```

```{r plot-growth-rates-ex}
#| fig-width: 7
edf <- us_edf
gr1 <- edfg |>
  filter(between(time_value, ymd("2020-06-01"), ymd("2023-06-01"))) |>
  ggplot(aes(time_value)) +
  geom_ribbon(aes(ymin = 0, ymax = pmax(case_rate_7dav, 0), fill = geo_value)) +
  scale_fill_delphi() +
  ylab("Case rate\n(7 day avg.)") + xlab("") +
  facet_wrap(~geo_value) +
  scale_y_continuous(expand = expansion(c(0, 0.05))) +
  theme(legend.position = "none", axis.text.x = element_blank(), axis.ticks.x = element_blank())
  
gr2 <- edfg |>
  filter(between(time_value, ymd("2020-06-01"), ymd("2023-06-01"))) |>
  ggplot(aes(x = time_value)) +
  facet_grid(~ geo_value) +
  ylab("Growth rate") + xlab("") +
  geom_hline(yintercept = 0, alpha = .8) +
  geom_line(aes(y = gr_cases, color = geo_value)) +
  scale_color_delphi() +
  theme(legend.position = "none", strip.background = element_blank(),
  strip.text.x = element_blank())
cowplot::plot_grid(gr1, gr2, align = "v", ncol = 1, axis = "lr")
```

## Outlier detection

MAKE OFFLINE

```{r outlier-query}
#| eval: false
outliers <- pub_covidcast("jhu-csse", "confirmed_incidence_num", "state", "day",
                          geo_values = "fl,nj", as_of = "20210601") |>
  select(time_value, geo_value, cases = value) |>
  as_epi_df(as_of = "2021-06-01")
```

```{r outlier-ex}
#| eval: false
#| echo: true
outliers <- outliers |>
  mutate(detect_outlr_rm(time_value, cases), .by = geo_value)
```


```{r plot-outlier-ex}
#| eval: false
#| fig-width: 7
outp1 <- outliers |> 
  ggplot(aes(x = time_value)) +
  geom_line(aes(y = cases, color = geo_value)) +
  geom_line(aes(y = lower), color = primary, alpha = .6) +
  geom_line(aes(y = upper), color = primary, alpha = .6) +
  scale_color_manual(name = "", values = c(secondary, tertiary), guide = "none") +
  geom_hline(yintercept = 0) + xlab("") + ylab("Cases") +
  facet_wrap(~ geo_value, scales = "free_y", nrow = 1) +
  scale_x_date(date_breaks = "6 months", date_labels = "%b %Y", expand = expansion(0)) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
outp2 <- outliers |> 
  ggplot(aes(x = time_value)) +
  geom_line(aes(y = cases, color = geo_value)) +
  geom_line(aes(y = replacement), color = primary) +
  scale_color_manual(name = "", values = c(secondary, tertiary), guide = "none") +
  xlab("") + ylab("Cases") +
  coord_cartesian(ylim = c(0, NA)) +
  facet_wrap(~ geo_value, scales = "free_y", nrow = 1) +
  scale_x_date(date_breaks = "6 months", date_labels = "%b %Y", expand = expansion(0)) +
  scale_y_continuous(expand = expansion(c(0, 0.05))) +
  theme(strip.background = element_blank(), strip.text.x = element_blank())
cowplot::plot_grid(outp1, outp2, align = "v", ncol = 1, axis = "lr")
```


## Advanced sliding on an `epi_df`

* Compute rolling summaries of signals. 
* These depend on the reference time
* Computed separately over geographies (and other groups). 


```{r epi-slide-example-call}
#| echo: true
#| eval: false
epi_slide(
  .x,
  .f,
  ..., # for tidy-evaluation
  .window_size = NULL,
  .align = c("right", "center", "left"),
  .ref_time_values = NULL, # at which time values do I calculate the function
  .new_col_name = NULL, # add a new column with this name rather than the default
  .all_rows = FALSE # do return all available time_values, or only the ones with a result
)
```
::: {.fragment}
`.f` "sees" a data set with a time value and other columns

That data is labeled with  

1. A reference time (the time around which the window is taken)
2. A grouping key
:::

::: {.fragment .box-text .absolute top=10%}
* `epi_slide()` is [very]{.secondary} general, often too much so.
* We already saw the [most]{.secondary} common special case `epi_slide_mean()`
* For other common cases, there is `epi_slide_opt()`
:::

## Really ugly, but actually deployed slide functions

Function to flag outliers for corrections during late-2020 and early-2021

```{r covid-corrections}
#| echo: true
#| code-line-numbers: 1-3,22-25|4-8|9-10,23|11-17|18-22
flag_covid_outliers <- function(signal, sig_cut = 2.75, size_cut = 20, sig_consec = 1.2) {
  signal <- rlang::enquo(signal)
  function(x, g, t) {
    .fns <- list(m = ~ mean(.x, na.rm = TRUE), med = ~ median(.x, na.rm = TRUE), 
                 sd = ~ sd(.x, na.rm = TRUE), mad = ~ median(abs(.x - median(.x)))
    )
    fs <- filter(x, time_value <= t) |> summarise(across(!!signal, .fns, .names = "{.fn}"))
    ss <- summarise(x, across(!!signal, .fns, .names = "{.fn}"))
    mutate(
      x, 
      ftstat = abs(!!signal - fs$med) / fs$sd, # mad in denominator is wrong scale, 
      ststat = abs(!!signal - ss$med) / ss$sd, # basically results in all the data flagged
      flag = 
        (abs(!!signal) > size_cut & !is.na(ststat) & ststat > sig_cut) | # best case
        (is.na(ststat) & abs(!!signal) > size_cut & !is.na(ftstat) & ftstat > sig_cut) | 
        # use filter if smoother is missing
        (!!signal < -size_cut & (!is.na(ststat) | !is.na(ftstat))), # big negative
      flag = flag | # these allow smaller values to also be outliers if they are consecutive
        (lead(flag) & !is.na(ststat) & ststat > sig_consec) | 
        (lag(flag) & !is.na(ststat) & ststat > sig_consec) |
        (lead(flag) & is.na(ststat) & ftstat > sig_consec) |
        (lag(flag) & is.na(ststat) & ftstat > sig_consec)
    ) |> filter(time_value == t) |> pull(flag)
  }
}
```

## Really ugly, but actually deployed slide functions

Function to back distribute data [randomly]{.secondary}

```{r covid-multinomial}
#| echo: true
#| code-line-numbers: 1-4,11-13|5-7|14-18|8-10
corrections_multinom_roll <- function(x, excess, flag, time_value, max_lag = 30L, reweight = exp_w) {
  locs <- which(flag)
  if (length(locs) == 0) return(x)
  for (ii in locs) {
    if (ii <= max_lag) ii_lag <- seq_len(ii)
    else ii_lag <- seq(ii - max_lag + 1, ii)
    w <- reweight(length(ii_lag)) / length(ii_lag)
    x[ii] <- x[ii] - excess[ii]
    prop <- x[ii_lag] + sign(excess[ii]) * rmultinom(1, abs(excess[ii]), w)
    x[ii_lag] <- prop
  }
  x
}
exp_w <- function(n, std_decay = 30L, b0 = 8, a = exp(1) / 2){
  w <- (1:std_decay) / std_decay
  w <- tail(w, n)
  1 / (1 + exp(-w * b0 + a))
}
```

## Roll our outlier detector, then calculate the corrections

```{r api-corrections}
#| eval: false
corrections_df <- pub_covidcast(
  "jhu-csse", "deaths_incidence_num", "state", "day", 
  geo_values = "nj,oh", as_of = 20210214
) |> select(geo_value, time_value, deaths = value) |>
  as_epi_df(as_of = ymd("2021-02-14"))
write_rds(corrections_df, here::here("_data", "corrections_df.rds"))
```

```{r replicate-corrections}
#| echo: true
#| code-line-numbers: 1-3|4
corrections_df <- epi_slide(
  corrections_df, .align = "center", .window_size = 14L, .new_col_name = "flag",
  .f = flag_covid_outliers(deaths)
) |> mutate(corrected_deaths = corrections_multinom_roll(deaths, deaths, flag, time_value), .by = geo_value)
```

```{r plot-corrections}
#| fig-width: 7
corrections_df |>
  pivot_longer(contains("deaths")) |>
  ggplot(aes(time_value)) +
  geom_line(aes(y = value, color = name), key_glyph = "timeseries") +
  geom_point(data = filter(corrections_df, flag), aes(y = deaths), color = tertiary) +
  facet_wrap(~ geo_value, scales = "free_y") + ylab("Deaths") + xlab("Date") +
  coord_cartesian(xlim = ymd(c("2020-03-01", NA))) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "4 months", expand = expansion(c(0, 0.03))) +
  geom_hline(yintercept = 0, color = theme_black) +
  scale_color_delphi()
```

# Tracking Revisions {.inverse}

## `epi_archive`: Collection of `epi_df`s

* Full version history of a data set
* Acts like a bunch of `epi_df`'s --- but stored [compactly]{.secondary}
* Similar functionality as we saw but using only [data that would have been available at the time]{.secondary}

::: {.callout-note}
## Revisions

Epidemiology data gets revised frequently.

* We may want to use the data [as it looked in the past]{.secondary}.
* or we may want to examine [the history of revisions]{.secondary}.
:::

## `epi_archive`: Collection of `epi_df`s

Subset of daily COVID-19 doctor visits (Optum) and cases (JHU CSSE) from all U.S. states in `archive` format:

```{r archive-ex}
#| echo: true
#| eval: false
archive_cases_dv_subset_all_states
```

![](gfx/archive_cases_dv.png){.codelike-format}

## Summarize revision behaviour



```{r epiprocess-revision-summary-demo}
#| echo: true
#| eval: false
revision_data <- revision_summary(archive_cases_dv_subset, case_rate_7d_av)
revision_data
```

![](gfx/revision_data.png){.codelike-format}



## Visualize revision patterns

```{r plot-revision-patterns}
#| fig-width: 7
#| fig-height: 3.5
autoplot(
  archive_cases_dv_subset, percent_cli, 
  .versions = seq(ymd("2020-07-01"), ymd("2021-11-30"), by = "1 month"), 
  .mark_versions = TRUE
) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "", y = "% of doctor's visits with\n Covid-like illness") + 
  scale_color_viridis_c(option = "B", end = .8) +
  scale_y_continuous(limits = c(0, NA), expand = expansion(c(0, 0.05))) +
  theme(legend.position = "none")
```

## Finalized data

* Counts are revised as time proceeds
* Want to know the [final]{.secondary} value 
* Often not available until weeks/months later

  
Backcasting
: At time $t$, predict the final value for time $t-h$, $h < 0$

  <br>
  
Nowcasting
: At time $t$, predict the final value for time $t$

<br>

Forecasting
: At time $t$, predict the final value for time $t+h$, $h > 0$


## Sliding computations over archives

```{r}
#| echo: true
#| eval: false
epix_slide(
  .x,
  .f,
  ...,
  .before = Inf,
  .versions = NULL,
  .new_col_name = NULL,
  .all_versions = FALSE
)
```

::: {.fragment .box-text .absolute top=10%}

* To perform nowcasts we need to track how values get revised
* To evaluate forecasting [models]{.secondary}, we need to test them on the data we
[would have seen]{.secondary}

:::



# Nowcasting Using [{epiprocess}]{.monotype} {.inverse}

<!-- predicting a finalized value from a provisional value and making predictions. -->
## Why this matters
  
* Every week [BC CDC released COVID-19 hospitalization data](http://www.bccdc.ca/health-info/diseases-conditions/covid-19/archived-b-c-covid-19-data).

* The following week, they revised the number upward (by ~25%) due to lagged reports.

![](gfx/bc_hosp_admissions_ex.jpg){fig-align="center"}

::: {.fragment .box-text .absolute top=30%}
Comparing preliminary to revised data [often]{.secondary} shows a decline.

Due to [backfill]{.secondary}
:::

## Backfill American edition - NCHS COVID-19 mortality


```{r}
#| fig-width: 7
#| fig-height: 3.5
nchs_versions <- seq(ymd("2024-01-07"), ymd("2024-03-24"), by = "1 week")
autoplot(nchs_archive, mortality, .versions = nchs_versions) +
  scale_y_continuous(limits = c(0, 250), expand = expansion()) +
  scale_x_date(limits = ymd(c("2023-12-31", "2024-04-01")), expand = expansion())
```

## The revision triangle

```{r revision-triangle}
#| fig-height: 4
#| dev: png
ca_nchs_finalized <- epix_as_of(nchs_archive, nchs_archive$versions_end) |>
  filter(geo_value == "ca", time_value %in% nchs_versions)
ca_nchs_complete <- map(
  nchs_versions, 
  ~ epix_as_of(nchs_archive, .x) |> 
    filter(geo_value == "ca", time_value >= min(nchs_versions)) |>
    mutate(version = .x)
) |> list_rbind()
    
ca_revision_triangle <- ca_nchs_complete |>
  left_join(ca_nchs_finalized |> rename(final_mortality = mortality)) |>
  mutate(mortality = mortality / final_mortality, final_mortality = NULL)

alt_versions <- nchs_versions[seq_along(nchs_versions) %% 2 == 1]

ca_revision_triangle |>
  ggplot(aes(version, time_value, fill = mortality)) +
  geom_raster() +
  scale_y_date(
    name = "Reference date", breaks = alt_versions - weeks(1), 
    date_labels = "%b %d", expand = expansion()) +
  scale_x_date(
    name = "Report date", breaks = alt_versions, 
    date_labels = "%b %d", expand = expansion()) +
  coord_equal() +
  theme(legend.position = "right", legend.title = element_text()) +
  scale_fill_viridis_c(
    transform = "logit", direction = -1, breaks = c(.2, .5, .75, .9, .95),
    option = "B", name = "Ratio to final"
  )
```


::: {.fragment .box-text .absolute top=10%}
* On day $t$, predict the finalized value of signal $y_t$

* We may have a provisional value for $y_t$ (subject to revision)

* Most likely, we only have provisional values for earlier dates

* We may only have "finalized" values for $y_{t-s}$, $s\gg0$

:::




## Formal analysis of versioning behavior 

Latency
: the time difference between Reference Date and Initial Report Date

Backfill
: the characteristics of updates after initial report (typical positive)


```{r inspect-revision_summary}
#| echo: true
#| code-line-numbers: 1-2
# same as before, but the NCHS data
revision_data <- revision_summary(nchs_archive, mortality, within_latest = .05, return_only_tibble = TRUE)
revision_data |> filter(geo_value == "ca", time_value %in% nchs_versions) |> print(width = 120)
```



## Revision pattern visualization  

```{r mortality-by-accessed-weeks-later}
#| echo: false
ref_dates <- unique(nchs_archive$DT$time_value)
offsets = seq(1, 7) * 7
max_version <- nchs_archive$versions_end

get_val_asof <- function(time_val, archive, offsets) {
  as_of_dates <- pmin(time_val + offsets, max_version)
  map(as_of_dates, \(.x) {
    archive |>
      epix_as_of(.x) |>
      filter(time_value == time_val) |>
      select(geo_value, time_value, mortality) |>
      mutate(lag = .x - time_val)
  }) |> list_rbind()
}

value_at_lags <- map(ref_dates, get_val_asof, 
  archive <- nchs_archive, offsets <- offsets) |>
  list_rbind()
  
values_final <- epix_as_of(nchs_archive, max(nchs_archive$versions_end))
```

```{r final-vs-revisions-plot}
#| echo: false
#| fig-width: 7
#| fig-height: 3.5
value_at_lags |> filter(time_value >= ymd("2021-03-01")) |>
  ggplot(aes(x = time_value, y = mortality)) +  
  geom_line(aes(y = mortality, color = factor(lag / 7))) + 
  geom_line(data = values_final |> filter(time_value >= ymd("2021-03-01")), 
            color = "black") +
  facet_wrap(~ geo_value, scales = "free_y", ncol = 1) +
  scale_color_viridis_d(option = "B", end = 0.8, direction = -1, name = "Lag (weeks)") +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y", expand = expansion()) +
  labs(x = "", y = "Weekly new COVID deaths") + 
  scale_y_continuous(expand = expansion(c(0, 0.05))) +
  theme(legend.position = "right", legend.title = element_text())
```


## Building training data for nowcasting

* Can't estimate [any]{.secondary} statistical model, unless I "see" the response
* The finalized value, is often [very]{.secondary} slow (~ 1 year for this signal)
* So a compromise is to use something close, here I'm using 95% of the finalized value

```{r revision-summary-time-near-latest-show-again}
#| echo: true
revision_ca <- filter(revision_data, geo_value == "ca")
revision_ca |> select(geo_value, time_value, lag_near_latest) |> slice_sample(n = 5)
(lag_quantiles <- quantile(revision_data$lag_near_latest))
approx_final_lag <- lag_quantiles["75%"]
```

::: {.fragment .box-text .absolute top=20%}
* Pretend that for time $s$, the $Y_s$ with version $s +$  `r approx_final_lag` weeks is "final".
* At time $t$, the most recent data in our training set will be `r approx_final_lag` weeks old.
:::

## What about features?

Must use features that would have been available at test time.

Must have enough samples to ensure sensible estimation results.

1. [provisional value(s) for time $t$]{.fragment .strike}
2. provisional value(s) for time $t-\ell$
3. exogenous signals that may be available

::: {.fragment}
### Potential model

Predictors are 

* Provisional values for time $t-\ell$, $\ell =$ 1 week, 2 weeks when available
* Provisional $Z_{t-k}$ for HHS/NHSN COVID-19 hospitalizations (these are daily, so different lags)

Exclude a potential predictor if it doesn't have much training data available.

:::





# Nowcasting with Regression {.inverse}


## Operationalizing

* Function needs to work on multiple nowcast dates
* Sometimes reporting changes, so we should adjust, not error
* If a predictor isn't available, we remove it from the model and proceed
* Make sure we have "enough" training data to fit a model
* The nowcaster needs access to all versions prior to the nowcast date
* We want to retrain at every date: `epix_slide(..., .all_versions = TRUE)`
* Allow for linear regression or median regression

## Big ugly function 

* Goal: eventually refactor and put in `{epipredict}`

```{r regression-nowcaster-helpers}
nchs_ca_archive <- nchs_archive$DT |> filter(geo_value == "ca") |> as_epi_archive()

library(data.table)
get_predictor_training_data <- function(archive, varname, lag_days, predictor_name) {
  epikeytime_names <- setdiff(key(archive$DT), "version")
  requests <- unique(archive$DT, by = epikeytime_names, 
                     cols = character())[, version := time_value + ..lag_days]
  setkeyv(requests, c(epikeytime_names, "version"))
  result <- archive$DT[
    requests, c(key(archive$DT), varname), roll = TRUE, nomatch = NULL, 
    allow.cartesian = TRUE, with = FALSE
  ][, time_value := version][, version := NULL]
  nms <- names(result)
  nms[[match(varname, nms)]] <- predictor_name
  setnames(result, nms)
  setDF(result)
  as_tibble(result)
}

# We can apply this separately for each nowcast_date to ensure that we consider
# the latest possible value for every signal, though whether that is advisable
# or not may depend on revision characteristics of the signals.
thin_daily_to_weekly_archive <- function(archive) {
  key_nms <- key(archive$DT)
  val_nms <- setdiff(names(archive$DT), key_nms)
  update_tbl <- as_tibble(archive$DT)
  val_nms |>
    lapply(function(val_nm) {
      update_tbl[c(key_nms, val_nm)] |>
        # thin out to weekly, making sure that we keep the max time_value with non-NA value:
        filter(as.POSIXlt(time_value)$wday == as.POSIXlt(max(time_value[!is.na(.data[[val_nm]])]))$wday) |>
        # re-align:
        mutate(
          time_value = time_value - as.POSIXlt(time_value)$wday, # Sunday of same epiweek
          old_version = version,
          version = version - as.POSIXlt(version)$wday # Sunday of same epiweek
        ) |>
        slice_max(old_version, by = all_of(key_nms)) |>
        select(-old_version) |>
        as_epi_archive(other_keys = setdiff(key_nms, c("geo_value", "time_value", "version")),
                       compactify = TRUE)
    }) |>
    reduce(epix_merge, sync = "locf")
}
```

```{r regression-nowcaster-function}
#| echo: true
#| code-line-numbers: 1,6-7|9-27|36-42|44-52|54-59
regression_nowcaster <- function(archive, model_settings, return_info = FALSE) {
  if (!inherits(archive, "epi_archive")) stop("`archive` isn't an `epi_archive`")
  if (n_distinct(archive$DT$geo_value) != 1L) stop("Expected exactly one unique `geo_value`")
  if (archive$time_type == "day") archive <- thin_daily_to_weekly_archive(archive)
  nowcast_date <- archive$versions_end
  target_time_value <- nowcast_date
  latest_edf <- archive |> epix_as_of(nowcast_date)

  predictor_descriptions <-
    latest_edf |>
    mutate(lag_days = as.integer(nowcast_date - time_value)) |>
    select(-c(geo_value, time_value)) |>
    pivot_longer(-lag_days, names_to = "varname", values_to = "value") |>
    drop_na(value) |>
    inner_join(model_settings$predictors, by = "varname", unmatched = "error") |>
    filter(abs(lag_days) <= max_abs_shift_days) |>
    arrange(varname, abs(lag_days)) |>
    group_by(varname) |>
    filter(seq_len(n()) <= max_n_shifts[[1]]) |>
    ungroup() |>
    mutate(predictor_name = paste0(varname, "_lag", lag_days, "_realtime")) |>
    select(varname, lag_days, predictor_name)

  predictor_edfs <- predictor_descriptions |>
    pmap(function(varname, lag_days, predictor_name) get_predictor_training_data(archive, varname, lag_days, predictor_name)) |>
    lapply(na.omit) |>
    keep(~ nrow(.x) >= model_settings$min_n_training_per_predictor)

  if (length(predictor_edfs) == 0) stop("Couldn't find acceptable predictors in the latest data.")

  predictors <- reduce(predictor_edfs, full_join, by = c("geo_value", "time_value"))
  target <- latest_edf |>
    filter(time_value <= max(time_value) - model_settings$days_until_target_semistable) |>
    select(geo_value, time_value, mortality_semistable = mortality)

  training_and_nowcast <- full_join(predictors, target, by = c("geo_value", "time_value"))

  training <- training_and_nowcast |>
    drop_na() |>
    slice_max(time_value, n = model_settings$max_n_training_intersection)

  nowcast_features <- training_and_nowcast |> filter(time_value == nowcast_date)

  form <- as.formula("mortality_semistable ~ .")
  fit_fun <- switch(
    model_settings$method,
    rq = function(x) { quantreg::rq(data = x, formula = form, tau = 0.5) },
    lm = function(x) { lm(formula = form, data = x) }
  )
  the_fit <- training |>
    select(any_of(predictor_descriptions$predictor_name), mortality_semistable) |>
    fit_fun()
      
  pred <- tibble(
    geo_value = "ca",
    nowcast_date = nowcast_date,
    target_date = target_time_value,
    prediction = unname(predict(the_fit, nowcast_features))
  )

  if (return_info) return(tibble(coefficients = list(coef(fit)), predictions = list(pred)))
  return(pred)
}
```

```{r baseline-nowcaster-model}
locf_nowcaster <- function(archive) {
  nowcast_date <- archive$versions_end
  target_time_value <- nowcast_date
  latest_edf <- archive |> epix_as_of(nowcast_date)

  latest_edf |>
    complete(geo_value, time_value = target_time_value) |>
    arrange(geo_value, time_value) |>
    group_by(geo_value) |>
    fill(mortality) |>
    ungroup() |>
    filter(time_value == target_time_value) |>
    transmute(
      geo_value,
      nowcast_date = nowcast_date,
      target_date = time_value,
      prediction = mortality
    )
}
```

## Model settings

We'll compare 4 different configurations:

* Using `lm()` and only mortality predictors
* Using `lm()` with mortality and hospitalizations as a predictor
* Using `rq()` and only mortality predictors
* Using `rq()` with mortality and hospitalizations as a predictor


```{r hhs-archive-api}
#| eval: false
hhs_hosp_archive <- pub_covidcast(
  source = "hhs",
  signals = "confirmed_admissions_covid_1d_7dav",
  geo_type = "state",
  time_type = "day",
  geo_values = "ca",
  time_values = epirange(20210301, 20241001),
  issues = "*"
) |>
  transmute(geo_value, time_value, version = issue, admissions = 7 * value) |>
  as_epi_archive(compactify = TRUE)
write_rds(hhs_hosp_archive$DT, here::here("_data", "hhs_hosp_archive_dt.rds"))
```


```{r regression-model-settings}
#| echo: true
#| code-line-numbers: 1-14
reg1_settings <- list(
  predictors = tribble(
    ~varname,    ~max_abs_shift_days, ~max_n_shifts,
    "mortality",                  35,             3,
    ),
  min_n_training_per_predictor = 30, # or else exclude predictor
  days_until_target_semistable = 7 * 7, # filter out unstable when training (and evaluating)
  min_n_training_intersection = 20, # or else raise error
  max_n_training_intersection = Inf # or else filter down rows
)
```

```{r more-regression-model-settings}
all_nowcast_dates = nchs_archive$DT |>
  filter(time_value >= as.Date("2022-01-01")) |>
  distinct(time_value) |>
  pull(time_value)

reg2_settings <- list(
  predictors = tribble(
    ~varname,     ~max_abs_shift_days, ~max_n_shifts,
    "admissions",                  35,             3,
    "mortality",                   35,             3,
    ),
  min_n_training_per_predictor = 30, # or else exclude predictor
  days_until_target_semistable = 7 * 7, # filter out unstable when training (and evaluating)
  min_n_training_intersection = 20, # or else raise error
  max_n_training_intersection = Inf # or else filter down rows
)

reg3_settings <- c(reg1_settings, method = "rq")
reg1_settings <- c(reg1_settings, method = "lm")
reg4_settings <- c(reg2_settings, method = "rq")
reg2_settings <- c(reg2_settings, method = "lm")
```


```{r regression-run-nowcasts-backtesting}
nchs_ca_archive_daily <- nchs_ca_archive$DT |>
  as_tibble() |>
  mutate(
    time_value = time_value + 6L, # align with trailing averages
    version = version + 4L # assume NCHS data were released only on Thursdays
  ) |>
  group_by(geo_value, time_value, version) |>
  reframe(time_value = time_value - 6:0,
          mortality = c(rep(NA, 6), mortality)) |>
  as_epi_archive(compactify = TRUE)

hosp_mort_archive <- epix_merge(hhs_hosp_archive, nchs_ca_archive_daily, sync = "locf")

locf_nowcasts <- nchs_ca_archive |>
  epix_slide(~ locf_nowcaster(.x), .versions = all_nowcast_dates, .all_versions = TRUE)

reg1_nowcasts <- nchs_ca_archive |>
  epix_slide(~ regression_nowcaster(.x, reg1_settings), .versions = all_nowcast_dates, .all_versions = TRUE)

reg2_nowcasts <- hosp_mort_archive |>
  epix_slide(~ regression_nowcaster(.x, reg2_settings),
             .versions = all_nowcast_dates + 4, # assume we nowcast on Thursday, same day as assumed NCHS release
             .all_versions = TRUE)

reg3_nowcasts <- nchs_ca_archive |>
  epix_slide(~ regression_nowcaster(.x, reg3_settings), .versions = all_nowcast_dates, .all_versions = TRUE)

reg4_nowcasts <- hosp_mort_archive |>
  epix_slide(~ regression_nowcaster(.x, reg4_settings),
             .versions = all_nowcast_dates + 4, # assume we nowcast on Thursday, same day as assumed NCHS release
             .all_versions = TRUE)
```

```{r regression-nowcast-wrangling}
nowcast_comparison <-
  list(
    locf_nowcasts |> rename(prediction_locf = prediction),
    reg1_nowcasts |> rename(prediction_reg1 = prediction),
    reg2_nowcasts |> rename(prediction_reg2 = prediction),
    reg3_nowcasts |> rename(prediction_reg3 = prediction),
    reg4_nowcasts |> rename(prediction_reg4 = prediction)#,
    #   get_predictor_training_data(nchs_ca_archive, "mortality", 14L, "mortality_lag14_realtime") |>
    #     transmute(geo_value, nowcast_date = time_value, target_date = time_value, mortality_lag14_realtime)
    ) |>
  lapply(select, -any_of("version")) |>
  reduce(full_join, by = c("geo_value", "nowcast_date", "target_date")) |>
  full_join(nchs_ca_archive |> epix_as_of(nchs_ca_archive$versions_end),
            by = c("geo_value", "target_date" = "time_value")) |>
  pivot_longer(starts_with("prediction"), names_to = "Nowcaster", values_to = "prediction") |>
  mutate(Nowcaster = recode(Nowcaster,
                            prediction_locf = "Baseline",
                            prediction_reg1 = "LinReg",
                            prediction_reg2 = "LinReg + hosp",
                            prediction_reg3 = "QntReg",
                            prediction_reg4 = "QntReg + hosp",
                            .default = Nowcaster))
```

## Comparison: linear regression

```{r regression-nowcast-plot-linreg}
#| fig-width: 7
#| fig-height: 3.5
nowcast_comparison |>
  filter(target_date >= min(all_nowcast_dates) - 35,
         str_detect(Nowcaster, "LinReg") | Nowcaster == "Baseline") |>
  ggplot() +
  geom_line(aes(target_date, prediction, color = Nowcaster)) +
  geom_line(aes(target_date, mortality), linewidth = 1) +
  scale_y_continuous(expand = expansion(), limits = c(0, 2500)) +
  scale_x_date(expand = expansion()) +
  scale_color_delphi() +
  xlab("Reference date") +
  ylab("Mortality") +
  theme(legend.position = "right")
```

## Comparison: quantile regression


```{r regression-nowcast-plot-quantreg}
#| fig-width: 7
#| fig-height: 3.5
nowcast_comparison |>
  filter(target_date >= min(all_nowcast_dates) - 35,
         str_detect(Nowcaster, "QntReg") | Nowcaster == "Baseline") |>
  ggplot() +
  geom_line(aes(target_date, prediction, color = Nowcaster)) +
  geom_line(aes(target_date, mortality), linewidth = 1) +
  scale_y_continuous(expand = expansion(), limits = c(0, 2500)) +
  scale_x_date(expand = expansion()) +
  scale_color_delphi() +
  xlab("Reference date") +
  ylab("Mortality") +
  theme(legend.position = "right")
```


## Evaluations

```{r regression-nowcast-eval-comparison}
n_models <- length(unique(nowcast_comparison$Nowcaster))
nowcast_comparison |>
  # Filter evaluation based on target stability
  filter(target_date <= nchs_ca_archive$versions_end - 49) |>
  # Filter evaluated tasks to those with all models available
  group_by(target_date) |>
  filter(sum(!is.na(prediction)) == n_models) |>
  ungroup() |>
  summarize(
    .by = Nowcaster,
    MAE = mean(abs(mortality - prediction)),
    MAPE = 100 * mean(abs(mortality - prediction) / abs(mortality))
  ) |>
  knitr::kable(digits = 2)
```



## Aside on nowcasting

* To many Epis, [nowcasting]{.secondary} means [estimate the instantaneous 
reproduction number, $R_t$]{.secondary}

* Example: Reported COVID-19 cases in British Columbia (Jan. 2020 &ndash; Apr. 2023) 

```{r rtestim}
#| fig-width: 7
#| fig-height: 3
library(rtestim)
source(here::here("_code/bccovid.R"))

p1 <- bccovid|>
  ggplot(aes(date, cases)) + 
  geom_line(colour = primary) +
  labs(y = "BC Covid-19 cases", x = "") +
  scale_y_continuous(expand = expansion(c(0, 0.05))) +
  scale_x_date(expand = expansion())
bc_rt <- estimate_rt(bccovid$cases, x = bccovid$date, 
                     lambda = c(1e6, 1e5))
p2 <- plot(confband(bc_rt, lambda = 1e5)) + 
  coord_cartesian(ylim = c(0.5, 2)) +
  labs(x = "") +
  scale_y_continuous(expand = expansion(0)) +
  scale_x_date(expand = expansion())
cowplot::plot_grid(p1, p2)
```

* More after lunch...

## More practice with the worksheet

* Signal processing and exploratory data analysis

* Examining revision and latency patterns

* Thinking about the revision triangle and how to use it

* Basics of statistical nowcasting 


```{r wksheet}
#| dev: png
wkshtqr
```
