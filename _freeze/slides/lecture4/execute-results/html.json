{
  "hash": "a40265d5da5ef9ead5e679233ef473dd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Forecasting and Advanced Topics\"\nshort-title: \"Forecasting\"\nsubtitle: \"Lecture 4\"\norg: \"CANSSI Prairies &mdash; Epi Forecasting Workshop 2025\"\nformat: revealjs\n---\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n## Outline\n\n\n1. Fundamentals of Forecasting\n1. Evaluation\n1. A Workflow for Forecasting\n1. Advanced Customizations\n1. Build a Forecaster from Scratch\n1. Advanced Topics\n\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\renewcommand{\\top}{\\mathsf{T}}\n\\newcommand{\\one}{\\mathbf{1}}\n\n# Fundamentals of Forecasting {.inverse}\n(with some review)\n\n## Forecasting is not magic\n\n- Forecasts are generally comprised of two parts: trend and seasonality\n- Methods for detecting and projecting trends are not magic; in general they're\n  not qualitatively that different from what you can do with your eyeballs\n- That said, assimilating information from exogenous features (ideally, leading\n  indicators) can lead highly nontrivial gains, beyond the eyeballs\n- Remember &#8230; good data is just as (more?) important as a good model! \n- Seasonality can help short-term forecasts. Long-term forecasts, absent of \n  strong seasonality, are generally not very tractable\n\n## Basics of linear regression \n\n* Assume we observe a predictor $x_i$ and an outcome $y_i$ for $i = 1, \\dots, n$.\n\n* Linear regression supposes\n\n$$\\E[y_i\\given x_i] \\doteq \\beta_0 + \\beta_1 x_i,\\quad i=1,\\dots,n.$$\n\n\n* In `R`, run `lm(y ~ x)`, where `y` is the vector \nof responses and `x` the vector of predictors.\n\n\n. . .\n\n* Given $p$ different predictors\n\n$$\n\\begin{aligned}\n\\E[y_i\\given \\mathbf{x}_i] &\\doteq \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}\\\\\n&= \\mathbf{x}^\\top_i\\beta &i=1,\\dots,n.\n\\end{aligned}\n$$\n\n\n## Linear regression with lagged predictor\n\n\n* In time series, outcomes and predictors are usually indexed by time $t$. \n\n::: {.fragment .fade-in}\n* [Goal]{.secondary}: predict future $y$, given present $x$. \n\n:::\n\n::: {.fragment .fade-in}\n* [Model]{.secondary}: linear regression with lagged predictor\n\n$$\\E[y_t\\given x_{t-k}] \\doteq \\beta + \\beta_0 x_{t-k}$$\n\n:::\n\n::: {.fragment .fade-in}\n* [Equivalent]{.secondary} way to write the model: \n\n$$\\E[y_{t+k}\\given x_t] \\doteq \\beta + \\beta_0 x_t$$\n\n:::\n\n\n## Autoregressive exogenous input (ARX) model\n\n* Predict the outcome via a linear combination of its (own) lags \nand exogenous predictors\n\n\n$$\\E[y_{t+h}\\given \\mathbf{y}_{\\leq t}, \\mathbf{x}_{\\leq t}] \\doteq \n\\phi + \\sum_{i=0}^p \\phi_i y_{t-i} + \\sum_{j=0}^q \\beta_j x_{t-j}$$\n\n* [Notice]{.secondary}: we don't need to include all contiguous lags, and we could estimate e.g.,\n\n$$\\E[y_{t+h}\\given \\mathbf{y}_{\\leq t}, \\mathbf{x}_{\\leq t}] \\doteq  \\phi + \n\\phi_0 y_{t} + \\phi_1 y_{t-7} + \\phi_2 y_{t-14} +\n\\beta_0 x_{t} + \\beta_1 x_{t-7} + \\beta_2 x_{t-14}$$\n\n\n## Popular forecasting frameworks\n\n- Autoregressive integrated model average (ARIMA) models\n- Exponential smoothing with trend and seasonality (ETS) \n- Prophet forecaster\n- DeepAR (neural network)\n\nFirst two here are classic and standard, second two are more recent. \n\nNone are particularly well-suited for epi forecasting out-of-the-box \n(ask me about them if you're curious)\n\nI'll briefly mention ARIMA as it's closest to the setup so far.\n\n## Dissecting ARIMA \n\nAR \n: autoregressive\n: include lags of response as features\n\nMA \n: moving average\n: include lags of noise terms \n: correlated noise model\n\nI\n: integrated\n: model and forecast differences between successive observations rather than levels\n\n## ARIMA vs ARX\n\n### The way lags are handled\n\n* In what you've seen, we can include arbitrary lags\n* Could use a different [engine](.secondary) (regularized linear, or general functional form)\n* Traditional AR models require lags to be contiguous (e.g., all of 0&ndash;14, \n    instead of 0, 7, 14)\n\n### The way multi-step forecasts are made\n* In what you've seen, we model h-step ahead directly \n* Traditional AR models only do 1-step ahead prediction, and iterate this \nto get forecasts at longer horizons\n\n## ARIMA vs ARX\n\n### The way nonstationarity is handled\n\n* In what you've seen, we address nonstationarity via trailing training \n    windows (or observation weights more generally)\n* Traditional ARIMA models use the I component for this: remove linear or\n    quadratic trends by differences, add them back in at prediction time\n\n### The way exogenous features are included\n\n* In what you've seen, they appear directly as an exogenous predictor\n* Traditional ARIMA models (software, such as `{fable}`) includes them in \n    a different manner; they are effectively subject to the same lags as the AR\n    and MA terms\n\n## Supplementary time series resources\n\n- Hyndman and Athanasopoulos, [Forecasting: Principles and Practice](https://otexts.com/fpp3/)\n- Ryan Tibshirani's course notes, [Introduction to Time Series](https://stat153.berkeley.edu/fall-2024/)\n\n\n# Evaluation {.inverse}\n\n## Error metrics\n\n* Assume we have predictions $\\hat y_{t}$ for the unseen observations \n$\\tilde y_{t}$ over times $t = 1, \\dots, N$.\n\n### Four commonly used error metrics for point forecasts:\n1. mean squared error (MSE)\n1. mean absolute error (MAE)\n1. mean absolute percentage error (MAPE)\n1. mean absolute scaled error (MASE)\n    \n### Interval metrics:\n1. coverage\n1. interval score\n1. weighted interval score\n\n## Error metrics: MSE and MAE\n\n$$\\textrm{MSE} = \\frac{1}{N} \\sum_{t=1}^N (\\tilde y_{t}- \\hat y_{t})^2$$\n$$\\textrm{MAE} = \\frac{1}{N} \\sum_{t=1}^N |\\tilde y_{t}- \\hat y_{t}|$$\n\n* MAE gives less importance to extreme errors than MSE.\n\n* MSE is not on the same scale as the data (squared units), use RMSE instead.\n\n* [Drawback]{.primary}: both metrics are scale-dependent, so they are not universally \ninterpretable.  \n(For example, if $y$ captures height, MSE and MAE will vary depending on whether we measure in feet or meters.)\n\n## Error metrics: MAPE\n\nFixing scale-dependence:\n\n$$\\textrm{MAPE} = 100 \\times \\frac{1}{N} \\sum_{t=1}^N \n\\left|\\frac{\\tilde y_{t}- \\hat y_{t}}{\\tilde y_{t}}\\right|$$\n\n\n### Drawbacks\n* Erratic behavior when $\\tilde y_{t}$ is close to zero\n* Assumes the unit of measurement has a meaningful zero  \n(e.g. using \nFahrenheit or Celsius to measure temperature will lead to different MAPE)\n\n\n## Error metrics: MASE\n\n$$\\textrm{MASE} = 100 \\times \\frac{\\frac{1}{N} \\sum_{t=1}^N \n|\\tilde y_{t}- \\hat y_{t}|}\n{\\frac{1}{N-1} \\sum_{t=2}^N \n|\\tilde y_{t}- y_{t-1}|}$$\n\n### Advantages\n\n* universally interpretable (not scale dependent)\n\n* avoids the zero-pitfall (unless the first difference is 0&#8230; )\n\n[Heuristic description]{.secondary}: normalize the error of our forecasts by that of a naive method \nwhich always predicts the last observation.\n\n\n## Comparing MAE, MSE, MAPE and MASE\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture4_files/figure-revealjs/mae-mape-mase-example-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|      |   MAE|  RMSE|   MAPE|    MASE|\n|:-----|-----:|-----:|------:|-------:|\n|yhat1 | 2.873| 4.024| 43.140|  66.100|\n|yhat2 | 5.382| 9.689| 36.083| 123.817|\n\n\n:::\n:::\n\n\n\n\n## Interval metrics\n\nGiven a set of predictive intervals $(l_t^{\\alpha_1}, u_t^{\\alpha_1}), \\dots, (l_t^{\\alpha_K}, u_t^{\\alpha_K}),\\quad t=1,\\dots,N$\n\n### Coverage\n\n$$\\textrm{Coverage}(\\alpha) = \\frac{1}{N} \\sum_{t=1}^N \\one(l_t^{\\alpha} \\leq \\tilde y_{t} \\leq u_t^\\alpha)$$\n\n### Interval Score\n\n$$\\textrm{IS}(\\alpha) = \\frac{1}{N} \\sum_{t=1}^N \\alpha|u_t^\\alpha - l_t^\\alpha| + 2(l_t^\\alpha - \\tilde y_t)_+ + 2(\\tilde y_t - u_t^\\alpha)_+$$\n\n### Weighted Interval Score\n\n$$\\textrm{WIS} = \\sum_{k=1}^K\\textrm{IS}(\\alpha_k)$$\n\n\n\n## Defining the error metrics in R\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nMSE <- function(obs, pred) mean((obs - pred)^2)\n\nMAE <- function(obs, pred) mean(abs(obs - pred))\n\nMAPE <- function(obs, pred) 100 * mean(abs(obs - pred) / obs)\n\nMASE <- function(obs, pred) 100 * MAE(obs, pred) / mean(abs(diff(obs)), na.rm = TRUE)\n\nCoverage <- function(obs, ql, qu) mean(obs >= ql & obs <= qu)\n\nIS <- function(obs, ql, qu, alpha) alpha * mean(abs(qu - ql)) + 2 * pmax(ql - obs, obs - qu)\n```\n:::\n\n\n\n## Estimating the prediction error\n\nGiven an error metric, we want to estimate the prediction error under that metric. \n\n### Methods we'll discuss briefly\n\n1. Training error\n1. Split-sample error\n1. Time series cross-validation error\n\n\n## Training error\n\n* The easiest but [worst]{.secondary} estimate of the prediction error\n\n* The training error is\n\n  1. generally too optimistic as an estimate of prediction error\n  1. [more optimistic the more complex the model!]{.secondary}\n  \n### Training MSE\n\n$$\\text{TrainMSE} = \\frac{1}{N-h} \\sum_{t = 1}^{N-h} (\\hat y_{t+h|N} - y_{t+h})^2$$\n\n## Split-sample error \n\nTo compute the split-sample error  \n\n  1. [Split]{.secondary} data into training (up to time $t_0$), and test set (after $t_0$)\n\n  1. [Estimate]{.secondary} the model to the [training]{.primary} data only\n\n  1. Make [predictions]{.secondary} for the [test]{.tertiary} set\n\n  1. Compute the [error]{.primary} metric on the [test]{.tertiary} set only\n\n\n::: {.callout-important icon=\"false\"}\n\nSplit-sample estimates of prediction error don't mimic real forecasting.\n\nWe would [refit]{.secondary} with new data.\n\nTherefore, split-sample is [pessimistic]{.primary} if the relation between outcome and predictors \nchanges over time.\n:::\n\n## Split-sample MSE \n\n* Want $h$-step ahead predictions\n* at time $t$, forecast for $t+h$. \n\nThen, the split-sample MSE is\n\n$$\\text{SplitMSE} = \\frac{1}{N-h-t_0} \\sum_{t = t_0}^{N-h} (\\hat y_{t+h|t_0} - y_{t+h})^2$$\n\n* $\\hat y_{t+h|t_0}$ is a prediction for $y$ at time $t+h$ \n* that was made with a model that was estimated on data up to time $t_0$.\n\n## Time series cross-validation (CV)\n#### $h$-step ahead predictions\n\n[Re-estimate]{.primary} once new data are available\n\nTo get $h$-step ahead predictions, for each time $t = t_0, t_0+1, \\dots$,\n\n  * [Estimate]{.primary} the model using data [up to time $t$]{.primary}\n\n  * Make a [prediction for $t+h$]{.primary} \n\n  * Record the [prediction error]{.primary}\n\n\n$$\\textrm{CVMSE} = \\frac{1}{N-h-t_0} \\sum_{t = t_0}^{N-h} (\\hat y_{t+h|t} - y_{t+h})^2$$\n\n* $\\hat y_{t+h|t}$ is the forecast for $y$ at time $t+h$ \n* that was made with data available up to time $t$.\n\n# A Workflow for Forecasting {.inverse}\n\n## Care with your data\n\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\n\n1. Data splitting\n    * Some data you see. You can use it to create your model: [Training data]{.primary}.\n    * Some data you don't see. It may arrive later, or you may hold it out to validate your process.\n\n::: {.fragment}\n2. Only training data can be used to create your model.\n    * Much more subtle than it sounds.\n    * [Everything]{.primary} about your model must flow from this\n        1. Choosing the model: Compartmental vs Time Series, exogenous predictors\n        1. Estimates of model parameters\n        1. How much regularization to use\n        1. Any transformations you make of your data\n:::\n\n::: {.fragment .box-text .absolute top=30%}\nBut that point about transformations is [VERY]{.primary} important. And often overlooked.\n:::\n\n## Preprocessing correctly\n\n* A standard proprecessing routine is to `scale()` each of the predictors.\n* This requires calculating the mean and standard deviation on the training data.\n* And using those values when you make predictions\n* This is hard to do with standard `R` operations.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-2|4-7|9-12\"}\ndata(Chicago, package = \"modeldata\") \nChicago <- select(Chicago, c(ridership, temp, humidity, percip))\n\nchicago_train <- Chicago |>\n  slice_head(prop = .8) |>\n  mutate(across(everything(), scale))\nmod <- lm(ridership ~ ., data = chicago_train)\n\nchicago_test <- Chicago |>\n  slice_tail(prop = .2) |>\n  mutate(across(everything(), scale))\npreds <- predict(mod, chicago_test)\n```\n:::\n\n\n\n::: {.fragment .box-text .absolute top=10%}\n* Scaled the test set with [their own]{.secondary} means and variances.\n* Should have used sample statistics from the [training set]{.secondary}\n* We didn't save the means and variances from the training set.\n* We would also need to invert (postprocess) `preds` to get them in the original units\n\n[This is all wrong]{.secondary}\n:::\n\n\n## `{tidymodels}`\n\n* The `{tidymodels}` suite of packages is intended to handle this situation correctly.\n\n* It's written by programmers at Posit (the people behind `{tidyverse}`)\n\n* It doesn't work for panel data.\n\n* That's what we need for Epidemiological Time Series\n\n* We've been working with their team to develop this functionality.\n\n![](gfx/tidymodels.png){fig-align=\"center\"}\n\n## Anatomy of a forecaster framework\n\n::: {.incremental}\n* We should build up modular components\n* Be able to add/remove layers of complexity sequentially, not all at once\n* We should be able to make processing independent of the model\n* Fitting should also be independent (`glm()` vs `lm()` vs `glmnet()`)\n* We should be able to postprocess the predictions\n* The framework shouldn't contaminate test data with training data (data leakage)\n* We should be able to access intermediate portions\n:::\n\n\n## What `{epipredict}` provides\n\nBasic and easy to use [\"canned\" forecasters]{.primary}: \n\n  * Baseline flat forecaster\n  * Autoregressive forecaster (ARX)\n  * Autoregressive classifier (also \"ARX\")\n  * CDC FluSight flatline forecaster\n  * Climatological forecaster\n  \n. . .\n  \nThese are supposed to work easily\n\n<br>\n\nHandle lots of cases we've already seen\n\n<br>\n\n[We'll start here]{.secondary}\n\n\n## What `{epipredict}` provides\n\n* A framework for creating [custom forecasters]{.primary} out of [modular]{.primary} components. \n* This is highly customizable, extends `{tidymodels}` to panel data\n* Good for building a new forecaster from scratch\n* We'll do an example at the end\n\n### There are four of components:\n\n1. [Preprocessor]{.primary}: do things to the data before model training\n1. [Trainer]{.primary}: train a model on data, resulting in a fitted model object\n1. [Predictor]{.primary}: make predictions, using a fitted model object\n1. [Postprocessor]{.primary}: do things to the predictions before returning\n  \n  \n\n## Examples of pre-processing\n\n::: {.fragment .fade-in-then-semi-out}\n\n### EDA type stuff\n\n1. Making locations/signals commensurate (scaling)\n1. Dealing with revisions \n1. Detecting and removing outliers\n1. Imputing or removing missing data\n\n:::\n\n::: {.fragment .fade-in-then-semi-out}\n\n### Feature engineering\n\n1. Creating lagged predictors\n1. Day of Week effects\n1. Rolling averages for smoothing \n1. Lagged differences\n1. Growth rates instead of raw signals\n1. The sky's the limit\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Fit `arx_forecaster()` on training set\n\n* [ARX(1)]{.primary} model for COVID Deaths per 100K (7 day average):\n$\\quad \\E[y_{t+28} | y_t,\\ x_t] \\doteq \\phi + \\phi_0 y_{t} + \\beta_0 x_{t}$\n* Only focus on California (for now)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-4|6-12\"}\nt0_date <- as.Date('2021-04-01')\ntrain <- ca |> filter(time_value <= t0_date)\ntest <- ca |> filter(time_value > t0_date)\n\nca_arx <- arx_forecaster(\n  epi_data = train |> as_epi_df(), \n  outcome = \"deaths\", \n  predictors = c(\"cases\", \"deaths\"),\n  trainer = linear_reg(),\n  args_list = arx_args_list(lags = 0, ahead = 28, quantile_levels = c(0.1, 0.9))\n)\n```\n:::\n\n\n\n## `arx_forecaster()` output\n\n* A [workflow]{.primary} object which can be used any time in the future to create forecasts (`$epi_workflow`).\n    * All necessary preprocessing; both the sequence of steps, and any necessary statistics\n    * The fitted model object\n    * The sequence of steps for postprocessing\n\n* A [forecast]{.primary} (point prediction + interval) \nfor 28 days after the last available time value in the data (`$predictions`).\n\n\n\n\n## `arx_forecaster()` output\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nca_arx\n```\n:::\n\n\n\n\n![](gfx/ca_arx.png){.codelike-format}\n\n\n\n## Extract fitted object\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nca_arx$epi_workflow\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n\n```\n══ Epi Workflow [trained] ══════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\nPostprocessor: Frosting\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n\n7 Recipe steps.\n1. step_epi_lag()\n2. step_epi_lag()\n3. step_epi_ahead()\n4. step_naomit()\n5. step_naomit()\n6. step_training_window()\n7. check_enough_data()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n (Intercept)   lag_0_cases  lag_0_deaths  \n   21.181007      0.008929      0.321781  \n\n── Postprocessor ───────────────────────────────────────────────────────────────\n\n5 Frosting layers.\n1. layer_predict()\n2. layer_residual_quantiles()\n3. layer_add_forecast_date()\n4. layer_add_target_date()\n5. layer_threshold()\n\n```\n:::\n\n\n\n\n\n## Extract predictions\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nca_arx$predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  geo_value .pred .pred_distn forecast_date target_date\n  <chr>     <dbl>   <qtls(3)> <date>        <date>     \n1 ca         105.       [105] 2021-04-01    2021-04-29 \n```\n\n\n:::\n:::\n\n\n\n::: {.fragment .callout-important icon=\"false\"}\n## Note \n\n* `.pred_distn` is actually a “distribution”, parameterized by its quantiles\n\n* `arx_forecaster` estimates the quantiles in a different way than `lm` \n:::\n\n::: {.fragment}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nca_arx$predictions |> pivot_quantiles_wider(.pred_distn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 7\n  geo_value .pred forecast_date target_date `0.1` `0.5` `0.9`\n  <chr>     <dbl> <date>        <date>      <dbl> <dbl> <dbl>\n1 ca         105. 2021-04-01    2021-04-29   29.4  105.  180.\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n## Plot predictions\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nca_arx |> autoplot() +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  geom_vline(xintercept = ca_arx$predictions$forecast_date[1], alpha = .5, color = secondary)\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-revealjs/epi-pred-autoplot-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## Split sample forecasting\n\n* `arx_forecaster()` estimates a model on the training set\n\n* Outputs only the prediction for time $t_0+h$\n\n* To get predictions for the [test set]{.secondary}:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(ca_arx$epi_workflow, test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 706 x 6 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2025-04-21 14:26:40.214048\n\n# A tibble: 706 × 6\n   geo_value time_value .pred .pred_distn forecast_date target_date\n   <chr>     <date>     <dbl>   <qtls(3)> <date>        <date>     \n 1 ca        2021-04-02 113.        [113] 2021-04-01    2021-04-29 \n 2 ca        2021-04-03  64.3      [64.3] 2021-04-01    2021-04-29 \n 3 ca        2021-04-04  50.9      [50.9] 2021-04-01    2021-04-29 \n 4 ca        2021-04-05  61.2      [61.2] 2021-04-01    2021-04-29 \n 5 ca        2021-04-06  71.9      [71.9] 2021-04-01    2021-04-29 \n 6 ca        2021-04-07  78.1      [78.1] 2021-04-01    2021-04-29 \n 7 ca        2021-04-08  89.2      [89.2] 2021-04-01    2021-04-29 \n 8 ca        2021-04-09 124.        [124] 2021-04-01    2021-04-29 \n 9 ca        2021-04-10 101.        [101] 2021-04-01    2021-04-29 \n10 ca        2021-04-11  64.7      [64.7] 2021-04-01    2021-04-29 \n# ℹ 696 more rows\n```\n\n\n:::\n:::\n\n\n\n## Time series prediction with ARX (with re-fitting)\n\n::: {.incremental}\n* To [re-train]{.secondary} the forecaster as [new data]{.secondary} arrives\n* Combine `arx_forecaster()` with `epi_slide()`\n* But that isn't version aware\n* To [REALLY]{.secondary} mimic what forecasts would have looked like\n* Slide on an `epi_archive` with `epix_slide()`\n* This is the only justifiable way to evaluate forecasting models\n* [This is the only justifiable way to evaluate forecasting models]{.secondary}\n* From now on, we will only used [versioned data]{.secondary}\n* For illustration and speed, we'll make predictions once a week (daily data)\n:::\n\n## Predict with ARX (re-fitting on trailing window)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1,13|3,12|2,6-11|5,14\"}\nfc_time_values <- seq(from = t0_date, to = as.Date(\"2023-02-09\"), by = \"1 week\")\nh <- 28         # horizon\nw <- 120 + h    # trailing window length \n\npred_arx <- ca_archive |> epix_slide(\n  ~ arx_forecaster(epi_data = .x,\n                   outcome = \"deaths\", \n                   predictors = c(\"cases\", \"deaths\"), \n                   trainer = linear_reg(),\n                   args_list = arx_args_list(lags = 0, ahead = h, quantile_levels = c(0.1, 0.9))\n  ) |> pluck(\"predictions\") |> pivot_quantiles_wider(.pred_distn),\n  .before = w, \n  .versions = fc_time_values\n)\n```\n:::\n\n\n\n## Regression on a trailing window\n\n[Fit the model on a window of data of length $w$]{.primary} \n\n* starting at $t-w$ and ending at $t$.\n\n[Advantage:]{.primary} \n\n* if the predictor-outcome relation changes over time,\n* training the forecaster on only recent data better captures the recent \nrelationship\n* potentially more relevant for near-term forecasts\n\n<br>\n\n. . .\n\n### Window length [$w$]{.primary} considerations: \n\n* if $w$ is too [big]{.secondary}, the model [can't adapt]{.primary} to the \n  recent predictors-outcome relation\n* if $w$ is too [small]{.secondary}, the fitted model may be [too volatile]{.primary} \n  (trained on too little data)\n  \n\n\n## Predict with ARX \n\n::: {.callout-important icon=\"false\"}\n## Note (window length)\n\n* Setting $w = 120 + h$ actually only uses $N=120$ observations\n\n* It filters to data within the window, then performs leads/lags\n\n* To make this explicit, for a horizon $h$, we need to \"back\" $h$ days\nto see which predictors align with it\n:::\n  \n::: {.callout-important icon=\"false\"}\n## Note (all past)\n\nTo [fitting on all past data]{.primary} up to the forecasting date use\n\n`epix_slide(...,.before = Inf)`\n:::\n\n\n\n## Predict with ARX (re-fitting on trailing window)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_arx \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 98 × 8\n   version    geo_value  .pred forecast_date target_date  `0.1`  `0.5` `0.9`\n * <date>     <chr>      <dbl> <date>        <date>       <dbl>  <dbl> <dbl>\n 1 2021-04-01 ca        0.396  2021-03-31    2021-04-28  0.192  0.396  0.599\n 2 2021-04-08 ca        0.395  2021-04-07    2021-05-05  0.197  0.395  0.594\n 3 2021-04-15 ca        0.403  2021-04-14    2021-05-12  0.211  0.403  0.595\n 4 2021-04-22 ca        0.312  2021-04-21    2021-05-19  0.142  0.312  0.482\n 5 2021-04-29 ca        0.261  2021-04-28    2021-05-26  0.0879 0.261  0.433\n 6 2021-05-06 ca        0.209  2021-05-05    2021-06-02  0.0238 0.209  0.394\n 7 2021-05-13 ca        0.158  2021-05-12    2021-06-09  0      0.158  0.345\n 8 2021-05-20 ca        0.118  2021-05-19    2021-06-16  0      0.118  0.296\n 9 2021-05-27 ca        0.0775 2021-05-26    2021-06-23  0      0.0775 0.239\n10 2021-06-03 ca        0.0552 2021-06-02    2021-06-30  0      0.0552 0.137\n# ℹ 88 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Predict with ARX (re-fitting on trailing window)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture4_files/figure-revealjs/arx-plot-cv-predictions-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|  MAE|   MASE| Coverage 80|\n|----:|------:|-----------:|\n| 0.08| 218.86|        0.43|\n\n\n:::\n:::\n\n\n\n\n\n## Customizing `arx_forecaster()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|4|5|6\"}\narx_forecaster(\n  epi_data = train, \n  outcome = \"deaths\", \n  predictors = c(\"cases\", \"deaths\"),\n  trainer = linear_reg(),\n  args_list = arx_args_list(lags = 0, ahead = 28, quantile_levels = c(0.1, 0.9))\n)\n```\n:::\n\n\n\n\n::: {.fragment}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\narx_args_list(\n  lags = c(0L, 7L, 14L),\n  ahead = 7L,\n  n_training = Inf,\n  forecast_date = NULL,\n  target_date = NULL,\n  adjust_latency = c(\"none\", \"extend_ahead\", \"extend_lags\", \"locf\"),\n  warn_latency = TRUE,\n  quantile_levels = c(0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95),\n  symmetrize = TRUE,\n  nonneg = TRUE,\n  quantile_by_key = character(0L),\n  check_enough_data_n = NULL,\n  check_enough_data_epi_keys = NULL,\n  ...\n)\n```\n:::\n\n\n:::\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Doctor visits instead of cases in predictor set\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture4_files/figure-revealjs/arx-with-dv-plot-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|  MAE|   MASE| Coverage 80|\n|----:|------:|-----------:|\n| 0.06| 169.72|        0.52|\n\n\n:::\n:::\n\n\n\n\n\n\n## Customizing `arx_forecaster()`\n\n### Multiple horizons\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-2,12|15\"}\nh <- c(7, 14, 21, 28)\nforecast_times <- seq(from = t0_date, to = as.Date(\"2023-02-23\"), by = \"1 month\")\npred_h_days_ahead <- function(epi_archive, ahead = 7) {\n  epi_archive |>\n    epix_slide(~ arx_forecaster(epi_data = .x,\n                                outcome = \"deaths\", \n                                predictors = c(\"deaths\", \"doctor_visits\"), \n                                trainer = linear_reg(),\n                                args_list = arx_args_list(lags = 0, ahead = ahead, quantile_levels = c(0.1, 0.9))\n    )|> pluck(\"predictions\") |> pivot_quantiles_wider(.pred_distn),\n  .before = w, \n  .versions = forecast_times\n  )\n}\nforecasts <- bind_rows(map(h, ~ pred_h_days_ahead(ca_archive_dv, ahead = .x)))\n```\n:::\n\n\n\n## Predictions (multiple horizons)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture4_files/figure-revealjs/arx-multiple-h-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n# Advanced Customizations {.inverse}\n\n## Changing trainer\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\narx_forecaster(epi_data = train |> as_epi_df(), \n               outcome = \"deaths\", \n               predictors = c(\"cases\", \"deaths\"),\n               trainer = linear_reg() |> set_engine(\"lm\"),\n               args_list = arx_args_list(lags = 0, ahead = 28,\n                                         quantile_levels = c(0.1, 0.9)))\n```\n:::\n\n\n\nModify `trainer` to use a model that is not `lm` (default)\n\n* e.g. `trainer = rand_forest()`\n* can use any `{parsnip}` models, see [list](https://www.tidymodels.org/find/parsnip/)\n* `{epipredict}` has a number of custom engines as well\n  \n\n## Changing trainer\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\npred_arx_rf <- ca_archive_dv |>\n  epix_slide(\n    ~ arx_forecaster(epi_data = .x,\n                     outcome = \"deaths\", \n                     predictors = c(\"deaths\", \"doctor_visits\"), \n                     trainer = parsnip::rand_forest(mode = \"regression\"), # defaults to ranger\n                     args_list = arx_args_list(lags = 0, ahead = 28, quantile_levels = c(0.1, 0.9))\n    ) |> pluck(\"predictions\") |> pivot_quantiles_wider(.pred_distn),\n  .before = w, \n  .versions = fc_time_values\n)\n```\n:::\n\n\n\n## Predictions (trained using random forest)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture4_files/figure-revealjs/arx-with-random-forests-plot-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|  MAE|   MASE| Coverage 80|\n|----:|------:|-----------:|\n| 0.08| 226.43|        0.09|\n\n\n:::\n:::\n\n\n\n::: {.absolute .box-text .fragment top=10%}\n* Random forests has really [poor coverage]{.primary} here.\n* The reason is the way intervals are calculated.\n* Can [change engine]{.primary} to get better coverage: \n\nspecify `parsnip::rand_forest(mode = \"regression\", engine = \"grf_quantiles\")`\n:::\n\n\n## Predictions from a random forest with `grf_quantiles()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture4_files/figure-revealjs/plot-arx-with-grf-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|  MAE|   MASE| Coverage 80|\n|----:|------:|-----------:|\n| 0.09| 248.24|        0.39|\n\n\n:::\n:::\n\n\n\n## What are these intervals?\n\n* `{epipredict}` takes quantiles of training residuals to form its prediction intervals\n* In comparison to traditional (parametric) intervals from `lm()`, this is more flexible\n* It can in principle adapt to asymmetric or heavy-tailed error distributions\n\n<br>\n\nTaking quantiles of training residuals can be problematic if the model is overfit. \n\n<br>\n\nQuantile regression provides an alternative, wherein we estimate these quantiles directly\n\nTechnically, `grf_quantiles()` was using Quantile Loss with Random Forests\n\n## Quantile regression\n\nNow we directly target conditional quantiles of the outcome over time. \n\nEstimating tail quantiles [requires more data]{.primary}, so\n\n  * unsuitable for settings with small training set (e.g. trailing window on one state)\n  \n  * can benefit by combination with geo-pooling (much more data to train on)\n\n\n## Geo-pooling\n\n* When we observe data over time from [multiple locations]{.primary}\n(e.g. states or counties).\n\n<br>\n\n* We could\n\n  * Estimate coefficients [separately]{.primary} for each location (as we have done so far), or\n  * Fit one model using all locations together at each time point ([geo-pooling]{.primary}).\n  * Estimated coefficients will not be location specific.\n\n<br>\n\n* We will now pool data from [all US states]{.primary} to make predictions.\n\n* Also switch to using (linear) quantile regression (medians and intervals) `quantreg::rq()`\n\n## Geo-pooling\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1|5-6\"}\npred_arx_geo_pool <- usa_archive_dv |> epix_slide(\n  ~ arx_forecaster(epi_data = .x,\n                   outcome = \"deaths\", \n                   predictors = c(\"deaths\", \"doctor_visits\"), \n                   trainer = quantile_reg(),\n                   args_list = arx_args_list(ahead = 28, quantile_levels = c(0.1, 0.9))\n  ) |> pluck(\"predictions\") |> pivot_quantiles_wider(.pred_distn),\n  .before = w, \n  .versions = fc_time_values\n)\n```\n:::\n\n\n\n[Note]{.primary}: geo-pooling is the default in `epipredict`\n\n\n\n## Predictions (geo-pooling, $h=28$)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture4_files/figure-revealjs/arx-geo-pooling-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Geo-pooling or not?\n\n* Geo-pooled predictions tend to be [more stable]{.primary} \n\n* Generally with [wider intervals]{.primary} (and better coverage)\n\n* Meanwhile, predictions from state-specific models tend to be [more volatile]{.primary}\n\nThe extent to which this occurs differs based on the horizon. \n\n\n\n# Build a Forecaster from Scratch {.inverse}\n\n## Build a forecaster from scratch\n\n* So far, we performed [manual pre-processing]{.primary}, \n\n* and then relied on a [canned forecaster]{.primary}\n\n* to automatically perform [more pre-processing]{.primary}, [training]{.primary}, [predicting]{.primary}, and [post-processing]{.primary}.\n\n\n::: {.callout-important icon=\"false\"}\n## What if we want more direct control on each single step?\n\n:::\n\n## Under the hood of `arx_forecaster()` (roughly)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-6|8-9|11-16|18-20|21-29\"}\n# A preprocessing \"recipe\" that turns raw data into features / response\nrec <- epi_recipe(ca) |>\n  step_epi_lag(cases, lag = c(0, 7, 14)) |>\n  step_epi_lag(deaths, lag = c(0, 7, 14)) |>\n  step_epi_ahead(deaths, ahead = 28) |>\n  step_epi_naomit()\n\n# Training engine\neng <- quantile_reg(quantile_levels = c(.1, .5, .9))\n\n# A post-processing routine describing what to do to the predictions\nfrost <- frosting() |>\n  layer_predict() |>\n  layer_threshold(.pred, lower = 0) |> # predictions / intervals should be non-negative\n  layer_add_target_date() |>\n  layer_add_forecast_date()\n\n# Bundle up the preprocessor, training engine, and postprocessor\n# We use quantile regression\newf <- epi_workflow(rec, eng, frost)\n\n# Fit it to data (we could fit this to ANY data that has the same format)\ntrained_ewf <- fit(ewf, data = ca)\n\n# Make predictions from the end of our training data\n# we could have made predictions using the same model on ANY test data\nfcasts <- forecast(trained_ewf)\n```\n:::\n\n\n\n## Predicting influenza test positivity \n\n* Predict test positivity for 3 pathogens (flu, RSV, covid)\n* 6 regions (ON, BC, QC, Prairies, Atlantic, Territories) + National\n* Worked with UGuelph to build [AI4Casting Hub](https://4castinghub.uoguelph.ca/respiratory-virus-detections/)\n\n. . .\n\n\n1. From November 25, 2024 until May 31, 2025\n2. Every Saturday by 11pm EDT\n3. Predict -1, 0, 1, 2, 3 epiweeks ahead\n4. Point forecast + 7 quantiles\n5. Response is [RVDSS % Test Positivity from PHAC](https://www.canada.ca/en/public-health/services/surveillance/respiratory-virus-detections-canada.html)\n\n::: {.fragment .absolute .box-text top=10%}\n* Thanks to Christine Chuong (UBC MSc Student)\n* Built and ran this forecaster\n* Scraped RVDSS data, updates every week: <https://github.com/dajmcdon/rvdss-canada/>\n:::\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Almost our production forecaster\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprod_forecaster <- function(epidata, ahead) {\n  Logit <- function(x, a = 0.01) log((x + a) / (1 - x + a))\n  Sigmd <- function(y, a = 0.01) (exp(y) * (1 + a) - a) / (1 + exp(y))\n\n  quantile_levels <- c(0.025, 0.1, 0.25, 0.5, 0.75, 0.9, 0.975)\n  forecast_date <- attr(epidata, \"metadata\")$as_of\n  target_date <- forecast_date + ahead - 7\n  r <- epi_recipe(epidata) |>\n    step_adjust_latency(recipes::all_outcomes(), fixed_forecast_date = forecast_date, method = \"extend_ahead\") |>\n    recipes::step_mutate(flu_pct_positive =  Logit(flu_pct_positive / 100)) |>\n    step_epi_lag(flu_pct_positive, lag = c(0, 7, 14)) |>\n    step_epi_ahead(flu_pct_positive, ahead = ahead) |>\n    step_climate(flu_pct_positive, forecast_ahead = ahead / 7, time_type = \"epiweek\", window_size = 3L) |>\n    step_training_window(n_recent = 12) |>\n    step_epi_naomit()\n  \n  e <- quantile_reg(quantile_levels = c(0.025, 0.1, 0.25, 0.5, 0.75, 0.9, 0.975))\n  f <- frosting() |> layer_predict() |> \n    layer_quantile_distn(quantile_levels = quantile_levels) %>% \n    layer_point_from_distn() |>\n    layer_add_forecast_date(forecast_date = forecast_date) |>\n    layer_add_target_date()\n  \n  ewf <- epi_workflow(r, e, f)\n  trained_ewf <- ewf |> fit(epidata)\n  \n  preds <- forecast(trained_ewf) |>\n    pivot_quantiles_wider(.pred_distn) |>\n    mutate(across(c(.pred, `0.025`:`0.975`), ~ Sigmd(.x) * 100)) |>\n    select(-time_value)\n\n  preds\n}\n```\n:::\n\n\n\n## Make the forecasts for all dates this season\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naheads <- 1:4 * 7\nfcast_dates <- seq(ymd(\"2024-11-23\"), ymd(\"2025-04-12\"), by = \"1 week\")\nrvdss_forecasts <- function(archive, .ahead) {\n  epix_slide(archive, ~ prod_forecaster(.x, .ahead), .versions = fcast_dates)\n}\nall_forecasts <- bind_rows(map(aheads, ~ rvdss_forecasts(rvdss_data, .ahead = .x)))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture4_files/figure-revealjs/plot-prod-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n# Advanced Topics {.inverse}\n\n## Ensembling\n\nInstead of choosing one model, we can [combine]{.primary} the predictions from multiple base models.\n\n* [untrained]{.primary}: combine base models, agnostic to past performance \n  \n* [trained]{.primary}: weight base models, accounting for past performance\n  \nSimplest untrained method: simple average of base model forecasts \n\n$$\n\\hat{y}^{\\text{avg}}_{t+h|t} = \\frac{1}{p} \\sum_{j=1}^p \\hat{y}^j_{t+h|t} \n$$\n\nA more robust option: simple median of base model forecasts\n\n$$\n\\hat{y}^{\\text{med}}_{t+h|t} = \\mathrm{median}\\Big\\{ \\hat{y}^j_{t+h|t} : j = 1,\\dots,p \\Big\\}\n$$\n\n## Example from the Covid-19 Forecast Hub  \n  \n![](gfx/cramer.png){fig-align=\"center\"}\n\n## Two key goals of ensembling\n\n<br>\n\n#### 1. Compete-with-best: \nensemble should have accuracy competitive with best individual constituent model\n\n<br>\n\n#### 2. Robustness-over-all: \nensemble should have greater robustness than any individual constituent model  \n\n<br><br>\n\n::: {.fragment}\nTypically these are hard to accomplish simultaneously, \n\n* untrained methods excel at point 2, \n* trained methods can achieve point 1\n:::\n\n\n## Linear stacking (trained ensemble)\n\n\\newcommand{\\R}{\\mathbb{R}}\n\n* directly fit a weighted combination of base forecasts to optimize accuracy (MSE, MAE, etc.), \n* often called linear stacking: e.g., to form the forecast at time $t$, we solve\n\n$$\n\\begin{aligned}\n&\\minimize_{w \\in \\R^p} \\sum_{s=t_0+1}^t \\bigg( y_s - \\sum_{j=1}^p\nw_j \\cdot \\hat{y}^j_{s|s-h} \\bigg)^2 \\\\   \n&\\text{subject to} \\quad \\sum_{j=1}^p w_j = 1, \\;\\;\\text{and} \\;\\; w_j \\geq 0, \\;\nj=1,\\dots,p\n\\end{aligned}\n$$\n\nthen use\n\n$$\n\\hat{y}^{\\text{stack}}_{t+h|t} = \\sum_{j=1}^p \\hat{w}^t_j \\cdot\n\\hat{y}^j_{t+h|t} \n$$\n\n::: {.box-text .absolute .fragment top=30%}\nThe stacking optimization problem uses forward-looking predictions \n\n(as in time series cross-validation)\n:::\n\n## Recalibration\n\n<br>\n\nPrediction intervals often have [empirical coverage $\\ll$ nominal coverage]{.secondary}\n\n* e.g., our 80% predictive intervals in practice cover $\\approx$ 60% of the time\n\n<br>\n\nRecalibration methods adjust the intervals so that \n\n* nominal coverage $\\approx$ empirical coverage\n\n## Quantile tracking\n\nProduces calibrated prediction intervals from base forecasts and scores. \n\nIn the simplest case, we can take the score to be absolute error of point forecasts:\n\n$$e_t = |y_t - \\hat y_{t|t-1}|$$\n\n* Let $\\hat q_{t}^{1-\\alpha}$ be a predicted level $1-\\alpha$ quantile of the distribution of $e_t$\n\n* Define $I_{t|t-1}^{1-\\alpha} = [\\hat{y}_{t|t-1} - \\hat{q}_t^{1-\\alpha}, \\;     \\hat{y}_{t|t-1} + \\hat{q}_t^{1-\\alpha}]$. \n\n* Note that $e_t \\leq \\hat{q}_t^{1-\\alpha} \\iff y_t \\in I_{t|t-1}^{1-\\alpha}$\n\n* Therefore we the reduced the problem of producing prediction intervals $I_{t|t-1}^{1-\\alpha}$ to one of tracking a quantile of $e_t$\n\n## Quantile updates\n\n1. begin with some estimate $\\hat{q}_{t_0+1}^{1-\\alpha}$ based on a burn-in set.\n2. Then, for a step size $\\eta > 0$, repeat the following updates as $t$ increases:\n\n$$\\hat q_{t+1}^{1-\\alpha} = \\begin{cases} \n\\hat q_{t}^{1-\\alpha} + \\eta(1-\\alpha) \\quad \\text{if } y_t\\notin I_{t|t-1}^{1-\\alpha} \\\\\n\\hat q_{t}^{1-\\alpha} - \\eta\\alpha \\quad \\quad \\quad \\,\\,\\, \\text{if } y_t\\in I_{t|t-1}^{1-\\alpha}\n\\end{cases}$$\n\nIn words: \n\n* if the latest interval does not cover, then we increase the quantile (make the next interval wider), \n* otherwise we decrease the quantile by (make the next interval narrower).\n\n::: {.fragment .box-text .absolute top=10%}\n\nThis method has the following guarantee: \n\n$$\n\\Bigg| \\frac{1}{T} \\sum_{t=t_0+1}^{t_0+T} 1 \\big\\{ y_t \\in I_{t|t-1}^{1-\\alpha} \\big\\} - (1-\\alpha) \\Bigg| \\leq \\frac{b/\\eta + 1}{T}\n$$\n\nwhere $b$ is a bound on the errors (largest error possible/observable).\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}