{
  "hash": "0a8f2a47c0de65883b6a0e2d81f57519",
  "result": {
    "engine": "knitr",
    "markdown": "---\norg: \"CANSSI Prairies &mdash; Epi Forecasting Workshop 2025\"\ntitle: \"Data Cleaning, Versioning, and Nowcasting\"\nsubtitle: \"Lecture 2\"\nshort-title: \"Nowcasting\"\nformat: revealjs\n---\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Outline\n\n1. Warmup: Examining Snapshots\n1. Signal processing with snapshots\n1. Tracking Revisions\n1. Nowcasting Using `{epiprocess}`\n1. Nowcasting with Regression\n\n\n\n## Now that you have data, what do you do with it? {.nostretch}\n\n\n![R4DS by Wickham, Çetinkaya-Rundel, and Grolemund](https://r4ds.hadley.nz/diagrams/data-science/base.png){width=600}\n\n::: {.fragment}\n### Complications\n\n* Usually panel data (multiple locations at once)\n\n* Usually accessing in real time\n\n* Data have revisions\n\n* Data are reported irregularly, `NA`'s are frequent\n\n* Individual streams have high signal-to-noise ratio\n:::\n\n::: {.fragment .box-text .absolute top=50%}\n[Result:]{.secondary} spend lots of time doing processing and dealing with corner behaviour\n:::\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## R packages we maintain to facilitate typical analyses\n\n\n![](gfx/epiverse_packages_flow.jpg)\n\n# Warmup: Examining Snapshots {.inverse}\n  \n\n## `epi_df`: snapshot of a data set\n\n* a tibble with a couple of required columns, `geo_value` and `time_value`.\n* arbitrary additional columns containing measured values, called [signals]{.secondary}\n* additional [keys]{.secondary} that index subsets (health region, `age_group`, `ethnicity`, etc.)\n\n::: {.callout-note}\n## `epi_df`\n\nRepresents a [snapshot]{.secondary} that\ncontains the most [up-to-date values]{.secondary} of the signal variables, \n[as of]{.secondary} a given time.\n:::\n\n<!--\n\n## Example, linelist to `epi_df`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbc_linelist |>\n  rename(time_value = `Reported Date`, geo_value = `Health Authority`) |>\n  count(time_value, geo_value, name = \"cases\") |>\n  as_epi_df()\n```\n:::\n\n\n\n-->\n  \n\n## `epi_df`: Snapshot of a dataset\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-3\"}\ncan_edf <- can_cases_deaths |>\n  rename(geo_value = region) |>\n  as_epi_df(as_of = \"2024-04-13\", other_keys = \"hr\")\ncan_edf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 150,951 x 5 with metadata:\n* geo_type  = nation\n* time_type = day\n* other_keys = hr\n* as_of     = 2024-04-13\n\n# A tibble: 150,951 × 5\n   geo_value hr       time_value cases deaths\n * <chr>     <chr>    <date>     <dbl>  <dbl>\n 1 AB        South    2020-03-05     0      0\n 2 AB        Calgary  2020-03-05     1      0\n 3 AB        Central  2020-03-05     0      0\n 4 AB        Edmonton 2020-03-05     0      0\n 5 AB        North    2020-03-05     0      0\n 6 AB        Other    2020-03-05     0      0\n 7 AB        South    2020-03-06     0      0\n 8 AB        Calgary  2020-03-06     0      0\n 9 AB        Central  2020-03-06     0      0\n10 AB        Edmonton 2020-03-06     0      0\n# ℹ 150,941 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n## Warm up: plotting\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-3\"}\ncan_edf |>\n  filter(geo_value == \"MB\") |>\n  autoplot(cases, deaths) +\n  scale_y_continuous(name = \"\", expand = expansion(c(0, .05))) + xlab(\"\") + scale_color_delphi(name = \"\")\n```\n\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/plot-edf-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n::: {.fragment .box-text .absolute top=20% left=20%}\nWeird reporting behaviour.\n\n* MB stopped reporting deaths by HR.\n* Put them all in \"Other\"\n* Lots of missing values\n:::\n\n## Warm up: handling missingness\n\nTwo types of missing data\n\n1. Explicit missingness means that there's an `NA`\n2. Implicit missingness means that a combination of `time_value` and `geo_value` is not in the data.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"2\"}\ncan_edf <- can_edf |>\n  complete(time_value = full_seq(time_value, period = 1), fill = list(cases = 0, deaths = 0)) \ncan_edf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 150,951 x 5 with metadata:\n* geo_type  = nation\n* time_type = day\n* other_keys = hr\n* as_of     = 2024-04-13\n\n# A tibble: 150,951 × 5\n   time_value geo_value hr                               cases deaths\n   <date>     <chr>     <chr>                            <dbl>  <dbl>\n 1 2020-01-15 ON        Algoma                               0      0\n 2 2020-01-15 ON        Brant                                0      0\n 3 2020-01-15 ON        Durham                               0      0\n 4 2020-01-15 ON        Grey Bruce                           0      0\n 5 2020-01-15 ON        Haldimand-Norfolk                    0      0\n 6 2020-01-15 ON        Haliburton, Kawartha, Pine Ridge     0      0\n 7 2020-01-15 ON        Halton                               0      0\n 8 2020-01-15 ON        Hamilton                             0      0\n 9 2020-01-15 ON        Hastings and Prince Edward           0      0\n10 2020-01-15 ON        Chatham-Kent                         0      0\n# ℹ 150,941 more rows\n```\n\n\n:::\n:::\n\n\n\n## Warm up: aggregating\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"2\"}\ncan_prov <- can_edf |>\n  sum_groups_epi_df(c(cases, deaths), group_cols = \"geo_value\")\ncan_prov\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 16,862 x 4 with metadata:\n* geo_type  = nation\n* time_type = day\n* as_of     = 2024-04-13\n\n# A tibble: 16,862 × 4\n   geo_value time_value cases deaths\n   <chr>     <date>     <dbl>  <dbl>\n 1 AB        2020-03-05     1      0\n 2 AB        2020-03-06     0      0\n 3 AB        2020-03-07     1      0\n 4 AB        2020-03-08     1      0\n 5 AB        2020-03-09     4      0\n 6 AB        2020-03-10     9      0\n 7 AB        2020-03-11     7      0\n 8 AB        2020-03-12     3      0\n 9 AB        2020-03-13     8      0\n10 AB        2020-03-14    22      0\n# ℹ 16,852 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n## Warm up: per capita scaling\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"2|3\"}\ncan_prov <- can_prov |>\n  inner_join(prov_pop, by = join_by(geo_value == region)) |>\n  mutate(case_rate = cases / pop * 1e5, death_rate = deaths / pop * 1e6) |>\n  select(-pop)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/plot-can-prov-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n::: {.fragment .absolute .box-text top=5% left=5%}\n* Negative incidence is often due to cummulatives being differenced\n* But sometimes due to correcting an error. \n* With luck, the source would make an adjustment.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 15 x 4 with metadata:\n* geo_type  = nation\n* time_type = day\n* as_of     = 2024-04-13\n\n# A tibble: 15 × 4\n   geo_value time_value cases deaths\n   <chr>     <date>     <dbl>  <dbl>\n 1 AB        2020-10-08   186     -1\n 2 AB        2021-04-26  1547     -7\n 3 AB        2021-06-21    67     -2\n 4 AB        2021-08-05   365     -4\n 5 AB        2021-12-08   354     -1\n 6 AB        2021-12-15   467     -1\n 7 AB        2022-03-15   578    -12\n 8 AB        2022-07-11   176     -4\n 9 AB        2023-01-23    50     -5\n10 AB        2023-03-13    60     -3\n11 AB        2023-03-27    40   -700\n12 NL        2021-08-25     2     -1\n13 NS        2021-05-15    86     -1\n14 SK        2020-12-28     0     -1\n15 SK        2021-04-03   281     -1\n```\n\n\n:::\n:::\n\n\n\n:::\n\n# Signal Processing with Snapshots {.inverse}\n\n## Examples of signal processing\n\n<br><br>\n\n1. Correlating signals across location or time \n1. Computing growth rates\n1. Detecting and removing outliers\n1. Calculating summaries with rolling windows\n\n\n## Correlations at different lags (province-level)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor0 <- epi_cor(can_prov, case_rate, death_rate, cor_by = geo_value)\ncor21 <- epi_cor(can_prov, case_rate, death_rate, cor_by = geo_value, dt1 = -21)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/plot-corr-lags-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n::: {.fragment .box-text .absolute top=20% left=5%}\n* Are case and death rates linearly associated across all days for each geography?\n\n* Deaths and cases likely not contemporaneous, expect cases to precede deaths.\n:::\n\n## Lag analysis more systematically\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1|2|5\"}\nlags <- 0:35\ncan_lag_cors <- map(lags, \\(l) epi_cor(can_prov, case_rate, death_rate, cor_by = geo_value, dt1 = -l)) |>\n  set_names(lags) |> \n  list_rbind(names_to = \"lag\") |>\n  summarize(mean_cor = mean(cor, na.rm = TRUE), .by = lag) |>\n  mutate(lag = as.numeric(lag))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/plot-can-cors-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n::: {.fragment .absolute .box-text left=10% top=20%}\n* Really strong weekly pattern. \n\n* But we can fix that. \n:::\n\n## Quickly compute rolling functions by group\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-2|5\"}\n# trailing by default, new names are automatically created\ncan_prov <- epi_slide_mean(can_prov, c(case_rate, death_rate), .window_size = 7L)\ncan_lag_cors <- map(\n  lags, \n  \\(l) epi_cor(can_prov, case_rate_7dav, death_rate_7dav, cor_by = geo_value, dt1 = -l)\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/can-cors-again-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Notes on lagged correlations\n\n<br>\nTrailing average pushes the correlation [backward]{.secondary}\n\n<br>\nBut weekly reporting aggregates incidence [forward]{.primary}\n\n<br>\nThese may roughly offset, but better if you know the probability of reports and\ndeconvolve  \n[more on this later]{.small .grey}\n\n<br>\nWe were only averaging over 13 provinces + territories\n\n<br>\nImplicitly assuming that reporting / testing / disease behaviour is stable over\n4 years\n\n## Compare to the US\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-2\"}\n# Only consider the 50 US states (no territories)\nus_edf <- covid_case_death_rates |> filter(geo_value %in% tolower(state.abb)) \ncor0 <- epi_cor(us_edf, case_rate, death_rate, cor_by = geo_value)\ncor21 <- epi_cor(us_edf, case_rate, death_rate, cor_by = geo_value, dt1 = -21)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/us-cor-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Compare to the US\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/sys-lag-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n::: {.fragment .box-text .absolute top=20% left=10%}\nAggregate cases tend to lead deaths by &#8776;23 days (arg max $\\rho$)\n\n<br>\nSame was true for Canada after smoothing, but lower correlation\n:::\n\n## Examining how correlations change over time\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor0 <- epi_cor(us_edf, case_rate, death_rate, cor_by = time_value, method = \"kendall\")\ncor21 <- epi_cor(us_edf, case_rate, death_rate, cor_by = time_value, method = \"kendall\", dt1 = -21)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/plot-cor-by-time-value-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Compute growth rates\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"2\"}\nedfg <- filter(can_prov, geo_value %in% c(\"MB\", \"BC\"), !is.na(case_rate_7dav)) |>\n  mutate(gr_cases = growth_rate(case_rate_7dav, time_value, method = \"linear_reg\", h = 21L), .by = geo_value)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/plot-growth-rates-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Outlier detection\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noutliers <- outliers |>\n  mutate(detect_outlr_rm(time_value, cases), .by = geo_value)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/plot-outlier-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Advanced sliding on an `epi_df`\n\n* Compute rolling summaries of signals. \n* These depend on the reference time\n* Computed separately over geographies (and other groups). \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepi_slide(\n  .x,\n  .f,\n  ..., # for tidy-evaluation\n  .window_size = NULL,\n  .align = c(\"right\", \"center\", \"left\"),\n  .ref_time_values = NULL, # at which time values do I calculate the function\n  .new_col_name = NULL, # add a new column with this name rather than the default\n  .all_rows = FALSE # do return all available time_values, or only the ones with a result\n)\n```\n:::\n\n\n::: {.fragment}\n`.f` \"sees\" a data set with a time value and other columns\n\nThat data is labeled with  \n\n1. A reference time (the time around which the window is taken)\n2. A grouping key\n:::\n\n::: {.fragment .box-text .absolute top=10%}\n* `epi_slide()` is [very]{.secondary} general, often too much so.\n* We already saw the [most]{.secondary} common special case `epi_slide_mean()`\n* For other common cases, there is `epi_slide_opt()`\n:::\n\n## Really ugly, but actually deployed slide functions\n\nFunction to flag outliers for corrections during late-2020 and early-2021\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-3,22-25|4-8|9-10,23|11-17|18-22\"}\nflag_covid_outliers <- function(signal, sig_cut = 2.75, size_cut = 20, sig_consec = 1.2) {\n  signal <- rlang::enquo(signal)\n  function(x, g, t) {\n    .fns <- list(m = ~ mean(.x, na.rm = TRUE), med = ~ median(.x, na.rm = TRUE), \n                 sd = ~ sd(.x, na.rm = TRUE), mad = ~ median(abs(.x - median(.x)))\n    )\n    fs <- filter(x, time_value <= t) |> summarise(across(!!signal, .fns, .names = \"{.fn}\"))\n    ss <- summarise(x, across(!!signal, .fns, .names = \"{.fn}\"))\n    mutate(\n      x, \n      ftstat = abs(!!signal - fs$med) / fs$sd, # mad in denominator is wrong scale, \n      ststat = abs(!!signal - ss$med) / ss$sd, # basically results in all the data flagged\n      flag = \n        (abs(!!signal) > size_cut & !is.na(ststat) & ststat > sig_cut) | # best case\n        (is.na(ststat) & abs(!!signal) > size_cut & !is.na(ftstat) & ftstat > sig_cut) | \n        # use filter if smoother is missing\n        (!!signal < -size_cut & (!is.na(ststat) | !is.na(ftstat))), # big negative\n      flag = flag | # these allow smaller values to also be outliers if they are consecutive\n        (lead(flag) & !is.na(ststat) & ststat > sig_consec) | \n        (lag(flag) & !is.na(ststat) & ststat > sig_consec) |\n        (lead(flag) & is.na(ststat) & ftstat > sig_consec) |\n        (lag(flag) & is.na(ststat) & ftstat > sig_consec)\n    ) |> filter(time_value == t) |> pull(flag)\n  }\n}\n```\n:::\n\n\n\n## Really ugly, but actually deployed slide functions\n\nFunction to back distribute data [randomly]{.secondary}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-4,11-13|5-7|14-18|8-10\"}\ncorrections_multinom_roll <- function(x, excess, flag, time_value, max_lag = 30L, reweight = exp_w) {\n  locs <- which(flag)\n  if (length(locs) == 0) return(x)\n  for (ii in locs) {\n    if (ii <= max_lag) ii_lag <- seq_len(ii)\n    else ii_lag <- seq(ii - max_lag + 1, ii)\n    w <- reweight(length(ii_lag)) / length(ii_lag)\n    x[ii] <- x[ii] - excess[ii]\n    prop <- x[ii_lag] + sign(excess[ii]) * rmultinom(1, abs(excess[ii]), w)\n    x[ii_lag] <- prop\n  }\n  x\n}\nexp_w <- function(n, std_decay = 30L, b0 = 8, a = exp(1) / 2){\n  w <- (1:std_decay) / std_decay\n  w <- tail(w, n)\n  1 / (1 + exp(-w * b0 + a))\n}\n```\n:::\n\n\n\n## Roll our outlier detector, then calculate the corrections\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-3|4\"}\ncorrections_df <- epi_slide(\n  corrections_df, .align = \"center\", .window_size = 14L, .new_col_name = \"flag\",\n  .f = flag_covid_outliers(deaths)\n) |> mutate(corrected_deaths = corrections_multinom_roll(deaths, deaths, flag, time_value), .by = geo_value)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/plot-corrections-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n# Tracking Revisions {.inverse}\n\n## `epi_archive`: Collection of `epi_df`s\n\n* Full version history of a data set\n* Acts like a bunch of `epi_df`'s --- but stored [compactly]{.secondary}\n* Similar functionality as we saw but using only [data that would have been available at the time]{.secondary}\n\n::: {.callout-note}\n## Revisions\n\nEpidemiology data gets revised frequently.\n\n* We may want to use the data [as it looked in the past]{.secondary}.\n* or we may want to examine [the history of revisions]{.secondary}.\n:::\n\n## `epi_archive`: Collection of `epi_df`s\n\nSubset of daily COVID-19 doctor visits (Optum) and cases (JHU CSSE) from all U.S. states in `archive` format:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\narchive_cases_dv_subset_all_states\n```\n\n\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs//archive-ex-ascii.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Summarize revision behaviour\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrevision_data <- revision_summary(archive_cases_dv_subset, case_rate_7d_av)\nrevision_data\n```\n\n\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs//epiprocess-revision-summary-demo.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n\n## Visualize revision patterns\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/plot-revision-patterns-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Finalized data\n\n* Counts are revised as time proceeds\n* Want to know the [final]{.secondary} value \n* Often not available until weeks/months later\n\n  \nBackcasting\n: At time $t$, predict the final value for time $t-h$, $h < 0$\n\n  <br>\n  \nNowcasting\n: At time $t$, predict the final value for time $t$\n\n<br>\n\nForecasting\n: At time $t$, predict the final value for time $t+h$, $h > 0$\n\n\n## Sliding computations over archives\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepix_slide(\n  .x,\n  .f,\n  ...,\n  .before = Inf,\n  .versions = NULL,\n  .new_col_name = NULL,\n  .all_versions = FALSE\n)\n```\n:::\n\n\n\n::: {.fragment .box-text .absolute top=10%}\n\n* To perform nowcasts we need to track how values get revised\n* To evaluate forecasting [models]{.secondary}, we need to test them on the data we\n[would have seen]{.secondary}\n\n:::\n\n\n\n# Nowcasting Using [{epiprocess}]{.monotype} {.inverse}\n\n<!-- predicting a finalized value from a provisional value and making predictions. -->\n## Why this matters\n  \n* Every week [BC CDC released COVID-19 hospitalization data](http://www.bccdc.ca/health-info/diseases-conditions/covid-19/archived-b-c-covid-19-data).\n\n* The following week, they revised the number upward (by ~25%) due to lagged reports.\n\n![](gfx/bc_hosp_admissions_ex.jpg){fig-align=\"center\"}\n\n::: {.fragment .box-text .absolute top=30%}\nComparing preliminary to revised data [often]{.secondary} shows a decline.\n\nDue to [backfill]{.secondary}\n:::\n\n## Backfill American edition - NCHS COVID-19 mortality\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## The revision triangle\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/revision-triangle-1.png){fig-align='center'}\n:::\n:::\n\n\n\n\n::: {.fragment .box-text .absolute top=10%}\n* On day $t$, predict the finalized value of signal $y_t$\n\n* We may have a provisional value for $y_t$ (subject to revision)\n\n* Most likely, we only have provisional values for earlier dates\n\n* We may only have \"finalized\" values for $y_{t-s}$, $s\\gg0$\n\n:::\n\n\n\n\n## Formal analysis of versioning behavior \n\nLatency\n: the time difference between Reference Date and Initial Report Date\n\nBackfill\n: the characteristics of updates after initial report (typical positive)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-2\"}\n# same as before, but the NCHS data\nrevision_data <- revision_summary(nchs_archive, mortality, within_latest = .05, return_only_tibble = TRUE)\nrevision_data |> filter(geo_value == \"ca\", time_value %in% nchs_versions) |> print(width = 120)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 × 11\n   time_value geo_value n_revisions min_lag max_lag  lag_near_latest spread\n   <date>     <chr>           <dbl> <drtn>  <drtn>   <drtn>           <dbl>\n 1 2024-01-07 ca                 19 1 weeks 47 weeks 10 weeks           170\n 2 2024-01-14 ca                 11 1 weeks 21 weeks  6 weeks           144\n 3 2024-01-21 ca                 12 1 weeks 29 weeks  5 weeks           111\n 4 2024-01-28 ca                 13 1 weeks 23 weeks 10 weeks           113\n 5 2024-02-04 ca                 13 1 weeks 42 weeks  9 weeks           102\n 6 2024-02-11 ca                  9 1 weeks 14 weeks  5 weeks            89\n 7 2024-02-18 ca                 10 1 weeks 37 weeks 11 weeks            81\n 8 2024-02-25 ca                 10 1 weeks 53 weeks  8 weeks            74\n 9 2024-03-03 ca                  8 1 weeks 14 weeks  8 weeks            46\n10 2024-03-10 ca                 11 1 weeks 50 weeks  9 weeks            63\n11 2024-03-17 ca                  7 1 weeks 25 weeks  8 weeks            50\n12 2024-03-24 ca                  8 1 weeks 12 weeks 10 weeks            28\n   rel_spread min_value max_value median_value\n        <dbl>     <dbl>     <dbl>        <dbl>\n 1      0.859        28       198        190. \n 2      0.754        47       191        184  \n 3      0.730        41       152        148  \n 4      0.785        31       144        136. \n 5      0.685        47       149        140. \n 6      0.856        15       104         99.5\n 7      0.779        23       104         97  \n 8      0.712        30       104         97  \n 9      0.639        26        72         67  \n10      0.851        11        74         70.5\n11      0.769        15        65         60.5\n12      0.683        13        41         37  \n```\n\n\n:::\n:::\n\n\n\n\n\n## Revision pattern visualization  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/final-vs-revisions-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Building training data for nowcasting\n\n* Can't estimate [any]{.secondary} statistical model, unless I \"see\" the response\n* The finalized value, is often [very]{.secondary} slow (~ 1 year for this signal)\n* So a compromise is to use something close, here I'm using 95% of the finalized value\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrevision_ca <- filter(revision_data, geo_value == \"ca\")\nrevision_ca |> select(geo_value, time_value, lag_near_latest) |> slice_sample(n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n  geo_value time_value lag_near_latest\n  <chr>     <date>     <drtn>         \n1 ca        2021-07-11 16 weeks       \n2 ca        2024-03-17  8 weeks       \n3 ca        2021-04-04  9 weeks       \n4 ca        2024-03-03  8 weeks       \n5 ca        2023-03-19  6 weeks       \n```\n\n\n:::\n\n```{.r .cell-code}\n(lag_quantiles <- quantile(revision_data$lag_near_latest))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime differences in weeks\n  0%  25%  50%  75% 100% \n   1    6    8   13  156 \n```\n\n\n:::\n\n```{.r .cell-code}\napprox_final_lag <- lag_quantiles[\"75%\"]\n```\n:::\n\n\n\n::: {.fragment .box-text .absolute top=20%}\n* Pretend that for time $s$, the $Y_s$ with version $s +$  13 weeks is \"final\".\n* At time $t$, the most recent data in our training set will be 13 weeks old.\n:::\n\n## What about features?\n\nMust use features that would have been available at test time.\n\nMust have enough samples to ensure sensible estimation results.\n\n1. [provisional value(s) for time $t$]{.fragment .strike}\n2. provisional value(s) for time $t-\\ell$\n3. exogenous signals that may be available\n\n::: {.fragment}\n### Potential model\n\nPredictors are \n\n* Provisional values for time $t-\\ell$, $\\ell =$ 1 week, 2 weeks when available\n* Provisional $Z_{t-k}$ for HHS/NHSN COVID-19 hospitalizations (these are daily, so different lags)\n\nExclude a potential predictor if it doesn't have much training data available.\n\n:::\n\n\n\n\n\n# Nowcasting with Regression {.inverse}\n\n\n## Operationalizing\n\n* Function needs to work on multiple nowcast dates\n* Sometimes reporting changes, so we should adjust, not error\n* If a predictor isn't available, we remove it from the model and proceed\n* Make sure we have \"enough\" training data to fit a model\n* The nowcaster needs access to all versions prior to the nowcast date\n* We want to retrain at every date: `epix_slide(..., .all_versions = TRUE)`\n* Allow for linear regression or median regression\n\n## Big ugly function \n\n* Goal: eventually refactor and put in `{epipredict}`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1,6-7|9-27|36-42|44-52|54-59\"}\nregression_nowcaster <- function(archive, model_settings, return_info = FALSE) {\n  if (!inherits(archive, \"epi_archive\")) stop(\"`archive` isn't an `epi_archive`\")\n  if (n_distinct(archive$DT$geo_value) != 1L) stop(\"Expected exactly one unique `geo_value`\")\n  if (archive$time_type == \"day\") archive <- thin_daily_to_weekly_archive(archive)\n  nowcast_date <- archive$versions_end\n  target_time_value <- nowcast_date\n  latest_edf <- archive |> epix_as_of(nowcast_date)\n\n  predictor_descriptions <-\n    latest_edf |>\n    mutate(lag_days = as.integer(nowcast_date - time_value)) |>\n    select(-c(geo_value, time_value)) |>\n    pivot_longer(-lag_days, names_to = \"varname\", values_to = \"value\") |>\n    drop_na(value) |>\n    inner_join(model_settings$predictors, by = \"varname\", unmatched = \"error\") |>\n    filter(abs(lag_days) <= max_abs_shift_days) |>\n    arrange(varname, abs(lag_days)) |>\n    group_by(varname) |>\n    filter(seq_len(n()) <= max_n_shifts[[1]]) |>\n    ungroup() |>\n    mutate(predictor_name = paste0(varname, \"_lag\", lag_days, \"_realtime\")) |>\n    select(varname, lag_days, predictor_name)\n\n  predictor_edfs <- predictor_descriptions |>\n    pmap(function(varname, lag_days, predictor_name) get_predictor_training_data(archive, varname, lag_days, predictor_name)) |>\n    lapply(na.omit) |>\n    keep(~ nrow(.x) >= model_settings$min_n_training_per_predictor)\n\n  if (length(predictor_edfs) == 0) stop(\"Couldn't find acceptable predictors in the latest data.\")\n\n  predictors <- reduce(predictor_edfs, full_join, by = c(\"geo_value\", \"time_value\"))\n  target <- latest_edf |>\n    filter(time_value <= max(time_value) - model_settings$days_until_target_semistable) |>\n    select(geo_value, time_value, mortality_semistable = mortality)\n\n  training_and_nowcast <- full_join(predictors, target, by = c(\"geo_value\", \"time_value\"))\n\n  training <- training_and_nowcast |>\n    drop_na() |>\n    slice_max(time_value, n = model_settings$max_n_training_intersection)\n\n  nowcast_features <- training_and_nowcast |> filter(time_value == nowcast_date)\n\n  form <- as.formula(\"mortality_semistable ~ .\")\n  fit_fun <- switch(\n    model_settings$method,\n    rq = function(x) { quantreg::rq(data = x, formula = form, tau = 0.5) },\n    lm = function(x) { lm(formula = form, data = x) }\n  )\n  the_fit <- training |>\n    select(any_of(predictor_descriptions$predictor_name), mortality_semistable) |>\n    fit_fun()\n      \n  pred <- tibble(\n    geo_value = \"ca\",\n    nowcast_date = nowcast_date,\n    target_date = target_time_value,\n    prediction = unname(predict(the_fit, nowcast_features))\n  )\n\n  if (return_info) return(tibble(coefficients = list(coef(fit)), predictions = list(pred)))\n  return(pred)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Model settings\n\nWe'll compare 4 different configurations:\n\n* Using `lm()` and only mortality predictors\n* Using `lm()` with mortality and hospitalizations as a predictor\n* Using `rq()` and only mortality predictors\n* Using `rq()` with mortality and hospitalizations as a predictor\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-14\"}\nreg1_settings <- list(\n  predictors = tribble(\n    ~varname,    ~max_abs_shift_days, ~max_n_shifts,\n    \"mortality\",                  35,             3,\n    ),\n  min_n_training_per_predictor = 30, # or else exclude predictor\n  days_until_target_semistable = 7 * 7, # filter out unstable when training (and evaluating)\n  min_n_training_intersection = 20, # or else raise error\n  max_n_training_intersection = Inf # or else filter down rows\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Comparison: linear regression\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/regression-nowcast-plot-linreg-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Comparison: quantile regression\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/regression-nowcast-plot-quantreg-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Evaluations\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Nowcaster       |    MAE|   MAPE|\n|:---------------|------:|------:|\n|Baseline        | 197.43|  75.28|\n|LinReg          | 172.03| 106.74|\n|LinReg + hosp   | 100.49|  58.17|\n|QuantReg        | 103.70|  48.81|\n|QuantReg + hosp |  93.33|  48.94|\n\n\n:::\n:::\n\n\n\n\n\n## Aside on nowcasting\n\n* To many Epis, [nowcasting]{.secondary} means [estimate the instantaneous \nreproduction number, $R_t$]{.secondary}\n\n* Example: Reported COVID-19 cases in British Columbia (Jan. 2020 &ndash; Apr. 2023) \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lecture2_files/figure-revealjs/rtestim-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n* More after lunch...\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}