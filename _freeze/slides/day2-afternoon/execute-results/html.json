{
  "hash": "782b4b21a6233cbfc7247a3e2c0039fd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntalk-title: \"Forecasting With `{epipredict}` and Other Advanced Topics\"\ntalk-short-title: \"Forecasting\"\ntalk-subtitle: \"InsightNet Forecasting Workshop 2024\"\ntalk-date: \"12 December -- Afternoon\"\nformat: revealjs\n---\n\n---\n---\n\n\n\\DeclareMathOperator*{\\minimize}{minimize}\n\n\n\n\n\n\n\n\n\n::: flex\n::: w-20\n\n:::\n::: w-80\n## {{< meta talk-title >}} {background-image=\"gfx/cover-art-1.svg\" background-position=\"bottom\"}\n\n### {{< meta talk-subtitle >}}\n\n<br>\n\n#### {{< meta author >}} \n[with huge thanks to Logan Brooks, Xueda Shen, and also to Nat DeFries, Dmitry Shemetov, and David Weber]{.fstyle}\n\n\n{{< meta talk-date >}}\n\n\n:::\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Outline\n\n1. Fundamentals of Forecasting\n\n1. `{epipredict}`\n\n1. Customizing `arx_forecaster()`\n\n1. Build a Forecaster from Scratch\n\n1. Advanced Topics\n\n# Fundamentals of Forecasting\n\n## Care with your data\n\n1. Data splitting\n    * Some data you see. You can use it to create your model: [Training data]{.primary}.\n    * Some data you don't see. It may arrive later, or you may hold it out to validate your process.\n\n2. Only training data can be used to create your model.\n    * Much more subtle than it sounds.\n    * [Everything]{.primary} about your model must flow from this\n        1. Choosing the model: AR vs ARX, number of lags to use\n        1. Estimates of model parameters\n        1. How much regularization to use\n        1. Any transformations you make of your data\n        \nWe've emphasized most of this already.\n\nBut that point about transformations is [VERY]{.primary} important. And often overlooked.\n\n## Preprocessing correctly\n\n* A standard proprecessing routine is to `scale()` each of the predictors.\n* This requires calculating the mean and standard deviation on the training data.\n* And using those values when you make predictions\n* This is hard to do with standard `R` operations.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nchicago_ell <- modeldata::Chicago |>\n  select(ridership, temp, humidity, percip) |>\n  mutate(across(everything(), scale))\n\n\nlm(ridership ~ ., data = chicago_ell)\n```\n:::\n\n\n\nWe didn't save the means and variances.\n\nWe need them to process the test data.\n\nWe would also need to invert (postprocess) the predictions.\n\n[For example:]{.secondary} undoing scaling to predict [deaths]{.primary} not [deaths per 100K population]{.primary}\n\n\n## `{tidymodels}`\n\n* The `{tidymodels}` suite of packages is intended to handle this situation correctly.\n\n* It's written by programmers at Posit (the people behind `{tidyverse}`)\n\n* It doesn't work for panel data.\n\n* That's what we need for Epidemiological Time Series\n\n* We've been working with their team to develop this functionality.\n\n## Anatomy of a forecaster\n\n::: {.fragment .fade-in-then-semi-out}\n\nWe should build up modular components\n\nBe able to add/remove layers of complexity sequentially, not all at once\n\nWe should be able to make preprocessing independent of the model fitting\n\nWe should be able to postprocess the predictions\n\n:::\n\n::: {.fragment .fade-in-then-semi-out}\n\n  1. [Preprocessor]{.primary}: do things to the data before model training\n  \n  1. [Trainer]{.primary}: train a model on data, resulting in a fitted model object\n\n  1. [Predictor]{.primary}: make predictions, using a fitted model object\n\n  1. [Postprocessor]{.primary}: do things to the predictions before returning\n  \n:::\n\n\n\n# `{epipredict}` \n\n## `{epipredict}` \n\n<https://cmu-delphi.github.io/epipredict>\n\n#### Installation \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Stable version\npak::pkg_install(\"cmu-delphi/epipredict@main\")\n\n\n# Development version\n# We're using this.\npak::pkg_install(\"cmu-delphi/epipredict@dev\")\n```\n:::\n\n\n\n## What `{epipredict}` provides (i)\n\nBasic and easy to use [\"canned\" forecasters]{.primary}: \n\n  * Baseline flat forecaster\n  \n  * Autoregressive forecaster (ARX)\n  \n  * Autoregressive classifier\n  \n  * CDC FluSight flatline forecaster\n  \nThese are supposed to work easily\n\n<br>\n\nHandle lots of cases we've already seen\n\n<br>\n\n[We'll start here]{.base}\n\n\n\n\n  \n## What `{epipredict}` provides (ii)\n\n* A framework for creating [custom forecasters]{.primary} out of [modular]{.primary} components. \n\n* This is highly customizable, extends `{tidymodels}` to panel data\n\n* Good for building a new forecaster from scratch\n\n* We'll do an example at the end\n\n* There are four types of components:\n\n  1. [Preprocessor]{.primary}: do things to the data before model training\n  \n  1. [Trainer]{.primary}: train a model on data, resulting in a fitted model object\n\n  1. [Predictor]{.primary}: make predictions, using a fitted model object\n\n  1. [Postprocessor]{.primary}: do things to the predictions before returning\n  \n  \n\n## Examples of pre-processing\n\n::: {.fragment .fade-in-then-semi-out}\n\n### EDA type stuff\n\n1. Making locations/signals commensurate (scaling)\n1. Dealing with revisions \n1. Detecting and removing outliers\n1. Imputing or removing missing data\n\n:::\n\n::: {.fragment .fade-in-then-semi-out}\n\n### Feature engineering\n\n1. Creating lagged predictors\n1. Day of Week effects\n1. Rolling averages for smoothing \n1. Lagged differences\n1. Growth rates instead of raw signals\n1. The sky's the limit\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Get some data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(epidatr)\nlibrary(epiprocess)\nlibrary(epipredict)\n\ncases <- pub_covidcast(\n  source = \"jhu-csse\",\n  signals = \"confirmed_incidence_num\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20200401, 20230401),\n  geo_values = \"*\") |>\n  select(geo_value, time_value, cases = value)\n\ndeaths <- pub_covidcast(\n  source = \"jhu-csse\",\n  signals = \"deaths_incidence_num\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20200401, 20230401),\n  geo_values = \"*\") |>\n  select(geo_value, time_value, deaths = value)\n\ncases_deaths <- full_join(cases, deaths, by = c(\"time_value\", \"geo_value\")) |>\n  as_epi_df()\n```\n:::\n\n\n\n\n\n## Pre-processing: data scaling\n\nScale cases and deaths by population and multiply by 100K\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncases_deaths <- left_join(\n  x = cases_deaths,\n  y = state_census |> select(pop, abbr),   # state_census is available in epipredict\n  by = join_by(geo_value == abbr)\n) |>\n  mutate(\n    cases = cases / pop * 1e5, \n    deaths = deaths / pop * 1e5\n  ) |> \n  select(-pop)\n```\n:::\n\n\n\n\n## Scaled COVID cases and deaths \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ncases_deaths |> \n  filter(geo_value %in% c(\"ca\", \"ma\", \"ny\", \"tx\")) |>\n  autoplot(cases, deaths) +\n  scale_color_delphi(name = \"\") +\n  xlab(\"Reference date\")\n```\n\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/autoplot-deaths-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Pre-processing: smoothing\n\nSmooth the data by computing 7-day averages of cases and deaths for each state\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncases_deaths <- cases_deaths |>\n  group_by(geo_value) |>\n  epi_slide(\n    cases_7dav = mean(cases, na.rm = TRUE),\n    deaths_7dav = mean(deaths, na.rm = TRUE),\n    .window_size = 7\n  ) |>\n  ungroup() |>\n  mutate(cases = NULL, deaths = NULL) |>\n  rename(cases = cases_7dav, deaths = deaths_7dav)\n```\n:::\n\n\n\n## Scaled and smoothed COVID cases deaths \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ncases_deaths |> \n  filter(geo_value %in% c(\"ca\", \"ma\", \"ny\", \"tx\")) |>\n  autoplot(cases, deaths)  +\n  scale_color_delphi(name = \"\") +\n  xlab(\"Reference date\")\n```\n\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/autoplot-7dav-deaths-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## Pre-processing: fix outliers and negative values\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndeaths_outlr <- cases_deaths |> \n  group_by(geo_value) |>\n  mutate(outlr = detect_outlr_rm(time_value, deaths, detect_negatives = TRUE)) |>\n  unnest(outlr) |>\n  ungroup()\n```\n:::\n\n::: {.cell layout-align=\"center\" width='7'}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/outliers-fig-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n\n## Fit `arx_forecaster` on training set\n\n* Back to the [ARX(1)]{.primary} model for COVID deaths:\n$\\quad \\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\beta_0 x_{t}$\n\n* Only focus on California (for now)\n\n* Using `{epipredict}`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|7-13\"}\n# split into train and test \nca <- cases_deaths |> filter(geo_value == \"ca\")\nt0_date <- as.Date('2021-04-01')\ntrain <- ca |> filter(time_value <= t0_date)\ntest <- ca |> filter(time_value > t0_date)\n\n# fit ARX\nepi_arx <- arx_forecaster(\n  epi_data = train |> as_epi_df(), \n  outcome = \"deaths\", \n  predictors = c(\"cases\", \"deaths\"),\n  trainer = linear_reg(),\n  args_list = arx_args_list(lags = 0, ahead = 28, quantile_levels = c(0.1, 0.9))\n)\n```\n:::\n\n\n\n## `arx_forecaster` output\n\n* A [workflow]{.primary} object which can be used any time in the future to create forecasts (`$epi_workflow`).\n    * All necessary preprocessing; both the sequence of steps, and any necessary statistics\n    * The fitted model object\n    * The sequence of steps for postprocessing\n\n* A [forecast]{.primary} (point prediction + interval) \nfor 28 days after the last available time value in the data (`$predictions`).\n\n\n## `arx_forecaster` output\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepi_arx \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n══ A basic forecaster of type ARX Forecaster ═══════════════════════════════════\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis forecaster was fit on 2024-12-07 16:24:26.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTraining data was an <epi_df> with:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Geography: state,\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Other keys: ,\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Time type: day,\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Using data up-to-date as of: 2024-12-07 16:23:18.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Predictions ─────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nA total of 1 prediction is available for\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• 1 unique geographic region,\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• At forecast date: 2021-04-01,\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• For target date: 2021-04-29.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n:::\n\n\n\n\n## Extract fitted object\n\n<div class=\"scrollable-output\">\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepi_arx$epi_workflow\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n══ Epi Workflow [trained] ══════════════════════════════════════════════════════\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPreprocessor: Recipe\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nModel: linear_reg()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPostprocessor: Frosting\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Preprocessor ────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n6 Recipe steps.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n1. step_epi_lag()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n2. step_epi_lag()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n3. step_epi_ahead()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n4. step_naomit()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n5. step_naomit()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n6. step_training_window()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Model ───────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n (Intercept)   lag_0_cases  lag_0_deaths  \n    0.037694      0.009953      0.201329  \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Postprocessor ───────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n5 Frosting layers.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n1. layer_predict()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n2. layer_residual_quantiles()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n3. layer_add_forecast_date()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n4. layer_add_target_date()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n5. layer_threshold()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n:::\n\n\n\n</div>\n\n## `$epi_workflow`\n\nContains information on \n\n* [Pre-processing]{.primary} steps automatically performed by `arx_forecaster` (e.g. compute lags of the predictors)\n\n* [Fitted model]{.primary} \n\n* [Post-processing]{.primary} steps automatically performed by `arx_forecaster` (e.g. compute quantiles)\n\n## Extract predictions\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepi_arx$predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  geo_value .pred        .pred_distn forecast_date target_date\n  <chr>     <dbl>             <dist> <date>        <date>     \n1 ca        0.109 quantiles(0.11)[2] 2021-04-01    2021-04-29 \n```\n\n\n:::\n:::\n\n\n\n::: {.callout-important icon=\"false\"}\n## Note \n\n* `.pred_distn` is actually a “distribution”, parameterized by its quantiles\n\n* `arx_forecaster` estimates the quantiles in a different way than `lm` \n:::\n\n\n## Extract predictions\n\nWe can extract the distribution into a “long” `epi_df`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepi_arx$predictions |>\n  pivot_quantiles_longer(.pred_distn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  geo_value .pred values quantile_levels forecast_date target_date\n  <chr>     <dbl>  <dbl>           <dbl> <date>        <date>     \n1 ca        0.109 0.0684             0.1 2021-04-01    2021-04-29 \n2 ca        0.109 0.150              0.9 2021-04-01    2021-04-29 \n```\n\n\n:::\n:::\n\n\n\nor into a \"wide\" `epi_df`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepi_arx$predictions |>\n  pivot_quantiles_wider(.pred_distn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  geo_value .pred forecast_date target_date  `0.1` `0.9`\n  <chr>     <dbl> <date>        <date>       <dbl> <dbl>\n1 ca        0.109 2021-04-01    2021-04-29  0.0684 0.150\n```\n\n\n:::\n:::\n\n\n\n\n## Predict with fitted ARX (split-sample)\n\n* `arx_forecaster` fits a model to the training set, and outputs only one prediction (for time $t_0+h$).\n\n* To get [predictions]{.primary} for the [test]{.primary} set:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(epi_arx$epi_workflow, test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 707 x 6 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-12-07 16:23:18.293021\n\n# A tibble: 707 × 6\n   geo_value time_value  .pred        .pred_distn forecast_date target_date\n * <chr>     <date>      <dbl>             <dist> <date>        <date>     \n 1 ca        2021-04-02 0.106  quantiles(0.11)[2] 2021-04-01    2021-04-29 \n 2 ca        2021-04-03 0.101   quantiles(0.1)[2] 2021-04-01    2021-04-29 \n 3 ca        2021-04-04 0.0984  quantiles(0.1)[2] 2021-04-01    2021-04-29 \n 4 ca        2021-04-05 0.100   quantiles(0.1)[2] 2021-04-01    2021-04-29 \n 5 ca        2021-04-06 0.0993  quantiles(0.1)[2] 2021-04-01    2021-04-29 \n 6 ca        2021-04-07 0.0976  quantiles(0.1)[2] 2021-04-01    2021-04-29 \n 7 ca        2021-04-08 0.0975  quantiles(0.1)[2] 2021-04-01    2021-04-29 \n 8 ca        2021-04-09 0.0979  quantiles(0.1)[2] 2021-04-01    2021-04-29 \n 9 ca        2021-04-10 0.104   quantiles(0.1)[2] 2021-04-01    2021-04-29 \n10 ca        2021-04-11 0.106  quantiles(0.11)[2] 2021-04-01    2021-04-29 \n# ℹ 697 more rows\n```\n\n\n:::\n:::\n\n\n\n## Predict with ARX (when re-fitting)\n\n* In practice, if we want to [re-train]{.primary} the forecasters as [new data]{.primary} arrive,\nwe fit and predict combining `arx_forecaster` with `epix_slide`\n\n* From now on, we will only used [versioned data]{.primary}, and make predictions once a week\n\n## Predict with ARX (re-fitting on trailing window)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"9-14|\"}\nh <- 28         # horizon\nw <- 120 + h    # trailing window length\n\n# Specify the forecast dates\nfc_time_values <- seq(from = t0_date, to = as.Date(\"2023-02-09\"), by = \"1 week\")\n\n# Slide the arx_forecaster over the epi_archive\npred_arx <- ca_archive |> epix_slide(\n  ~ arx_forecaster(epi_data = .x,\n                   outcome = \"deaths\", \n                   predictors = c(\"cases\", \"deaths\"), \n                   trainer = linear_reg(),\n                   args_list = arx_args_list(lags = 0, ahead = h, quantile_levels = c(0.1, 0.9))\n  )$predictions |>\n    pivot_quantiles_wider(.pred_distn),\n  .before = w, \n  .versions = fc_time_values\n)\n```\n:::\n\n\n\n## Predict with ARX \n\n::: {.callout-important icon=\"false\"}\n## Note (window length)\n\nWe set $w = 120 + h$ to match the window size of the ARX model we fitted manually.\n\nWhen considering a window from $t-w$ to $t$, \nwe had access to all outcomes in that window, and to all predictors between \n$t-w-h$ and $t-h$. \n\n(That's because we lagged $x$ before applying the window.) \n\nSo we were \"cheating\" by saying that \nthe trailing window had length $w=120$, as its actual size was $120+h$! \n:::\n  \n::: {.callout-important icon=\"false\"}\n## Note (all past)\n\nThe method [fitting on all past data]{.primary} up to the forecasting date can be \nimplemented by setting:\n\n`.before = Inf` in `epix_slide()`.\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Predict with ARX (re-fitting on trailing window)\n\n<div class=\"large-output\">\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_arx \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 98 × 7\n   version    geo_value  .pred forecast_date target_date  `0.1` `0.9`\n * <date>     <chr>      <dbl> <date>        <date>       <dbl> <dbl>\n 1 2021-04-01 ca        0.396  2021-03-31    2021-04-28  0.192  0.599\n 2 2021-04-08 ca        0.395  2021-04-07    2021-05-05  0.197  0.594\n 3 2021-04-15 ca        0.403  2021-04-14    2021-05-12  0.211  0.595\n 4 2021-04-22 ca        0.312  2021-04-21    2021-05-19  0.142  0.482\n 5 2021-04-29 ca        0.261  2021-04-28    2021-05-26  0.0879 0.433\n 6 2021-05-06 ca        0.209  2021-05-05    2021-06-02  0.0238 0.394\n 7 2021-05-13 ca        0.158  2021-05-12    2021-06-09  0      0.345\n 8 2021-05-20 ca        0.118  2021-05-19    2021-06-16  0      0.296\n 9 2021-05-27 ca        0.0775 2021-05-26    2021-06-23  0      0.239\n10 2021-06-03 ca        0.0552 2021-06-02    2021-06-30  0      0.137\n# ℹ 88 more rows\n```\n\n\n:::\n:::\n\n\n\n</div>\n\n## Predict with ARX (re-fitting on trailing window)\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/arx-plot-cv-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n       MAE     MASE  Coverage\n 0.1077706 722.6583 0.3367347\n```\n\n\n:::\n:::\n\n\n\n\n## Customizing `arx_forecaster()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|4\"}\narx_forecaster(\n  epi_data = train, \n  outcome = \"deaths\", \n  predictors = c(\"cases\", \"deaths\"),\n  trainer = linear_reg() |> set_engine(\"lm\"),\n  args_list = arx_args_list(lags = 0, ahead = 28, quantile_levels = c(0.1, 0.9))\n)\n```\n:::\n\n\n\n::: {.fragment .fade-in}\n* Modify `predictors` to add/drop predictors \n\n  * <span class=\"inner-list\">e.g. drop `deaths` for regression with a \n  lagged predictor, or drop `cases` to get AR model</span>\n\n  * <span class=\"inner-list\">default: `predictors = outcome`</span>\n\n:::  \n  \n\n## Customizing `arx_forecaster()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"6-7\"}\narx_forecaster(\n  epi_data = train, \n  outcome = \"deaths\", \n  predictors = c(\"cases\", \"deaths\"),\n  trainer = linear_reg(),\n  args_list = arx_args_list(lags = 0, ahead = 28, quantile_levels = c(0.1, 0.9))\n)\n```\n:::\n\n\n\n* Modify `arx_args_list` to change lags, horizon, quantile levels, ...\n\n::: {.fragment .fade-in}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\narx_args_list(\n  lags = c(0L, 7L, 14L),\n  ahead = 7L,\n  n_training = Inf,\n  forecast_date = NULL,\n  target_date = NULL,\n  adjust_latency = c(\"none\", \"extend_ahead\", \"extend_lags\", \"locf\"),\n  warn_latency = TRUE,\n  quantile_levels = c(0.05, 0.95),\n  symmetrize = TRUE,\n  nonneg = TRUE,\n  quantile_by_key = character(0L),\n  check_enough_data_n = NULL,\n  check_enough_data_epi_keys = NULL,\n  ...\n)\n```\n:::\n\n\n:::\n\n## Customizing `arx_forecaster`\n\n### Change predictors: doctor visits instead of cases\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndv_archive <- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20200401, 20230401),\n  geo_values = \"*\",\n  issues = epirange(20200401, 20230401)) |>\n  select(geo_value, time_value, version = issue, doctor_visits = value) |>\n  arrange(geo_value, time_value) |>\n  as_epi_archive(compactify = FALSE)\n```\n:::\n\n\n\n## Customizing `arx_forecaster`\n\n### Change predictors: doctor visits instead of cases\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\npred_arx_hosp <- ca_archive_dv |> epix_slide(\n  ~ arx_forecaster(epi_data = .x,\n                   outcome = \"deaths\", \n                   predictors = c(\"deaths\", \"doctor_visits\"), \n                   trainer = linear_reg(),\n                   args_list = arx_args_list(lags = 0, ahead = 28, quantile_levels = c(0.1, 0.9))\n  )$predictions |>\n    pivot_quantiles_wider(.pred_distn),\n  .before = w, \n  .versions = fc_time_values\n)\n```\n:::\n\n\n\n## Predictions (doctor visits instead of cases in predictor set)\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/arx-with-dv-plot-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n        MAE     MASE  Coverage\n 0.08215756 550.9094 0.4285714\n```\n\n\n:::\n:::\n\n\n\n\n## Customizing `arx_forecaster`\n\n### Add more lags\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"7\"}\npred_arx_more_lags <- ca_archive_dv |> epix_slide(\n  ~ arx_forecaster(epi_data = .x,\n                   outcome = \"deaths\", \n                   predictors = c(\"deaths\", \"doctor_visits\"), \n                   trainer = linear_reg(),\n                   args_list = arx_args_list(\n                     lags = c(0, 7, 14), \n                     ahead = 28, quantile_levels = c(0.1, 0.9)\n                   )\n  )$predictions |>\n    pivot_quantiles_wider(.pred_distn),\n  .before = w, \n  .versions = fc_time_values\n)\n```\n:::\n\n\n\n## Predictions (more lags)\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/arx-with-more-lags-plot-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n        MAE    MASE  Coverage\n 0.09623067 645.277 0.2857143\n```\n\n\n:::\n:::\n\n\n\n\n\n## Customizing `arx_forecaster`\n\n### Multiple horizons\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-2,11,18|19-20\"}\nforecast_times <- seq(from = t0_date, to = as.Date(\"2023-02-23\"), by = \"1 month\")\npred_h_days_ahead <- function(epi_archive, ahead = 7) {\n  epi_archive |>\n    epix_slide(\n      ~ arx_forecaster(epi_data = .x,\n                       outcome = \"deaths\", \n                       predictors = c(\"deaths\", \"doctor_visits\"), \n                       trainer = linear_reg() |> set_engine(\"lm\"),\n                       args_list = arx_args_list(\n                         lags = 0,  \n                         ahead = ahead,\n                         quantile_levels = c(0.1, 0.9))\n      )$predictions |> \n        pivot_quantiles_wider(.pred_distn),\n  .before = w, \n  .versions = forecast_times\n  )\n}\nh <- c(7, 14, 21, 28)\nforecasts <- bind_rows(map(h, ~ pred_h_days_ahead(ca_archive_dv, ahead = .x)))\n```\n:::\n\n\n\n## Predictions (multiple horizons)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/arx-multiple-h-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n# Advanced Customizations\n\n## Changing trainer\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\narx_forecaster(epi_data = train |> as_epi_df(), \n               outcome = \"deaths\", \n               predictors = c(\"cases\", \"deaths\"),\n               trainer = linear_reg() |> set_engine(\"lm\"),\n               args_list = arx_args_list(lags = 0, ahead = 28,\n                                         quantile_levels = c(0.1, 0.9)))\n```\n:::\n\n\n\nModify `trainer` to use a model that is not `lm` (default)\n\n* e.g. `trainer = rand_forest()`\n* can use any `{parsnip}` models, see [list](https://www.tidymodels.org/find/parsnip/)\n* `{epipredict}` has a number of custom engines as well\n  \n\n## Changing trainer\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\npred_arx_rf <- ca_archive_dv |>\n  epix_slide(\n    ~ arx_forecaster(epi_data = .x,\n                     outcome = \"deaths\", \n                     predictors = c(\"deaths\", \"doctor_visits\"), \n                     trainer = parsnip::rand_forest(mode = \"regression\"), # defaults to ranger\n                     args_list = arx_args_list(\n                       lags = 0,\n                       ahead = 28,\n                       quantile_levels = c(0.1, 0.9))\n                     )$predictions |>\n        pivot_quantiles_wider(.pred_distn),\n  .before = w, \n  .versions = fc_time_values\n)\n```\n:::\n\n\n\n## Predictions (trained using random forest)\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/arx-with-random-forests-plot-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n        MAE     MASE  Coverage\n 0.09287021 622.7434 0.1122449\n```\n\n\n:::\n:::\n\n\n\n## Warning!\n\n* Random forests has really [poor coverage]{.primary} here.\n\n* The reason is the way intervals are calculated.\n\n* Can [change engine]{.primary} to get better coverage: \n\nspecify `engine = \"grf_quantiles\"` in the `rand_forest` call\n\n## Predictions from a random forest with `grf_quantiles`\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/arx-with-grf-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n        MAE     MASE  Coverage\n 0.08871281 297.4329 0.4081633\n```\n\n\n:::\n:::\n\n\n\n\n## Geo-pooling\n\n* When we observe data over time from [multiple locations]{.primary}\n(e.g. states or counties).\n\n<br>\n\n* We could\n\n  * Estimate coefficients [separately]{.primary} for each location (as we have done so far), or\n  * Fit one model using all locations together at each time point ([geo-pooling]{.primary}).\n  * Estimated coefficients will not be location specific.\n\n<br>\n\n* We will now pool data from [all US states]{.primary} to make predictions.\n\n## Geo-pooling\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1\"}\npred_arx_geo_pool <- usa_archive_dv |> epix_slide(\n  ~ arx_forecaster(epi_data = .x,\n                   outcome = \"deaths\", \n                   predictors = c(\"deaths\", \"doctor_visits\"), \n                   args_list = arx_args_list(lags = 0, ahead = 28, quantile_levels = c(0.1, 0.9))\n  )$predictions |>\n    pivot_quantiles_wider(.pred_distn),\n  .before = w, \n  .versions = fc_time_values\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Predictions (geo-pooling, $h=28$)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/arx-geo-pooling-plot-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n         MAE      MASE  Coverage\nCA 0.1978253 1326.5220 0.6734694\nMA 0.1265703  626.3286 0.8762887\nNY 0.1643945  810.2403 0.8247423\nTX 0.1883987  807.3143 0.7422680\n```\n\n\n:::\n:::\n\n\n\n\n## Predictions (without geo-pooling, $h=28$)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/arx-no-geo-pooling-plot-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n          MAE      MASE  Coverage\nCA 0.08327042  558.3717 0.4285714\nMA 0.32752437 1608.4125 0.3571429\nNY 0.20184005  989.1230 0.4897959\nTX 0.13126701  568.3564 0.3673469\n```\n\n\n:::\n:::\n\n\n\n## Geo-pooling or not?\n\n* Geo-pooled predictions tend to be [more stable]{.primary} \n\n* Generally with [wider intervals]{.primary} (and better coverage)\n\n* Meanwhile, predictions from state-wise models tend to be [more volatile]{.primary}\n\nThe extent to which this occurs differs based on the horizon. \n\nPreviously we studied $h=28$. What happens for $h=7$?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Predictions (geo-pooling, $h = 7$)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/arx-geo-pooling-plot-h7-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n         MAE      MASE  Coverage\nCA 0.1725752 1144.9025 0.6020408\nMA 0.1216297  616.0302 0.8247423\nNY 0.1412645  695.6945 0.6804124\nTX 0.1625477  687.5675 0.6701031\n```\n\n\n:::\n:::\n\n\n\n\n## Predictions (without geo-pooling, $h=7$)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/arx-no-geo-pooling-plot-h7-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n          MAE     MASE  Coverage\nCA 0.08043761 533.6411 0.2959184\nMA 0.12069048 617.6409 0.2653061\nNY 0.10191169 507.1193 0.2551020\nTX 0.11461273 486.1615 0.4285714\n```\n\n\n:::\n:::\n\n\n\n## What are these ARX intervals?\n\n* `{epipredict}` takes quantiles of training residuals to form its prediction intervals\n* In comparison to traditional (parametric) intervals from `lm()`, this is more flexible\n* It can in principle adapt to asymmetric or heavy-tailed error distributions\n\n<br>\n\nTaking quantiles of training residuals can be problematic if the model is overfit. \n\n<br>\n\nQuantile regression provides an alternative, wherein we estimate these quantiles directly\n\nTechnically, `grf_quantiles` was using Quantile Loss with Random Forests\n\n## Quantile regression\n\nNow we directly target conditional quantiles of the outcome over time. \n\nEstimating tail quantiles [requires more data]{.primary}, so\n\n  * unsuitable for settings with small training set (e.g. trailing window on one state)\n  \n  * can benefit by combination with geo-pooling (much more data to train on)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|8\"}\nlibrary(quantreg)\n\npred_qr_geo_pool <- usa_archive_dv |>\n  epix_slide(\n    ~ arx_forecaster(epi_data = .x,\n                     outcome = \"deaths\", \n                     predictors = c(\"deaths\", \"doctor_visits\"), \n                     trainer = quantile_reg(),\n                     args_list = arx_args_list(\n                       lags = 0, #c(0, 7, 14),\n                       ahead = 28,\n                       quantile_levels = c(0.1, 0.9))\n                     )$predictions |>\n        pivot_quantiles_wider(.pred_distn),\n  .before = w, \n  .versions = fc_time_values\n)\n```\n:::\n\n\n\n## Predictions (geo-pooling + quantile regression, $h=28$)\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/qr-geo-pooling-plot-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n         MAE      MASE  Coverage\nCA 0.2086903 1399.3774 0.5510204\nMA 0.1280667  633.7333 0.7731959\nNY 0.1695396  835.5988 0.6288660\nTX 0.1921947  823.5808 0.5773196\n```\n\n\n:::\n:::\n\n\n\n\n# Build a forecaster from scratch\n\n## Build a forecaster from scratch\n\n* So far, we performed [manual pre-processing]{.primary}, \n\n* and then relied on a [canned forecaster]{.primary}\n\n* to automatically perform [more pre-processing]{.primary}, [training]{.primary}, [predicting]{.primary}, and [post-processing]{.primary}.\n\n\n::: {.callout-important icon=\"false\"}\n## What if we want more direct control on each single step?\n\n:::\n\n## Under the hood of `arx_forecaster()\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-6|8-9|11-16|18-20|21-29\"}\n# A preprocessing \"recipe\" that turns raw data into features / response\nrec <- epi_recipe(ca) |>\n  step_epi_lag(cases, lag = c(0, 7, 14)) |>\n  step_epi_lag(deaths, lag = c(0, 7, 14)) |>\n  step_epi_ahead(deaths, ahead = 28) |>\n  step_epi_naomit()\n\n# Training engine\neng <- quantile_reg(quantile_levels = c(.1, .5, .9))\n\n# A post-processing routine describing what to do to the predictions\nfrost <- frosting() |>\n  layer_predict() |>\n  layer_threshold(.pred, lower = 0) |> # predictions / intervals should be non-negative\n  layer_add_target_date() |>\n  layer_add_forecast_date()\n\n# Bundle up the preprocessor, training engine, and postprocessor\n# We use quantile regression\newf <- epi_workflow(rec, eng, frost)\n\n# Fit it to data (we could fit this to ANY data that has the same format)\ntrained_ewf <- fit(ewf, data = ca)\n\n# Make predictions from the end of our training data\nfcasts <- forecast(trained_ewf)\n\n# we could have made predictions using the same model on ANY test data\n```\n:::\n\n\n\n## Predicting influenza hospitalizations\n\n* Current task: predict influenza hospitalizations for all states + DC + PR.\n* Forecasts submitted to [CDC Flusight Forecast Hub](https://github.com/cdcepi/FluSight-forecast-hub)\n\nSpecifically:\n\n1. From November 20, 2024 until May 31, 2025\n2. Every Wednesday at 11pm EDT\n3. Predict 0, 1, 2, 3 epiweeks ahead\n4. Point forecast + 23 quantiles\n5. Response is [NHSN Weekly Hospitalizations](https://data.cdc.gov/Public-Health-Surveillance/Weekly-Hospital-Respiratory-Data-HRD-Metrics-by-Ju/mpgq-jmmr/about_data)\n\n\n## Aside: data issues\n\n* Hospital reporting was down for a period over the summer.\n\n* The current data doesn't seem to match the historical data very well.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## We don't know if we trust the data yet enough\n\n* It may get revised significantly\n\n* Let's do something super simple, until we're more confident\n\nClimatological forecaster\n: For a given epiweek, predict the historical quantiles\n: Make adjustments to address the fact that we have some new data\n: Privledge the history\n\nThink like the weather: \"what is the typical weather in February in Georgia, that's our forecast\"\n\n\n\n## Climatological forecaster\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclimatological_model <- function(epi_data, forecast_date, ahead, window_size = 3, geo_agg = FALSE) {\n  forecast_week <- epiweek(forecast_date)\n  last_date_data <- max(epi_data$time_value)\n  probs <- c(.1, .5, .9)\n  filtered <- epi_data |> \n    filter(\n      (season != \"2020/21\") & (season != \"2021/22\"), # drop weird years\n      # keep data either within the window, or within the past window weeks\n      (abs(forecast_week + ahead - epiweek) <= window_size) |\n        (last_date_data - time_value <= window_size * 7)\n    )\n  if (geo_agg) {\n    filtered <- filtered |>\n      left_join(state_census |> select(geo_value = abbr, pop), by = \"geo_value\") |>\n      mutate(nhsn = nhsn / pop * 1e5) %>%\n      select(geo_value, epiweek, epiyear, season, season_week, nhsn, pop)\n  } else {\n    filtered <- filtered |> group_by(geo_value)\n  }\n  naive_preds <- filtered |> reframe(enframe(\n    quantile(nhsn, probs = probs, na.rm = TRUE, type = 8), name = \"quantile\"\n  )) |>\n    mutate(\n      forecast_date = forecast_date,\n      target_end_date = forecast_date + ahead * 7,\n      quantile = as.numeric(sub(\"%\", \"\", quantile)) / 100\n    )\n  if (!geo_agg) {\n    naive_preds <- naive_preds |> group_by(geo_value)\n  }\n  naive_preds <- naive_preds |> mutate(value = pmax(0, value))\n  if (geo_agg) {\n    naive_preds <- naive_preds |>\n      expand_grid(filtered |> distinct(geo_value, pop)) |>\n      mutate(value = value * pop / 1e5) |>\n      select(-pop) |>\n      select(geo_value, forecast_date, target_end_date, quantile, value) |>\n      arrange(geo_value, forecast_date, target_end_date)\n  }\n  naive_preds |> ungroup() |> mutate(value = pmax(0, value))\n}\n```\n:::\n\n\n\n## Climate predictions for this week\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Flu data archive\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-01-01 / 2024-03-20\nℹ First/last version with update: 2023-10-04 / 2024-03-27\nℹ Versions end: 2024-03-27\nℹ A preview of the table (14420 rows x 4 columns):\nKey: <geo_value, time_value, version>\n          version geo_value time_value   hhs\n           <Date>    <char>     <Date> <num>\n    1: 2023-10-04        ak 2020-07-15    67\n    2: 2023-10-04        ak 2020-07-22   120\n    3: 2023-10-04        ak 2020-07-29    99\n    4: 2023-10-04        ak 2020-08-05   108\n    5: 2023-10-04        ak 2020-08-12    76\n   ---                                      \n14416: 2024-03-20        wy 2024-03-06    51\n14417: 2024-03-27        wy 2024-03-06    47\n14418: 2024-03-20        wy 2024-03-13    58\n14419: 2024-03-27        wy 2024-03-13    43\n14420: 2024-03-27        wy 2024-03-20    39\n```\n\n\n:::\n:::\n\n\n\n## Build forecaster\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# A preprocessing \"recipe\" that turns raw data into features / response\nr <- epi_recipe(flu) |>\n  #drop_non_seasons() |>\n  step_population_scaling(\n    hhs,\n    df = epidatasets::state_census,\n    df_pop_col = \"pop\",\n    create_new = FALSE,\n    rate_rescaling = 1e5,\n    by = c(\"geo_value\" = \"abbr\")) |>\n  step_epi_lag(hhs, lag = c(0, 7, 14)) |>\n  step_epi_ahead(hhs, ahead = 14) |>\n  step_epi_naomit()\n\n# Training engine\ne <- quantile_reg(quantile_levels = c(0.01, 0.025, 1:19 / 20, 0.975, 0.99)) # 23 ForecastHub quantiles\n\n# A post-processing routine describing what to do to the predictions\nf <- frosting() |>\n  layer_predict() |>\n  layer_threshold(.pred, lower = 0) |> # predictions / intervals should be non-negative\n  layer_population_scaling(\n    .pred, \n    df = epidatasets::state_census,\n    df_pop_col = \"pop\",\n    create_new = FALSE,\n    rate_rescaling = 1e5,\n    by = c(\"geo_value\" = \"abbr\")) |>\n  layer_add_target_date() |>\n  layer_add_forecast_date()\n\n# Bundle up the preprocessor, training engine, and postprocessor\n# We use quantile regression\newf <- epi_workflow(r, e, f)\n\n# Fit it to data (we could fit this to ANY data that has the same format)\ntrained_ewf <- ewf |> fit(flu)\n\n# examines the recipe to determine what we need to make the prediction\nlatest <- get_test_data(r, flu)\n\n# we could make predictions using the same model on ANY test data\npreds <- trained_ewf |> predict(new_data = latest)\n```\n:::\n\n\n\n## Predictions at one forecast date\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 27\n   geo_value time_value target_date forecast_date `0.01` `0.025` `0.05` `0.1`\n   <chr>     <date>     <date>      <date>         <dbl>   <dbl>  <dbl> <dbl>\n 1 al        2023-11-08 2023-11-22  2023-11-08      218.    218.   228.  242.\n 2 az        2023-11-08 2023-11-22  2023-11-08      706.    706.   741.  837.\n 3 ca        2023-11-08 2023-11-22  2023-11-08     1823.   1823.  1917. 2019.\n 4 ga        2023-11-08 2023-11-22  2023-11-08      527.    527.   563.  596.\n 5 hi        2023-11-08 2023-11-22  2023-11-08      138.    138.   152.  161.\n 6 il        2023-11-08 2023-11-22  2023-11-08      939.    939.  1015. 1109.\n 7 in        2023-11-08 2023-11-22  2023-11-08      298.    298.   316.  333.\n 8 ks        2023-11-08 2023-11-22  2023-11-08      169.    169.   175.  192.\n 9 ky        2023-11-08 2023-11-22  2023-11-08      281.    281.   304.  328.\n10 mi        2023-11-08 2023-11-22  2023-11-08      435.    435.   457.  481.\n# ℹ 11 more rows\n# ℹ 19 more variables: `0.15` <dbl>, `0.2` <dbl>, `0.25` <dbl>, `0.3` <dbl>,\n#   `0.35` <dbl>, `0.4` <dbl>, `0.45` <dbl>, `0.5` <dbl>, `0.55` <dbl>,\n#   `0.6` <dbl>, `0.65` <dbl>, `0.7` <dbl>, `0.75` <dbl>, `0.8` <dbl>,\n#   `0.85` <dbl>, `0.9` <dbl>, `0.95` <dbl>, `0.975` <dbl>, `0.99` <dbl>\n```\n\n\n:::\n:::\n\n\n\n## Slide forecaster\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nflu_forecast <- function(epi_archive, forecast_date, ahead = 14) {\n  flu <- epi_archive$DT |> \n    filter(version == forecast_date) |>\n    as_epi_df()\n\n  r <- epi_recipe(flu) |>\n    #drop_non_seasons() |>\n    step_population_scaling(\n      hhs,\n      df = epidatasets::state_census,\n      df_pop_col = \"pop\",\n      create_new = FALSE,\n      rate_rescaling = 1e5,\n      by = c(\"geo_value\" = \"abbr\")) |>\n    step_epi_lag(hhs, lag = c(0, 7, 14)) |>\n    step_epi_ahead(hhs, ahead = ahead) |>\n    step_epi_naomit()\n  \n  ewf <- epi_workflow(r, e, f)\n  trained_ewf <- ewf |> fit(flu)\n  latest <- get_test_data(r, flu)\n  preds <- trained_ewf |> predict(new_data = latest)\n  return(preds)\n}\n\nforecast_dates <- seq.Date(as.Date(\"2023-10-04\"), as.Date(\"2024-03-27\"), by = 7L)\nforecasts <- bind_rows(map(forecast_dates, \n                           ~ flu_forecast(weekly_archive, forecast_date = .x, ahead = 14)))\n```\n:::\n\n\n\n## Version-aware predictions\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-afternoon_files/figure-revealjs/plot-flu-predictions-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n# Advanced Topics\n\n## Ensembling\n\nInstead of choosing one model, we can [combine]{.primary} the predictions from multiple base models. Ensemble types:\n\n* [untrained]{.primary}: combine base models, agnostic to past performance \n  \n* [trained]{.primary}: weight base models, accounting for past performance\n  \nSimplest untrained method: simple average of base model forecasts \n\n$$\n\\hat{y}^{\\text{avg}}_{t+h|t} = \\frac{1}{p} \\sum_{j=1}^p \\hat{y}^j_{t+h|t} \n$$\n\nA more robust option: simple median of base model forecasts\n\n$$\n\\hat{y}^{\\text{med}}_{t+h|t} = \\mathrm{median}\\Big\\{ \\hat{y}^j_{t+h|t} : j = 1,\\dots,p \\Big\\}\n$$\n\n## Example from the Covid-19 Forecast Hub  \n  \n![](gfx/cramer.png)\n\n## Two key goals of ensembling\n\n1 [Compete-with-best]{.primary}: ensemble should have accuracy competitive with best individual constituent model\n\n2. [Robustness-over-all]{.primary}: ensemble should have greater robustness than any individual constituent model  \n\nTypically these are hard to accomplish simultaneously, and untrained methods excel at point 2, whereas trained methods can achieve point 1\n\n## Linear stacking\n\nOne of the simplest trained ensemble methods is to directly fit a weighted \ncombination of base forecasts to optimize accuracy (MSE, MAE, etc.), often \ncalled linear stacking: e.g., to form the forecast at time $t$, we solve\n\n\\begin{alignat*}{2}\n&\\min_{w \\in \\R^p} && \\hspace{-6pt} \\sum_{s=t_0+1}^t \\bigg( y_s - \\sum_{j=1}^p\nw_j \\cdot \\hat{y}^j_{s|s-h} \\bigg)^2 \\\\   \n&\\st \\quad && \\sum_{j=1}^p w_j = 1, \\;\\;\\text{and} \\;\\; w_j \\geq 0, \\;\nj=1,\\dots,p   \n\\end{alignat*}\n\nthen use\n\n$$\n\\hat{y}^{\\text{stack}}_{t+h|t} = \\sum_{j=1}^p \\hat{w}^t_j \\cdot\n\\hat{y}^j_{t+h|t} \n$$\n\nNote that the stacking optimization problem uses forward-looking predictions (as\nin time series cross-validation)\n\n## Recalibration\n\n* We have seen that prediction intervals often have [empirical coverage << nominal coverage]{.primary}, e.g., our 80% predictive intervals in practice cover $\\approx$ 60% of the time\n\n* Recalibration methods aim at adjusting the intervals so that nominal coverage $\\approx$ empirical coverage\n\n## Quantile tracking\n\nQuantile tracking is a method for producing calibrated prediction intervals \nfrom base forecasts and scores. In the simplest case, we can take the score\nto be absolute error of point forecasts:\n\n$$e_t = |y_t - \\hat y_{t|t-1}|$$\n\n* Let $\\hat q_{t}^{1-\\alpha}$ be a predicted level $1-\\alpha$ quantile of the distribution of $e_t$\n\n* Define $I_{t|t-1}^{1-\\alpha} = [\\hat{y}_{t|t-1} - \\hat{q}_t^{1-\\alpha}, \\;     \\hat{y}_{t|t-1} + \\hat{q}_t^{1-\\alpha}]$. Note that \n    \n    $$\n    e_t \\leq \\hat{q}_t^{1-\\alpha} \\iff y_t \\in I_{t|t-1}^{1-\\alpha}\n    $$\n* Therefore we the reduced the problem of producing prediction intervals $I_{t|t-1}^{1-\\alpha}$ to one of tracking a quantile of $e_t$\n\n## Quantile updates\n\nWe begin with some estimate $\\hat{q}_{t_0+1}^{1-\\alpha}$ based on a burn-in set.\nThen repeat the following updates as $t$ increases, for a step size $\\eta > 0$: \n\n$$\\hat q_{t+1}^{1-\\alpha} = \\begin{cases} \n\\hat q_{t}^{1-\\alpha} + \\eta(1-\\alpha) \\quad \\text{if } y_t\\notin I_{t|t-1}^{1-\\alpha} \\\\\n\\hat q_{t}^{1-\\alpha} - \\eta\\alpha \\quad \\quad \\quad \\,\\,\\, \\text{if } y_t\\in I_{t|t-1}^{1-\\alpha}\n\\end{cases}$$\n\nIn words: \n\n* if the latest interval does not cover, then we increase the quantile (make the next interval wider), \n* otherwise we decrease the quantile by (make the next interval narrower).\n\nThis method has the following guarantee: \n\n$$\n\\Bigg| \\frac{1}{T} \\sum_{t=t_0+1}^{t_0+T} 1 \\big\\{ y_t \\in I_{t|t-1}^{1-\\alpha} \\big\\} - (1-\\alpha) \\Bigg| \\leq \\frac{b/\\eta + 1}{T}\n$$\n\nwhere $b$ is a bound on the errors (largest error possible/observable).\n\n## Multi-horizon smoothing\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}