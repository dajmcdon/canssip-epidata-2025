{
  "hash": "f5d8a926e71ac856627f607c0903247f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntalk-title: \"Quick Tour of Time Series Forecasting\"\ntalk-short-title: \"Time Series\"\ntalk-subtitle: \"InsightNet Forecasting Workshop 2024\"\ntalk-date: \"12 December -- Morning\"\nformat: revealjs\n---\n\n---\n---\n\n\n\n\\DeclareMathOperator*{\\minimize}{minimize}\n\n\n\n\n\n\n\n\n\n\n\n::: flex\n::: w-20\n\n:::\n::: w-80\n## {{< meta talk-title >}} {background-image=\"gfx/cover-art-1.svg\" background-position=\"bottom\"}\n\n### {{< meta talk-subtitle >}}\n\n<br>\n\n#### {{< meta author >}} \n[with huge thanks to Alice Cima, and also Xueda Shen, Nat DeFries, Dmitry Shemetov, and David Webber]{.fstyle}\n\n\n\n{{< meta talk-date >}}\n\n\n\n\n:::\n:::\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n## Outline\n\n1. Some Words About Forecasting\n\n1. Linear Regression for Time Series Data\n\n1. Evaluation Methods\n\n1. ARX Models\n\n1. Overfitting and Regularization\n\n1. Prediction Intervals\n\n1. Forecasting with Versioned Data\n\n1. Other Approaches to Time Series Data\n\n# Some Words About Forecasting\n\n## Forecasting is not magic\n\n- Forecasts are generally comprised of two parts: trend and seasonality\n- Methods for detecting and projecting trends are not magic; in general they're\n  not qualitatively that different from what you can do with your eyeballs\n- That said, assimilating information from exogenous features (ideally, leading\n  indicators) can lead highly nontrivial gains, beyond the eyeballs\n- Remember ... good data is just as (more?) important as a good model! \n- Seasonality can help short-term forecasts. Long-term forecasts, absent of \n  strong seasonality, are generally not very tractable\n\n# Linear Regression for Time Series Data\n\n## Basics of linear regression \n\n* Assume we observe a predictor $x_i$ and an outcome $y_i$ for $i = 1, \\dots, n$.\n\n* Linear regression seeks coefficients $\\beta_0$ and $\\beta_1$ such that\n\n$$y_i \\approx \\beta_0 + \\beta_1 x_i$$\n\nis a good approximation for every $i = 1, \\dots, n$.\n\n* In R, the coefficients are found by running `lm(y ~ x)`, where `y` is the vector \nof responses and `x` the vector of predictors.\n\n\n## Multiple linear regression \n\n* Given $p$ different predictors, we seek $(p+1)$ coefficients such that\n\n$$y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}$$\nis a good approximation for every $i = 1, \\dots, n$.\n\n\n## Linear regression with lagged predictor\n\n\n* In time series, outcomes and predictors are usually indexed by time $t$. \n\n::: {.fragment .fade-in}\n* [Goal]{.primary}: predicting future $y$, given present $x$. \n\n:::\n\n::: {.fragment .fade-in}\n* [Model]{.primary}: linear regression with lagged predictor\n\n$$\\hat y_t = \\hat \\beta + \\hat \\beta_0 x_{t-k}$$\n\ni.e. regress the outcome $y$ at time $t$ on the predictor $x$ at time $t-k$.\n\n:::\n\n::: {.fragment .fade-in}\n* [Equivalent]{.primary} way to write the model: \n\n$$\\hat y_{t+k} = \\hat \\beta + \\hat \\beta_0 x_t$$\n\n:::\n\n## Example: predicting COVID deaths  \n\n* During the pandemic, interest in predicting COVID deaths 7, 14, 21, 28 days ahead.\n\n* Can we reasonably [predict COVID deaths 28 days ahead]{.primary} by just using cases today?\n\n\n::: {.fragment .fade-in}\n* If we let\n\n$$y_{t+28} = \\text{deaths at time } t+28 \\quad\\quad x_{t} = \\text{cases at time } t$$\n  is the following a good model?\n\n  $$\\hat y_{t+28} = \\hat\\beta_0 + \\hat\\beta_1 x_{t}$$\n\n:::\n\n## Example: COVID cases and deaths in California \n\n::: flex\n\n::: w-65\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-ca-cases-deaths-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n:::\n\n::: w-35\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(ca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-11-05 23:50:44.00687\n\n# A tibble: 6 √ó 4\n# Groups:   geo_value [1]\n  geo_value time_value cases deaths\n* <chr>     <date>     <dbl>  <dbl>\n1 ca        2020-04-01  3.17 0.0734\n2 ca        2020-04-02  3.48 0.0835\n3 ca        2020-04-03  3.44 0.0894\n4 ca        2020-04-04  3.05 0.0778\n5 ca        2020-04-05  3.28 0.0876\n6 ca        2020-04-06  3.37 0.0848\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n::: {.callout-important icon=\"false\"}\n## Note\n\n[Cases]{.primary} seem [highly correlated]{.primary} with [deaths]{.primary}\nseveral weeks later (but [relation]{.primary} cases-deaths [changes]{.primary} over time).\n:::\n\n## Checking correlation\n\n\n* Let‚Äôs split the data into a training and a test set (before/after 2021-04-01).\n\n* On training set: [large correlation]{.primary}\nbetween cases and deaths 28 days ahead (> 0.95).\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/correlation-cases-deaths-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n::: {.fragment .fade-in}\n* Let's use (base) R to prepare the data and fit \n\n$$\\hat y_{t+28} = \\hat\\beta + \\hat\\beta_0 x_{t}$$\n\n:::\n\n## Preparing the data\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nca$lagged_cases <- dplyr::lag(ca$cases, n = k)     # Add column with cases lagged by k\nt0_date <- as.Date('2021-04-01')                   # Split into train and test (before/after t0_date)\ntrain <- ca |> filter(time_value <= t0_date)\ntest <- ca |> filter(time_value > t0_date)\n```\n:::\n\n\n\n\nCheck if `deaths` is approximately linear in `lagged_cases`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-lag-cases-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Fitting lagged linear regression in R\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg_lagged = lm(deaths ~ lagged_cases, data = train)\ncoef(reg_lagged)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept) lagged_cases \n   0.1171839    0.0112714 \n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-linear-fit-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n# Evaluation\n\n## Error metrics\n\n* Assume we have predictions $\\hat y_{new, t}$ for the unseen observations \n$y_{new,t}$ over times $t = 1, \\dots, N$.\n\n* Four commonly used error metrics are:\n\n  * mean squared error (MSE)\n\n  * mean absolute error (MAE)\n\n  * mean absolute percentage error (MAPE)\n\n  * mean absolute scaled error (MASE)\n\n## Error metrics: MSE and MAE\n\n$$MSE = \\frac{1}{N} \\sum_{t=1}^N (y_{new, t}- \\hat y_{new, t})^2$$\n$$MAE = \\frac{1}{N} \\sum_{t=1}^N |y_{new, t}- \\hat y_{new, t}|$$\n\n* MAE gives less importance to extreme errors than MSE.\n\n* [Drawback]{.primary}: both metrics are scale-dependent, so they are not universally \ninterpretable.\n(For example, if $y$ captures height, MSE and MAE will vary depending on whether we measure in feet or meters.)\n\n## Error metrics: MAPE\n\n* Fixing scale-dependence:\n\n$$MAPE = 100 \\times \\frac{1}{N} \\sum_{t=1}^N \n\\left|\\frac{y_{new, t}- \\hat y_{new, t}}{y_{new, t}}\\right|$$\n\n* [Drawbacks]{.primary}:\n\n  * Erratic behavior when $y_{new, t}$ is close to zero\n\n  * It assumes the unit of measurement has a meaningful zero (e.g. using \nFahrenheit or Celsius to measure temperature will lead to different MAPE)\n\n## Comparing MAE and MAPE\n\n::: {.callout-important}\nThere are situations when MAPE is problematic!\n:::\n\n::: flex\n::: w-70\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/mae-mape-example-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n:::\n\n::: {.w-30}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|      |   MAE|   MAPE|\n|:-----|-----:|------:|\n|yhat1 | 2.873| 43.140|\n|yhat2 | 5.382| 36.083|\n\n\n:::\n:::\n\n\n\n\n:::\n:::\n\n\n## Error metrics: MASE\n\n$$MASE = 100 \\times \\frac{\\frac{1}{N} \\sum_{t=1}^N \n|y_{new, t}- \\hat y_{new, t}|}\n{\\frac{1}{N-1} \\sum_{t=2}^N \n|y_{new, t}- y_{new, t-1}|}$$\n\n* [Advantages]{.primary}:\n\n  * is universally interpretable (not scale dependent)\n\n  * avoids the zero-pitfall\n\n* MASE in words: we normalize the error of our forecasts by that of a naive method \nwhich always predicts the last observation.\n\n\n## Comparing MAE, MAPE and MASE\n\n::: flex\n::: w-70\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/mae-mape-mase-example-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n:::\n\n::: w-35\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|      |   MAE|   MAPE|    MASE|\n|:-----|-----:|------:|-------:|\n|yhat1 | 2.873| 43.140|  66.100|\n|yhat2 | 5.382| 36.083| 123.817|\n\n\n:::\n:::\n\n\n\n\n:::\n:::\n\n## Defining the error metrics in R\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nMSE <- function(truth, prediction) {\n  mean((truth - prediction)^2)}\n\nMAE <- function(truth, prediction) {\n  mean(abs(truth - prediction))}\n\nMAPE <- function(truth, prediction) {\n  100 * mean(abs(truth - prediction) / truth)}\n\nMASE <- function(truth, prediction) {\n  100 * MAE(truth, prediction) / mean(abs(diff(truth)))}\n```\n:::\n\n\n\n\n## Estimating the prediction error\n\n* Given an error metric, we want to estimate the prediction error under that metric. \n\n* This can be accomplished in different ways, using the\n\n  * Training error\n\n  * Split-sample error\n\n  * Time series cross-validation error (using all past data or a trailing window)\n\n\n## Training error\n\n* The easiest but [worst]{.primary} approach to estimate the prediction error is \nto use the training error, i.e. the average error on the training set that was \nused to fit the model.\n\n* The training error is\n\n  * generally too optimistic as an estimate of prediction error\n\n  * [more optimistic the more complex the model!]{.primary}^[More on this when we talk about overfitting.]\n\n\n## Training error \n#### Linear regression of COVID deaths on lagged cases\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Getting the predictions for the training set\npred_train <- predict(reg_lagged)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-train-predictions-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n               MAE     MASE\ntraining 0.0740177 380.9996\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Split-sample error \n\nTo compute the split-sample error  \n\n  1. [Split]{.primary} data into training (up to time $t_0$), and test set (after $t_0$)\n\n  1. [Fit]{.primary} the model to the [training]{.primary} data only\n\n  1. Make [predictions]{.primary} for the [test]{.primary} set\n\n  1. Compute the selected [error]{.primary} metric on the [test]{.primary} set only\n\n\n::: {.callout-important icon=\"false\"}\n## Note\n\nSplit-sample estimates of prediction error don't mimic a situation where we \nwould refit the model in the future. \nThey are [pessimistic]{.primary} if the relation between outcome and predictors \nchanges over time.\n:::\n\n## Split-sample MSE \n\nAssume we want to make $h$-step ahead predictions, i.e. at time $t$ we want to \nmake a forecast for $t+h$. Then, the split-sample MSE is\n\n$$\\text{SplitMSE} = \\frac{1}{n-h-t_0} \\sum_{t = t_0}^{n-h} (\\hat y_{t+h|t_0} - y_{t+h})^2$$\n\nwhere\t$\\hat y_{t+h|t_0}$ indicates a prediction for $y$ at time $t+h$ that was made \nwith a model that was fit on data up to time $t_0$.\n\n\n\n## Split-sample error\n#### Linear regression of COVID deaths on lagged cases\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Getting h-step ahead predictions for the test set\nh <- k\ntest_h <- test[-(1:h-1), ] # drop first h-1 rows to avoid data leakage\npred_test <- predict(reg_lagged, newdata = test_h)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-test-predictions-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                   MAE      MASE\ntraining     0.0740177  380.9996\nsplit-sample 0.3116854 2914.4575\n```\n\n\n:::\n:::\n\n\n\n\n::: {.notes}\nNote that we are overestimating the peak due to the changed relationship \nbetween cases - deaths over time.\n\nTalk about data leakage.\n:::\n\n## Warning!\n\n* [Predictions]{.primary} are [overshooting]{.primary} the target, \nespecially in early 2022 ([Omicron]{.primary} phase).\n\n* This is because we are predicting [deaths]{.primary} using [lagged cases]{.primary}, \nbut the [relation]{.primary} between \nthe two [changes]{.primary} over time.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-ca-cases-deaths-again-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n\n## Time-series cross-validation (CV)\n#### $h$-step ahead predictions\n\n* If we [refit]{.primary} in the future once new data are available, a more \nappropriate way to estimate the prediction error is time-series cross-validation.\n\n* To get $h$-step ahead predictions, for each time $t = t_0, t_0+1, \\dots$,\n\n  * [Fit]{.primary} the model using data [up to time $t$]{.primary}\n\n  * Make a [prediction for $t+h$]{.primary} \n\n  * Record the [prediction error]{.primary}\n\n* The cross-validation MSE is then\n\n$$CVMSE = \\frac{1}{n-h-t_0} \\sum_{t = t_0}^{n-h} (\\hat y_{t+h|t} - y_{t+h})^2$$\n\nwhere\t$\\hat y_{t+h|t}$ indicates a prediction for $y$ at time $t+h$ that was made \nwith data available up to time $t$.\n\n## Time-series cross-validation (CV) \n#### Linear regression of COVID deaths on lagged cases\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- nrow(ca)                               #length of time series\nh <- k                                      #number of days ahead for which prediction is wanted\npred_all_past <- rep(NA, length = n)        #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to all past data and make h-step ahead prediction\n  reg_all_past = lm(deaths ~ lagged_cases, data = ca, subset = (1:n) <= t) \n  pred_all_past[t+h] = predict(reg_all_past, newdata = data.frame(ca[t+h, ]))\n}\n```\n:::\n\n\n\n\n::: {.callout-important icon=\"false\"}\n## Note\n\nWith the current model, we can only predict $k$ days ahead (where $k$ = number of days by which predictor is lagged)!\n:::\n\n\n## Time-series cross-validation (CV)\n#### Linear regression of COVID deaths on lagged cases\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-cv-predictions-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                     MAE      MASE\ntraining       0.0740177  380.9996\nsplit-sample   0.3116854 2914.4575\ntime series CV 0.2374931 2212.5992\n```\n\n\n:::\n:::\n\n\n\n\n::: {.notes}\nSome improvement wrt split-sample, but still overestimating peak. \n:::\n\n## Warning!\n\n* [Predictions]{.primary} are still [overshooting]{.primary} the target, \nbut [error]{.primary} is [smaller]{.primary} than split-sample.\n\n* Why?\n\n  * <span class=\"inner-list\"> üëç Forecaster is partially learning the change in cases-deaths relation \n(especially in late 2022)</span>\n\n  * <span class=\"inner-list\">üëé We refit on all past data, so predictions are still influenced by \n    old cases-deaths relation</span>\n\n\n::: {.callout-important icon=\"false\"}\n## Idea üí°\n\n**Ignore old data when refitting?**\n:::\n\n## Regression on a trailing window\n\n* [Fit the model on a window of data of length $w$]{.primary}, \nstarting at $t-w$ and ending at $t$.\n\n* [Advantage]{.primary}: if the predictors-outcome relation changes over time,\ntraining the forecaster on a window of recent data can better capture the recent \nrelation which might be more relevant to predict the outcome in the near future.\n\n* Window length [$w$]{.primary} considerations: \n\n  * <span class=\"inner-list\">if $w$ is too [big]{.primary}, \n  the model [can't adapt]{.primary} to the \n  recent predictors-outcome relation </span>\n  \n\n  * <span class=\"inner-list\">if $w$ is too [small]{.primary}, \n  the fitted model may be [too volatile]{.primary} \n  (trained on too little data)</span>\n  \n\n## Trailing window\n#### Linear regression of COVID deaths on lagged cases\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Getting the predictions through CV with trailing window\nw <- 120                                    #trailing window size\nh <- k                                      #number of days ahead for which prediction is wanted\npred_trailing <- rep(NA, length = n)        #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to a trailing window of size w and make h-step ahead prediction\n  reg_trailing = lm(deaths ~ lagged_cases, data = ca, \n                    subset = (1:n) <= t & (1:n) > (t-w)) \n  pred_trailing[t+h] = predict(reg_trailing, newdata = data.frame(ca[t+h, ]))\n}\n```\n:::\n\n\n\n\n\n## Time-series CV: all past vs trailing window\n#### Linear regression of COVID deaths on lagged cases\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-cv-predictions-trailing-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                                 MAE      MASE\ntraining                  0.07401770  380.9996\nsplit-sample              0.31168536 2914.4575\ntime series CV            0.23749306 2212.5992\ntime series CV + trailing 0.09932651  925.3734\n```\n\n\n:::\n:::\n\n\n\n\n::: {.notes}\nA lot of improvement: trailing window allows to adapt to the change in relationship \nbetween cases and deaths over time.\n:::\n\n\n# ARX models\n\n## Autoregressive exogenous input (ARX) model\n\n* [Idea]{.primary}: predicting the outcome via a linear combination of its lags \nand a set of exogenous (i.e. external) input variables\n\n* Example:\n\n$$\\hat y_{t+h} = \\hat\\phi + \\sum_{i=0}^p \\hat\\phi_i y_{t-i} + \\sum_{j=0}^q \\hat\\beta_j x_{t-j}$$\n\n* [Notice]{.primary}: we don't need to include all contiguous lags, and we could fit e.g.\n\n$$\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \\hat\\phi_2 y_{t-14} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7} + \\hat\\beta_2 x_{t-14}$$\n\n\n## ARX model for COVID deaths\n\n* Let's add lagged deaths as a predictor to our previous forecaster: \n\n$$\\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\beta_0 x_{t}$$\n\n* We will refer to this model as [ARX(1)]{.primary}, as it only includes one lag for each predictor.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Prepare data: add column with deaths lagged by 28\nca$lagged_deaths <- dplyr::lag(ca$deaths, n = k)\n```\n:::\n\n\n\n\n* How does it compare to the previous model in terms of time-series CV?\n\n::: {.callout-important icon=\"false\"}\n## Note\n\nFrom now on, we will only consider regression on a [trailing window]{.primary},\nsince regression on all past data leads to overshooting during Omicron.\n:::\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n\n## Time-Series CV (trailing): ARX(1) vs `lm` on lagged cases\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/arx-lm-plot-cv-predictions-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                          MAE     MASE\nARX(1)             0.07852942 731.6178\nlm on lagged cases 0.09932651 925.3734\n```\n\n\n:::\n:::\n\n\n\n\n::: {.notes}\nErrors under both metrics are smaller than with previous model.\n:::\n\n## Warning!\n\nRegression on a trailing window can be quite sensitive to data issues.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/warning-cv-predictions-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Warning!\n\n* At the [forecast date]{.primary} when the downward dip in deaths is predicted, \nthe coefficients estimated by [ARX(1)]{.primary} are\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) lagged_deaths  lagged_cases \n  0.067259206   0.304075294  -0.004285251 \n```\n\n\n:::\n:::\n\n\n\n\n\n* The downward dip is explained by the [negative coefficient on `lagged_cases`]{.primary}, \nand by the fact that at the forecast date\n\n  * observed deaths are exactly equal to 0 (data issue)\n\n  * observed cases are increasing\n\n\n\n## Predictions for different $h$\n\n* So far we only focused on COVID death predictions 28 days ahead.\n\n* We will now compare the model with lagged cases as predictor\n\n$$\\hat y_{t+h} = \\hat\\beta + \\hat\\beta_0 x_t$$\n\nto the ARX(1) model\n\n$$\\hat y_{t+h} = \\hat\\phi + \\hat\\phi_0 y_t + \\hat\\beta_0 x_t$$\n\nfor [horizons $h = 7, 14, 21, 28$]{.primary}.\n\n* We will only make forecasts on the $1^{st}$ day of each month, and use a trailing window with $w = 120$.\n\n\n## Predictions for different $h$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nh_vals <- c(7, 14, 21, 28)  #horizons \npred_m1 = pred_m2 <- data.frame(matrix(NA, nrow = 0, ncol = 3))  #initialize df for predictions\ncolnames(pred_m1) = colnames(pred_m2) = c(\"forecast_date\", \"target_date\", \"prediction\")\nw <- 120    #trailing window size\n\nca_lags <- ca |> select(!c(lagged_cases, lagged_deaths))\n\n# Create lagged predictors \nfor (i in seq_along(h_vals)) {\n  ca_lags[[paste0(\"lagged_deaths_\", h_vals[i])]] <- dplyr::lag(ca_lags$deaths, n = h_vals[i])\n  ca_lags[[paste0(\"lagged_cases_\", h_vals[i])]] <- dplyr::lag(ca_lags$cases, n = h_vals[i])\n}\n\n# Only forecast on 1st day of the months\nforecast_time <- which(ca_lags$time_value >= t0_date & \n                         ca_lags$time_value < ca_lags$time_value[n-max(h_vals)] &\n                         day(ca_lags$time_value) == 1)\n\nfor (t in forecast_time) {\n  for (i in seq_along(h_vals)) {\n    h = h_vals[i]\n    # formulas including h-lagged variables\n    m1_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h))\n    m2_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h, \" + lagged_deaths_\", h))\n    # fit to trailing window of data\n    m1_fit = lm(m1_formula, data = ca_lags, subset = (1:n) <= t & (1:n) > (t-w)) \n    m2_fit = lm(m2_formula, data = ca_lags, subset = (1:n) <= t & (1:n) > (t-w)) \n    # make h-step ahead predictions\n    pred_m1 = rbind(pred_m1, \n                    data.frame(forecast_date = ca_lags$time_value[t],\n                               target_date = ca_lags$time_value[t+h],\n                               prediction = predict(m1_fit, newdata = data.frame(ca_lags[t+h, ]))))\n    pred_m2 = rbind(pred_m2, \n                    data.frame(forecast_date = ca_lags$time_value[t],\n                               target_date = ca_lags$time_value[t+h],\n                               prediction = predict(m2_fit, newdata = data.frame(ca_lags[t+h, ]))))\n    }\n}\n```\n:::\n\n\n\n\n## Predictions for different $h$, `lm` on lagged cases\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-m1-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                         MAE    MASE\nlm on lagged cases 0.1049742 304.007\n```\n\n\n:::\n:::\n\n\n\n\n## Predictions for different $h$, ARX(1)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-m2-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n              MAE     MASE\nARX(1) 0.04463132 129.2531\n```\n\n\n:::\n:::\n\n\n\n\n\n## Visualizing predictions for multiple horizons\n\nDifferent ways to visualize predictions for multiple $h$\n\n* Last slides: [group by forecast date]{.primary}, and show prediction \"trajectories\"\n\n* Other approach: [one line and color per horizon]{.primary} $h$\n\n## Predictions by horizon, ARX(1)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-by-h-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n# Overfitting and Regularization\n\n## ARX models with 2 and 3 lags\n\n* The [ARX(1)]{.primary} model \n$\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\beta_0 x_{t}$ \nhas good predictive performance\n\n* We will now try to improve the ARX(1) model by including [more lags]{.primary} in the set of predictors\n\n* Let's consider two extensions: the [ARX(2)]{.primary} model\n\n$$\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7}$$\n\nand the [ARX(3)]{.primary} model\n\n$$\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \\hat\\phi_2 y_{t-14} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7} + \\hat\\beta_2 x_{t-14}$$\n\nand fit them using a trailing window with $w = 120$.\n\n## Fit ARX(2) and ARX(3) on trailing window\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_arx2 = pred_arx3 <- rep(NA, length = n)  \nw <- 120     #trailing window size\nh <- 28      #number of days ahead\n\n# create lagged predictors\nca_lags$lagged_deaths_35 <- dplyr::lag(ca_lags$deaths, n = 35)\nca_lags$lagged_deaths_42 <- dplyr::lag(ca_lags$deaths, n = 42)\nca_lags$lagged_cases_35 <- dplyr::lag(ca_lags$cases, n = 35)\nca_lags$lagged_cases_42 <- dplyr::lag(ca_lags$cases, n = 42)\n\nfor (t in t0:(n-h)) {\n  arx2_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h, \" + lagged_deaths_\", h,\n                                  \" + lagged_cases_\", h+7, \" + lagged_deaths_\", h+7))\n  arx3_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h, \" + lagged_deaths_\", h,\n                                  \" + lagged_cases_\", h+7, \" + lagged_deaths_\", h+7,\n                                  \" + lagged_cases_\", h+14, \" + lagged_deaths_\", h+14))\n  # fit to trailing window of data\n  arx2_fit = lm(arx2_formula, data = ca_lags, subset = (1:n) <= t & (1:n) > (t-w-7)) \n  arx3_fit = lm(arx3_formula, data = ca_lags, subset = (1:n) <= t & (1:n) > (t-w-14)) \n  # make h-step ahead predictions\n  pred_arx2[t+h] <- max(0, predict(arx2_fit, newdata = data.frame(ca_lags[t+h, ])))\n  pred_arx3[t+h] <- max(0, predict(arx3_fit, newdata = data.frame(ca_lags[t+h, ])))\n}\n```\n:::\n\n\n\n\n## Time-Series CV (trailing): ARX(1), ARX(2), and ARX(3) \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/arx-2-and-3-plot-cv-predictions-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n              MAE      MASE\nARX(1) 0.07852942  731.6178\nARX(2) 0.08716160  812.0393\nARX(3) 0.12487694 1163.4135\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n## Warning!\n\nAs we add more predictors, [forecasts]{.primary} seem more [volatile]{.primary} and errors increase.\n\n::: {.callout-important icon=\"false\"}\n## **Overfitting**\n:::\n\n## Overfitting\n\nWhen we introduce [too many predictors]{.primary} in the model\n\n* The estimated coefficients will be chosen to mimic the observed data very \n  closely on the training set, leading to [small training error]{.primary}\n\n* The predictive performance on the test set might be very poor, \n  producing [large split-sample and CV error]{.primary}\n\n\n\n## Extreme case: ARX model with 120 predictors\n\n* What happens if we increase the number of predictors to 120? \n\n* Let's fit\n\n$$\\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-1} + \\dots + \n\\hat\\phi_{59} y_{t-59} + \n\\hat\\beta_0 x_{t} + \\dots + \\hat\\beta_{t-59} x_{t-59}$$\n\nand compare [training vs split-sample errors]{.primary}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Extreme case: predictions on training and test set \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/overfit-plot-train-test-trunc-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                   MAE    MASE\nsplit-sample 0.3978198 3706.28\n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.callout-important icon=\"false\"}\n## Note\n\nSome predictions were negative, which doesn't make sense for count data, so we truncated them at 0.\n:::\n\n\n## Back to ARX(1), ARX(2), and ARX(3)\n\nHow can we \n\n* select the \"right\" number of lags to include? \n\n* avoid overfitting, while considering a large number of predictors? \n\n\n## Regularization\n\n* [Idea]{.primary}: introduce a regularization parameter $\\lambda$ that [shrinks or sets]{.primary} some \nof the estimated coefficients to zero, i.e. some predictors are estimated to \nhave limited or no predictive power\n\n* Most common regularization methods\n\n  * [Ridge]{.primary}: shrinks coefficients to zero\n  \n  * [Lasso]{.primary}: sets some coefficients to zero\n\n\n* Let's predict $h=28$ days ahead by regularizing\n\n  * ARX(1)\n  * ARX(2)\n  * ARX(3)\n\n\n## Fit ARX(3) + ridge/lasso for COVID deaths\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(glmnet) # Implements ridge and lasso\n\nh <- 28\nX <- as_tibble(ca_lags) |> select(ends_with(\"_28\"), ends_with(\"_35\"), ends_with(\"_42\"))\ny <- ca_lags$deaths\n\n# We'll need to omit NA values explicitly, as otherwise glmnet will complain\nX_train <- X[43:t0, ]\ny_train <- y[43:t0]\n\n# Ridge regression: set alpha = 0, lambda sequence will be chosen automatically\nridge <- glmnet(X_train, y_train, alpha = 0)\nbeta_ridge <- coef(ridge)       # matrix of estimated coefficients \nlambda_ridge <- ridge$lambda    # sequence of lambdas used to fit ridge \n\n# Lasso regression: set alpha = 1, lambda sequence will be chosen automatically\nlasso <- glmnet(X_train, y_train, alpha = 1)\nbeta_lasso <- coef(lasso)       # matrix of estimated coefficients \nlambda_lasso <- lasso$lambda    # sequence of lambdas used to fit lasso \n\ndim(beta_lasso)                 # One row per coefficient, one column per lambda value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  8 85\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n\n## Time-series CV (trailing) for ARX(3) + ridge/lasso \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nh <- 28  # number of days ahead \nw <- 120 # window length\n\n# Initialize matrices for predictions (one column per lambda value)\nyhat_ridge_mat <- matrix(NA, nrow = n, ncol = length(lambda_ridge))\nyhat_lasso_mat <- matrix(NA, nrow = n, ncol = length(lambda_lasso)) \nyhat_ridge = yhat_lasso <- rep(NA, length = n)\n\n# Select index of best lambda value on training set\nridge_index = cv.glmnet(as.matrix(X_train), y_train, alpha = 0, lambda = lambda_ridge)$index[1]\nlasso_index = cv.glmnet(as.matrix(X_train), y_train, alpha = 1, lambda = lambda_lasso)$index[1]\n\nfor (t in t0:(n-h)) {\n  # Indices of data within window\n  inds = t-w < 1:n & 1:n <= t\n  # Fit ARX + ridge/lasso for each lambda value\n  ridge_trail = glmnet(X[inds, ], y[inds], alpha = 0, lambda = lambda_ridge)\n  lasso_trail = glmnet(X[inds, ], y[inds], alpha = 1, lambda = lambda_lasso)\n  # Predict for each lambda value\n  yhat_ridge_mat[t+h, ] = predict(ridge_trail, newx = as.matrix(X[(t+h), ]))\n  yhat_lasso_mat[t+h, ] = predict(lasso_trail, newx = as.matrix(X[(t+h), ]))\n  # Save prediction corresponding to best lambda so far\n  yhat_ridge[t+h] = max(0, yhat_ridge_mat[t+h, ridge_index])\n  yhat_lasso[t+h] = max(0, yhat_lasso_mat[t+h, lasso_index])\n  if (t >= t0+h) {\n    # Prediction error\n    mae_ridge = colMeans(abs(yhat_ridge_mat[1:(t+1), ] - y[1:(t+1)]), na.rm = T)\n    mae_lasso = colMeans(abs(yhat_lasso_mat[1:(t+1), ] - y[1:(t+1)]), na.rm = T)\n    # Select index of lambda vector which gives lowest MAE so far\n    ridge_index <- which.min(mae_ridge)\n    lasso_index <- which.min(mae_lasso)\n  }\n}\n```\n:::\n\n\n\n\n\n## Predictions: time-series CV (trailing) for ARX(1) + ridge/lasso \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/regularize-arx-1-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                      MAE     MASE\nARX(1)         0.07852942 731.6178\nARX(1) + ridge 0.07004585 652.5808\nARX(1) + lasso 0.07887651 734.8514\n```\n\n\n:::\n:::\n\n\n\n\n\n## Predictions: time-series CV (trailing) for ARX(2) + ridge/lasso \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/regularize-arx-2-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                      MAE     MASE\nARX(2)         0.08716160 812.0393\nARX(2) + ridge 0.08143228 758.6622\nARX(2) + lasso 0.07807801 727.4122\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Predictions: time-series CV (trailing) for ARX(3) + ridge/lasso \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-arx3-regularization-cv-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                      MAE      MASE\nARX(3)         0.12487694 1163.4135\nARX(3) + ridge 0.08555066  797.0310\nARX(3) + lasso 0.07633053  711.1318\n```\n\n\n:::\n:::\n\n\n\n\n## Comparison of regularized ARX(1), ARX(2), and ARX(3)\n\n* Best model: ARX(1) + ridge\n\n* Second best: ARX(3) + lasso\n\n* Ridge worsens as more predictors are included\n\n* Lasso improves as more predictors are included\n\n\n# Prediction Intervals\n\n## Point predictions vs intervals \n\n* So far, we have only considered [point predictions]{.primary}, i.e. \nwe have fitted models \nto provide our [best guess on the outcome]{.primary} at time $t+h$. \n\n::: {.callout-important icon=\"false\"}\n## \nWhat if we want to provide a [measure of uncertainty]{.primary} around the point \nprediction or a [likely range of values]{.primary} for the outcome at time $t+h$?\n\n:::\n\n* For each target time $t+h$, we can construct [prediction intervals]{.primary}, i.e. provide \nranges of values that are expected to cover the true outcome value a fixed \nfraction of times.\n\n## Prediction intervals for `lm` fits\n\n* To get prediction intervals for the models we previously fitted, \nwe only need to tweak our call to `predict` by adding as an input: \n\n  `interval = \"prediction\", level = p`\n\n  where $p \\in (0, 1)$ is the desired coverage.\n\n* The output from `predict` will then be a matrix with \n\n  * first column a [point estimate]{.primary}\n  \n  * second column the [lower limit]{.primary} of the interval\n  \n  * third column the [upper limit]{.primary} of the interval\n\n\n## Prediction intervals for ARX (CV, trailing window)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Initialize matrices to store predictions \n# 3 columns: point estimate, lower limit, and upper limit\npred_interval_lm <- matrix(NA, nrow = n, ncol = 3)\ncolnames(pred_interval_lm) <- c('prediction', 'lower', 'upper')\n\nfor (t in t0:(n-h)) {\n  # Fit ARX and predict\n  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, \n                    subset = (1:n) <= t & (1:n) > (t-w)) \n  pred_interval_lm[t+h, ] = pmax(0, \n                              predict(arx_trailing, newdata = data.frame(ca[t+h, ]),\n                                      interval = \"prediction\", level = 0.8))\n}\n```\n:::\n\n\n\n\n## Prediction intervals for ARX (CV, trailing window)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot arx-intervals-cv-trailing-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                   MAE     MASE\nlm.trailing 0.08932857 832.2278\n```\n\n\n:::\n:::\n\n\n\n\n## Expected vs actual coverage\n\n* We would [expect]{.primary} the ARX model to [cover]{.primary} the truth\nabout [80\\%]{.primary} of the times. Is this actually true in practice?\n\n* The actual coverage of the predictive intervals is [lower]{.primary}:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n         Actual Expected\nCoverage    0.6      0.8\n```\n\n\n:::\n:::\n\n\n\n\n* We can use [calibration]{.primary} to handle under-covering (more on this in the afternoon)\n\n\n\n# Forecasting with Versioned Data\n\n## Versioned data\n\nSo far: data never revised \n(or simply ignored revisions, `as_of` today)\n\n::: {.callout-important icon=\"false\"}\n## \nHow can we [train forecasters]{.primary} when dealing with [versioned data]{.primary}?\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nca_archive\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n‚Üí An `epi_archive` object, with metadata:\n‚Ñπ Min/max time values: 2020-04-01 / 2023-03-09\n‚Ñπ First/last version with update: 2020-04-02 / 2023-03-10\n‚Ñπ Versions end: 2023-03-10\n‚Ñπ A preview of the table (24953 rows x 5 columns):\nKey: <geo_value, time_value, version>\n       geo_value time_value    version case_rate death_rate\n          <char>     <Date>     <Date>     <num>      <num>\n    1:        ca 2020-04-01 2020-04-02  3.009195 0.06580240\n    2:        ca 2020-04-01 2020-05-07  3.009195 0.06327156\n    3:        ca 2020-04-01 2020-06-21  3.009195 0.06580242\n    4:        ca 2020-04-01 2020-07-02  2.978825 0.06580242\n    5:        ca 2020-04-01 2020-07-03  2.978825 0.06580242\n   ---                                                     \n24949:        ca 2023-03-07 2023-03-08  0.000000 0.00000000\n24950:        ca 2023-03-07 2023-03-10 27.397832 0.00000000\n24951:        ca 2023-03-08 2023-03-09 21.083071 0.00000000\n24952:        ca 2023-03-08 2023-03-10  0.000000 0.00000000\n24953:        ca 2023-03-09 2023-03-10 22.185487 0.52072650\n```\n\n\n:::\n:::\n\n\n\n\n## Version-aware forecasting\n\n[Important]{.primary}: when fitting and predicting, only use data in the latest \nversion available at the forecast date!\n\n## Version-aware forecasting\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# initialize dataframe for predictions\n# 5 columns: forecast date, target date, 10%, 50%, and 90% quantiles\npred_aware <- data.frame(matrix(NA, ncol = 5, nrow = 0))\n\nw <- 120         #trailing window size\nh <- 28          #number of days ahead\n\n# forecast once a week\nfc_time_values <- seq(from = t0_date, to = as.Date(\"2023-02-09\"), by = \"1 week\")\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data <- epix_as_of(ca_archive, version = as.Date(fc_date))\n  # create lagged predictors\n  data$lagged_deaths <- dplyr::lag(data$deaths, h) \n  data$lagged_cases <- dplyr::lag(data$cases, h)\n  # perform regression\n  lm_weekly <- lm(deaths ~ lagged_deaths + lagged_cases, \n                  # only consider window of data\n                  data = data |> filter(time_value > (max(time_value) - w))) \n  # construct data.frame with the right predictors for the target date\n  predictors <- data.frame(lagged_deaths = tail(data$deaths, 1), \n                           lagged_cases = tail(data$cases, 1))\n  # make predictions for target date and add them to dataframe of predictions\n  pred_aware <- rbind(pred_aware, \n                      data.frame('forecast_date' = max(data$time_value),\n                                 'target_date' = max(data$time_value) + h, \n                                 t(pmax(0, predict(lm_weekly, newdata = predictors,\n                                                   interval = \"prediction\", level = 0.8)))))\n}\n```\n:::\n\n\n\n\n\n## Version-aware predictions (CV, trailing)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-versioned-cv-trailing-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                     MAE     MASE\nversion-aware 0.08001814 224.2782\n```\n\n\n:::\n:::\n\n\n\n\n## Version-unaware predictions (CV, trailing)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-version-unaware-cv-trailing-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                       MAE     MASE\nversion-unaware 0.07934554 200.4657\n```\n\n\n:::\n:::\n\n\n\n\n# Other Approaches to Time Series Data\n\n## Traditional Time Series Forecasting\n\nTwo popular frameworks:\n\n* Exponential Smoothing with Trend and Seasonality ([ETS]{.primary})\n\n* Autoregressive Integrated Moving Average ([ARIMA]{.primary})\n\n\n## ETS\n\n### Simple Exponential Smoothing (SES)\n\n* The $h$-step ahead prediction is a weighted average of the current \nobservation $y_t$ and the forecast $\\hat y_{t|t-1}$:\n\n$$\\hat y_{t+h|t} = \\alpha y_t + (1- \\alpha) \\hat y_{t|t-1}$$\n\nwhere $\\alpha \\in [0,1]$\n\n* [Forecasts are flat]{.primary}, i.e. do not depend on $h$\n\n* If $\\alpha = 1$, we retrieve the [naive flatline]{.primary} forecaster\n\n$$ y_{t+h|t} = y_t$$\n\n\n## ETS\n\n### Simple Exponential Smoothing (SES)\n\n* The SES forecast can be re-expressed as\n\n$$\\hat y_{t+h|t} = \\alpha y_t + \\alpha (1- \\alpha) y_{t-1}  + \\alpha (1- \\alpha)^2 y_{t-2} + \\dots$$\n\ni.e. observations [$y_{t-k}$]{.primary}  that are $k$ steps into the past are [exponentially \ndown-weighted]{.primary}, with weight $(1- \\alpha)^2$. Hence the name exponential smoothing.\n\n* [Component form]{.primary} of the SES forecast\n\n\\begin{align*}\n\\hat y_{t+h | t} & = l_t \\\\\nl_t & = \\alpha y_t + (1- \\alpha) l_{t-1}\n\\end{align*}\n\n## ETS\n\n### Holt's linear trend method\n\n* Expands SES component form by introducing a [trend]{.primary} $b_t$\n\n\\begin{align*}\n\\hat y_{t+h | t} &= l_t + b_t h\\\\\nl_t &= \\alpha y_t + (1- \\alpha) (l_{t-1} + b_{t-1}) \\\\\nb_t & = \\beta (l_{t} - l_{t-1}) + (1-\\beta) b_{t-1}\n\\end{align*}\n\nwhere $\\alpha, \\beta \\in [0,1]$.\n\n* [Forecasts are linear]{.primary} in $h$, with slope $b_t$.\n\n::: {.callout-important icon=\"false\"}\n## Note\n\nLinear trend forecasts can be erratic for large $h$!\n:::\n\n\n## ETS\n\n### Damped Holt's method\n\n* Introduces a parameter $\\phi$ to [dampen the forecast trajectory]{.primary} \n\n\\begin{align*}\n\\hat y_{t+h | t} &= l_t + b_t (\\phi +\\phi^2 + \\dots + \\phi^h)\\\\\nl_t &= \\alpha y_t + (1- \\alpha) (l_{t-1} + b_{t-1}\\phi) \\\\\nb_t & = \\beta (l_{t} - l_{t-1}) + (1-\\beta) b_{t-1}\\phi\n\\end{align*}\n\nwhere $\\alpha, \\beta, \\phi \\in [0,1]$\n\n* Contribution of slope $b_t$ to a forecast diminishes at each step into the \nfuture, by a multiplicative factor $\\phi$\n\n* As $h\\to \\infty$, forecasts approach a constant level\n\n$$ y_{t+h | t} \\to l_t + b_t \\sum_{j=1}^\\infty \\phi^j = l_t + b_t \\frac{\\phi}{1-\\phi}$$\n\n## ARIMA\n\n### AR(p)\n\nAuto-regressive model of order $p$\n\n$$y_t = \\sum_{j=1}^p \\phi_j y_{t-j} + w_t$$\n\n### MA(q)\n\nMoving average model of order $q$\n\n$$y_t = w_t + \\sum_{j=1}^q \\theta_j w_{t-j} $$\n\nwhere $w_t$ is a white noise sequence, i.e. \n$\\mathbb{E}(w_t)=0$ and $\\text{Var}(w_t)=\\sigma^2$ for all $t$, and \n$\\text{Cov}(w_s, w_t)=0$ for all $s \\neq t$\n\n\n## ARIMA\n\n### ARMA(p, q)\n\n$$y_t = \\sum_{j=1}^p \\phi_j y_{t-j} + \\sum_{j=0}^q \\theta_j w_{t-j} $$\n\nwith $\\theta_0 = 1$\n\n### ARIMA(p, d, q)\n\nAssumes an $ARMA(p, q)$ model on the \n$d^{th}$-order differences of $y_t$\n\n## Mechanistic Models\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}