{
  "hash": "4a818bc0230028f817290050d89d63f1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntalk-title: \"Forecasting and Time-Series Models\"\ntalk-short-title: \"{{< meta talk-title >}}\"\ntalk-subtitle: \"\"\nauthor: \"\"\nother-authors: \"\"\nrepo-address: \"cmu-delphi/insightnet-workshop-2024\"\ntalk-date: \"Venue -- dd Somemonth yyyy\"\nformat: revealjs\nexecute:\n  cache: false\n---\n\n\n\n<!-- Set any of the above to \"\" to omit them -->\n\n<!-- Or adjust the formatting in _titleslide.qmd -->\n\n\n---\n---\n\n\n\\DeclareMathOperator*{\\minimize}{minimize}\n\n\n\n\n\n\n\n\n\n::: flex\n::: w-20\n\n:::\n::: w-80\n## {{< meta talk-title >}} {background-image=\"gfx/cover-art-1.svg\" background-position=\"bottom\"}\n\n### {{< meta talk-subtitle >}}\n\n<br>\n\n#### {{< meta author >}} \n\n{{< meta other-authors >}}\n\n\n{{< meta talk-date >}}\n\n\n:::\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Outline\n\n1. Linear Regression for Time Series Data\n\n1. Evaluation Methods\n\n1. ARX Models\n\n1. Overfitting and Regularization\n\n1. Prediction Intervals\n\n1. Forecasting with Versioned Data\n\n1. Modeling Multiple Time Series\n\n\n# Linear Regression for Time Series Data\n\n## Basics of linear regression \n\n* Assume we observe a predictor $x_i$ and an outcome $y_i$ for $i = 1, \\dots, n$.\n\n* Linear regression seeks coefficients $\\beta_0$ and $\\beta_1$ such that\n\n$$y_i \\approx \\beta_0 + \\beta_1 x_i$$\n\nis a good approximation for every $i = 1, \\dots, n$.\n\n* In R, the coefficients are found by running `lm(y ~ x)`, where `y` is the vector \nof responses and `x` the vector of predictors.\n\n\n## Multiple linear regression \n\n* Given $p$ different predictors, we seek $(p+1)$ coefficients such that\n\n$$y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}$$\nis a good approximation for every $i = 1, \\dots, n$.\n\n\n## Linear regression with lagged predictor\n\n* In time series, outcomes and predictors are usually indexed by time $t$. \n\n* [Goal]{.primary}: predicting future $y$, given present $x$. \n\n* [Model]{.primary}: linear regression with lagged predictor\n\n$$\\hat y_t = \\hat \\beta_0 + \\hat \\beta_1 x_{t-k}$$\n\ni.e. regress the outcome $y$ at time $t$ on the predictor $x$ at time $t-k$.\n\n* Equivalent way to write the model: \n\n$$\\hat y_{t+k} = \\hat \\beta_0 + \\hat \\beta_1 x_t$$\n\n\n## Example: predicting COVID deaths  \n\n* During the pandemic, interest in predicting COVID deaths 7, 14, 28 days ahead.\n\n* Can we reasonably predict COVID deaths 28 days ahead by just using cases today?\n\n* If we let\n\n$$y_{t+28} = \\text{deaths at time } t+28 \\quad\\quad x_{t} = \\text{cases at time } t$$\n  is the following a good model?\n\n  $$\\hat y_{t+28} = \\hat\\beta_0 + \\hat\\beta_1 x_{t}$$\n\n\n## Example: COVID cases and deaths in California \n\n* Let's focus on California.\n\n* Cases seem highly correlated with deaths several weeks later.\n\n::: flex\n::: w-50\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/plot-ca-cases-deaths-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n:::\n::: w-50\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(ca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-10-23 11:07:29.405142\n\n# A tibble: 6 × 4\n  geo_value time_value cases deaths\n* <chr>     <date>     <dbl>  <dbl>\n1 ca        2020-04-01  3.17 0.0734\n2 ca        2020-04-02  3.48 0.0835\n3 ca        2020-04-03  3.44 0.0894\n4 ca        2020-04-04  3.05 0.0778\n5 ca        2020-04-05  3.28 0.0876\n6 ca        2020-04-06  3.37 0.0848\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n\n## Checking correlation\n\n* Let’s split the data into a training and a test set (before/after 2021-03-01).\n\n* On the training set, the correlation between cases and deaths 28 days ahead is very large (> 0.95).\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/correlation-cases-deaths-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n* Let's use (base) R to prepare the data and fit \n\n$$\\hat y_{t+28} = \\hat\\beta_0 + \\hat\\beta_1 x_{t}$$\n\n\n## Preparing the data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Add column with cases lagged by k\nca$lagged_cases <- dplyr::lag(ca$cases, n = k)\n\n# Split into train and test (before/after t0_date)\nt0_date <- as.Date('2021-03-01')\ntrain <- ca %>% filter(time_value <= t0_date)\ntest <- ca %>% filter(time_value > t0_date)\n```\n:::\n\n\n\n* Check if `deaths` is approximately linear in `lagged_cases`:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/plot-lag-cases-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Fitting lagged linear regression in R\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg_lagged = lm(deaths ~ lagged_cases, data = train)\ncoef(reg_lagged)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept) lagged_cases \n  0.09853776   0.01132312 \n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/plot-linear-fit-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n# Evaluation\n\n## Error metrics\n\n* Assume we have predictions $\\hat y_{new, t}$ for the unseen observations \n$y_{new,t}$ over times $t = 1, \\dots, N$.\n\n* Four commonly used error metrics are:\n\n  * mean squared error (MSE)\n\n  * mean absolute error (MAE)\n\n  * mean absolute percentage error (MAPE)\n\n  * mean absolute scaled error (MASE)\n\n## Error metrics: MSE and MAE\n\n$$MSE = \\frac{1}{N} \\sum_{t=1}^N (y_{new, t}- \\hat y_{new, t})^2$$\n$$MAE = \\frac{1}{N} \\sum_{t=1}^N |y_{new, t}- \\hat y_{new, t}|$$\n\n* MAE gives less importance to extreme errors than MSE.\n\n* [Drawback]{.primary}: both metrics are scale-dependent, so they are not universally \ninterpretable.\n(For example, if $y$ captures height, MSE and MAE will vary depending on whether we measure in feet or meters.)\n\n## Error metrics: MAPE\n\n* Fixing scale-dependence:\n\n$$MAPE = 100 \\times \\frac{1}{N} \\sum_{t=1}^N \n\\left|\\frac{y_{new, t}- \\hat y_{new, t}}{y_{new, t}}\\right|$$\n\n* [Drawbacks]{.primary}:\n\n  * Erratic behavior when $y_{new, t}$ is close to zero\n\n  * It assumes the unit of measurement has a meaningful zero (e.g. using \nFahrenheit or Celsius to measure temperature will lead to different MAPE)\n\n\n## Error metrics: MASE\n\n$$MASE = 100 \\times \\frac{\\frac{1}{N} \\sum_{t=1}^N \n|y_{new, t}- \\hat y_{new, t}|}\n{\\frac{1}{N-1} \\sum_{t=2}^N \n|y_{new, t}- y_{new, t-1}|}$$\n\n* [Advantages]{.primary}:\n\n  * is universally interpretable (not scale dependent)\n\n  * avoids the zero-pitfall\n\n* MASE in words: we normalize the error of our forecasts by that of a naive method \nwhich always predicts the last observation.\n\n\n## Defining the error metrics in R\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nMSE <- function(truth, prediction) {\n  mean((truth - prediction)^2)}\n\nMAE <- function(truth, prediction) {\n  mean(abs(truth - prediction))}\n\nMAPE <- function(truth, prediction) {\n  100 * mean(abs(truth - prediction) / truth)}\n\nMASE <- function(truth, prediction) {\n  100 * MAE(truth, prediction) / mean(abs(diff(truth)))}\n```\n:::\n\n\n\n## Estimating the prediction error\n\n* Given an error metric (e.g. MSE), we want to estimate the prediction error under that metric. \n\n* This can be accomplished in different ways, using the\n\n  * Training error\n\n  * Split-sample error\n\n  * Time series cross-validation error (using all past data or a trailing window)\n\n\n## Training error\n\n* The easiest but [worst]{.primary} approach to estimate the prediction error is \nto use the training error, i.e. the average error on the training set that was \nused to fit the model.\n\n* The training error is\n\n  * generally too optimistic as an estimate of prediction error\n\n  * [more optimistic the more complex the model!]{.primary}^[More on this when we talk about overfitting.]\n\n\n## Training error\n#### Linear regression of COVID deaths on lagged cases\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Getting the predictions for the training set\npred_train <- predict(reg_lagged)\n```\n:::\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](gfx/plot-train-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                MAE     MASE\ntraining 0.05985631 351.4848\n```\n\n\n:::\n:::\n\n\n\n\n## Split-sample error {.smaller}\n\n* To compute the split-sample error  \n\n  1. Split data into training (up to time $t_0$), and test set (after $t_0$)\n\n  1. Fit the model to the training data only\n\n  1. Make predictions for the test set\n\n  1. Compute the selected error metric on the test set only\n\n* Formally, the split-sample MSE is\n\n$$\\text{SplitMSE} = \\frac{1}{n-t_0} \\sum_{t = t_0 +1}^n (\\hat y_t - y_t)^2$$\n\n* Split-sample estimates of prediction error don't mimic a situation where we \nwould refit the model in the future. \nThey are [pessimistic]{.primary} if the relation between outcome and predictors \nchanges over time.\n\n## Split-sample error\n#### Linear regression of COVID deaths on lagged cases\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Getting the predictions for the test set\npred_test <- predict(reg_lagged, newdata = test)\n```\n:::\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](gfx/plot-test-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                    MAE     MASE\ntraining     0.05985631 351.4848\nsplit-sample 0.10005007 659.8971\n```\n\n\n:::\n:::\n\n\n\n## Time-series cross-validation (CV) {.smaller}\n#### 1-step ahead predictions\n\n* If we refit in the future once new data are available, a more \nappropriate way to estimate the prediction error is time-series cross-validation.\n\n* To get 1-step ahead predictions (i.e. at time $t$ we forecast for $t+1$) we proceed as follows,\nfor $t = t_0, t_0+1, \\dots$\n\n  1. Fit the model using data up to time $t$\n\n  1. Make a prediction for $t+1$ \n\n  1. Record the prediction error\n\nThe cross-validation MSE is then\n\n$$CVMSE = \\frac{1}{n-t_0} \\sum_{t = t_0}^{n-1} (\\hat y_{t+1|t} - y_{t+1})^2$$\n\nwhere\t$\\hat y_{t+1|t}$ indicates a prediction for $y$ at time $t+1$ that was made \nwith data available up to time $t$.\n\n## Time-series cross-validation (CV) {.smaller}\n#### $h$-step ahead predictions\n\n* In general, if we want to make $h$-step ahead predictions (i.e. at time \n$t$ we forecast for $t+h$), we proceed as follows \nfor $t = t_0, t_0+1, \\dots$\n\n  * Fit the model using data up to time $t$\n\n  * Make a prediction for $t+h$ \n\n  * Record the prediction error\n\n* The cross-validation MSE is then\n\n$$CVMSE = \\frac{1}{n-t_0} \\sum_{t = t_0}^{n-h} (\\hat y_{t+h|t} - y_{t+h})^2$$\n\nwhere\t$\\hat y_{t+h|t}$ indicates a prediction for $y$ at time $t+h$ that was made \nwith data available up to time $t$.\n\n## Time-series cross-validation (CV) \n#### Linear regression of COVID deaths on lagged cases\n\nGetting the predictions requires slightly more code:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- nrow(ca)                               #length of time series\nh <- k                                      #number of days ahead for which prediction is wanted\npred_all_past <- rep(NA, length = n-h-t0+1) #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to all past data and make 1-step ahead prediction\n  reg_all_past = lm(deaths ~ lagged_cases, data = ca, subset = (1:n) <= t) \n  pred_all_past[t-t0+1] = predict(reg_all_past, newdata = data.frame(ca[t+h, ]))\n}\n```\n:::\n\n\n\n::: {.callout-important icon=\"false\"}\n## Note\n\nWith the current model, we can only predict $k$ days ahead (where $k$ = number of days by which predictor is lagged)!\n:::\n\n\n## Time-series cross-validation (CV)\n#### Linear regression of COVID deaths on lagged cases\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](gfx/plot-cv-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                      MAE     MASE\ntraining       0.05985631 351.4848\nsplit-sample   0.10005007 659.8971\ntime series CV 0.08973951 732.5769\n```\n\n\n:::\n:::\n\n\n\n## Time-series CV on a trailing window {.smaller}\n\n* So far, to get $h$-step ahead predictions for time $t+h$, we have fitted the \nmodel on all data available up to time $t$. We can instead use a trailing \nwindow, i.e. fit the model on a window of data of length $w$, starting at \ntime $t-w$ and ending at $t$.\n\n* [Advantage]{.primary}: if the predictors-outcome relation changes over time,\ntraining the forecaster on a window of recent data can better capture the recent \nrelation which might be more relevant to predict the outcome in the near future.\n\n* Window length [$w$]{.primary} considerations: \n\n  * if $w$ is too [big]{.primary}, the model [can't adapt]{.primary} to the \n  recent predictors-outcome relation \n\n  * if $w$ is too [small]{.primary}, the fitted model may be [too volatile]{.primary} \n  (trained on too little data)\n\n## Time-series CV on a trailing window\n#### Linear regression of COVID deaths on lagged cases\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Getting the predictions through CV with trailing window\nw <- 60                                     #trailing window size\nh <- k                                      #number of days ahead for which prediction is wanted\npred_trailing <- rep(NA, length = n-h-t0+1) #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to a trailing window of size w and make 1-step ahead prediction\n  reg_trailing = lm(deaths ~ lagged_cases, data = ca, \n                    subset = (1:n) <= t & (1:n) > (t-w)) \n  pred_trailing[t-t0+1] = predict(reg_trailing, newdata = data.frame(ca[t+h, ]))\n}\n```\n:::\n\n\n\n## Time-series CV: all past vs trailing window\n#### Linear regression of COVID deaths on lagged cases\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](gfx/plot-cv-predictions-trailing-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                                 MAE     MASE\ntraining                  0.05985631 351.4848\nsplit-sample              0.10005007 659.8971\ntime series CV            0.08973951 732.5769\ntime series CV + trailing 0.11305308 922.8942\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n# ARX Models\n\n## Autoregressive (AR) model\n\n* [Idea]{.primary}: predicting the outcome via a linear combination of (some of) its lags \n\n$$\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-1} + \\dots + \\hat\\phi_p y_{t-p}$$\n\n* [Notice]{.primary}: we don't need to include all contiguous lags^[Here we \ndepart from traditional AR models, which do include all contiguous lags.].\n\n* For example, we could fit\n\n$$\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \\hat\\phi_2 y_{t-14}$$\n\n\n## AR model for COVID deaths\n\n* Let's disregard `cases`, and only use past `deaths` to predict future `deaths`. \n\n* For now we use one lag only, the one for which deaths and lagged deaths have largest correlation.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/auto-cor-deaths-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n* We will fit the model: $\\quad y_t \\approx \\beta_0 + \\beta_1 y_{t-1}$\n\n## Preparing the data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Add column with deaths lagged by 1\nca$lagged_deaths <- dplyr::lag(ca$deaths, n = 1)\n\n# Split into train and test (before/after t0_date)\ntrain <- ca %>% filter(time_value <= t0_date)\ntest <- ca %>% filter(time_value > t0_date)\n```\n:::\n\n\n\nCheck that the relationship between COVID deaths and lagged COVID deaths is approximately linear (on the training set):\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/ar-plot-deaths-and-lagged-cases-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Fitting the AR model for COVID deaths\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nar_fit = lm(deaths ~ lagged_deaths, data = train)\ncoef(ar_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) lagged_deaths \n  0.002515034   1.001278147 \n```\n\n\n:::\n:::\n\n\n\n::: {.callout-important icon=\"false\"}\n## Note\n\nThe intercept is $\\approx 0$ and the coefficient is $\\approx 1$. \nThis means that we are naively predicting the number of deaths tomorrow with the\nnumber of deaths observed today.\n:::\n\n## Predictions on training and test sets (AR model)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_train <- predict(ar_fit)                 #get training predictions\npred_test <- predict(ar_fit, newdata = test)  #get test predictions\n```\n:::\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](gfx/ar-plot-train-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                    MAE     MASE\ntraining     0.01630727 100.1796\nsplit-sample 0.01571932 103.6794\n```\n\n\n:::\n:::\n\n\n\n\n## Time-Series CV: all past and trailing (AR model)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Getting 1-step ahead predictions through CV \npred_all_past = pred_trailing <- rep(NA, length = n - t0) #initialize vectors of predictions\nw <- 30                                                   #trailing window size\nh <- 1                                                    #number of days ahead for which prediction is wanted\n\nfor (t in (t0+1):n) {\n  # fit to all past data \n  ar_all_past = lm(deaths ~ lagged_deaths, data = ca, subset = (1:n) <= (t-h)) \n  # fit to trailing window of data\n  ar_trailing = lm(deaths ~ lagged_deaths, data = ca, subset = (1:n) <= (t-h) & (1:n) > (t-h-w)) \n  # make 1-step ahead predictions\n  pred_all_past[t-t0] = predict(ar_all_past, newdata = data.frame(ca[t, ]))\n  pred_trailing[t-t0] = predict(ar_trailing, newdata = data.frame(ca[t, ]))\n}\n```\n:::\n\n\n\n::: {.callout-important icon=\"false\"}\n## Reminder\n\nWe can predict at most 1-day ahead in this case, because the predictor is only lagged \nby 1 with respect to the outcome.\n:::\n\n## Time-Series CV: all past and trailing (AR model)\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](gfx/ar-plot-cv-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                                 MAE     MASE\ntraining                  0.01630727 100.1796\nsplit-sample              0.01571932 103.6794\ntime series CV            0.01531685 101.0248\ntime series CV + trailing 0.01682578 110.9773\n```\n\n\n:::\n:::\n\n\n\n## Autoregressive exogenous input (ARX) model\n\n* [Idea]{.primary}: predicting the outcome via a linear combination of its lags and a set of exogenous (i.e. external) input variables\n\n* Example of ARX model \n\n$$y_t \\approx \\sum_{i=1}^p \\phi_i y_{t-i} + \\sum_{j=1}^q \\psi_j x_{t-j}$$\n\n* We can construct more complex ARX models with multiple lags of several exogenous \nvariables\n\n## ARX model for COVID deaths\n\n* To improve our predictions for COVID deaths, we could merge the two models \nconsidered so far (i.e. linear regression on cases lagged by k = 28, and \nlinear regression on deaths lagged by 1).\n\n* This leads us to the ARX model\n\n$$y_t \\approx \\beta_0 + \\beta_1 y_{t-1} + \\beta_2 x_{t-k}$$\n\n* We can fit it on the training set by running\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\narx_fit = lm(deaths ~ lagged_deaths + lagged_cases, data = train)\ncoef(arx_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) lagged_deaths  lagged_cases \n 0.0006846861  1.0211510942 -0.0002334021 \n```\n\n\n:::\n:::\n\n\n\n## Predictions on training and test sets (ARX model)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_train <- predict(arx_fit)                  #get training predictions\npred_test <- predict(arx_fit, newdata = test)   #get test predictions\n```\n:::\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](gfx/arx-plot-train-test-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                    MAE     MASE\ntraining     0.01716877 100.8174\nsplit-sample 0.01595024 105.2025\n```\n\n\n:::\n:::\n\n\n\n## Time-Series CV: all past and trailing (ARX model)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Getting 1-step ahead predictions through CV \npred_all_past = pred_trailing <- rep(NA, length = n - t0) #initialize vector of predictions\nw <- 30                                                   #trailing window size\nh <- 1                                                    #number of days ahead for which prediction is wanted\n\nfor (t in (t0+1):n) {\n  # fit to all past data\n  arx_all_past = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, subset = (1:n) <= (t-h)) \n  # fit to trailing window of data\n  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, \n                    subset = (1:n) <= (t-h) & (1:n) > (t-h-w)) \n  # make 1-step ahead prediction\n  pred_all_past[t-t0] = predict(arx_all_past, newdata = data.frame(ca[t, ]))\n  pred_trailing[t-t0] = predict(arx_trailing, newdata = data.frame(ca[t, ]))\n}\n```\n:::\n\n\n\n## Time-Series CV: all past and trailing (ARX model)\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](gfx/arx-plot-cv-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                                 MAE     MASE\ntraining                  0.01716877 100.8174\nsplit-sample              0.01595024 105.2025\ntime series CV            0.01558245 102.7767\ntime series CV + trailing 0.01840599 121.3998\n```\n\n\n:::\n:::\n\n\n\n\n\n# Overfitting and Regularization\n\n## Too many predictors\n\n* What if we try to incorporate past information extensively by fitting a model \nwith a very large number of predictors?\n\n  * The estimated coefficients will be chosen to mimic the observed data very \n  closely on the training set, leading to [small training error]{.primary}\n\n  * The predictive performance on the test set might be very poor, \n  producing [large split-sample and CV error]{.primary}\n\n::: {.callout-important icon=\"false\"}\n## Issue\nOverfitting!\n:::\n\n## ARX model for COVID deaths with many predictors\n\n* When predicting COVID deaths at time $t$, we can try to use more past \ninformation by fitting a model that includes the past two months of COVID deaths \nand cases as predictors\n\n$$y_t \\approx \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_{60} y_{t-60} + \n\\psi_1 yx_{t-1} + \\psi_2 x_{t-2} + \\dots + \\psi_{60} x_{t-60}$$\n\n## Preparing the data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny <- ca$deaths  #outcome\nlags <- 1:60    #lags used for predictors (deaths and cases)\n\n# Build predictor matrix with 60 columns\nX <- data.frame(matrix(NA, nrow = length(y), ncol = 2*length(lags)))\ncolnames(X) <- paste('X', 1:ncol(X), sep = '')\n\nfor (j in 1:length(lags)) {\n  # first 60 columns contain deaths lagged by 1, 2,..., 60\n  X[, j] = dplyr::lag(ca$deaths, lags[j])\n  # last 60 columns contain cases lagged by 1, 2,..., 60\n  X[, length(lags) + j] = dplyr::lag(ca$cases, lags[j])\n}\n\nX[1:5, 1:5] #look at first few entries of predictor matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          X1         X2         X3         X4 X5\n1         NA         NA         NA         NA NA\n2 0.07339501         NA         NA         NA NA\n3 0.08351846 0.07339501         NA         NA NA\n4 0.08942381 0.08351846 0.07339501         NA NA\n5 0.07782402 0.08942381 0.08351846 0.07339501 NA\n```\n\n\n:::\n:::\n\n\n\n## Fitting the ARX model\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Train/test split\ny_train <- y[1:t0]\nX_train <- X[1:t0, ]\ny_test <- y[(t0+1):length(y)]\nX_test <- X[(t0+1):length(y), ]\n\n# Fitting the ARX model\nreg = lm(y_train ~ ., data = X_train)\ncoef(reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)            X1            X2            X3            X4 \n-0.0289752219  1.0036479336 -0.0697910592 -0.0965290853  0.1253086968 \n           X5            X6            X7            X8            X9 \n-0.2084337959  0.2883024258 -0.8537569681  0.9078062453 -0.1967353863 \n          X10           X11           X12           X13           X14 \n 0.0249038037  0.1204699405 -0.5801064428  0.5603828110 -0.6110256391 \n          X15           X16           X17           X18           X19 \n 0.8353450749 -0.4524450151  0.1033181504  0.1245158924 -0.3785864886 \n          X20           X21           X22           X23           X24 \n 0.6905902987 -0.6700783625  0.6263187331 -0.5753647865  0.2775892130 \n          X25           X26           X27           X28           X29 \n-0.0897320984 -0.1341105958  0.4351486697 -0.2602753831  0.4725943368 \n          X30           X31           X32           X33           X34 \n-0.8007746051  0.3937355737  0.0128097365 -0.1043945210  0.5427975537 \n          X35           X36           X37           X38           X39 \n-0.4767252646  0.3308456841 -0.6875831564  0.2761150796  0.2558659205 \n          X40           X41           X42           X43           X44 \n-0.1255477523  0.1558682187 -0.4422861566  0.6432639852 -0.6517865298 \n          X45           X46           X47           X48           X49 \n 0.3358910370  0.2200145225 -0.1375613552  0.0672151530 -0.3252118296 \n          X50           X51           X52           X53           X54 \n 0.2540547370 -0.1871310791  0.2459380841  0.1757874014 -0.1549375123 \n          X55           X56           X57           X58           X59 \n 0.0255017050 -0.1832944351  0.0593011197  0.2009740908 -0.3059064460 \n          X60           X61           X62           X63           X64 \n 0.1253184868  0.0031164120 -0.0018705523 -0.0014906132  0.0007032307 \n          X65           X66           X67           X68           X69 \n-0.0005234523  0.0002075257  0.0009498408  0.0001458265 -0.0018947471 \n          X70           X71           X72           X73           X74 \n-0.0008634595  0.0035604212 -0.0029179301  0.0025100174  0.0023823915 \n          X75           X76           X77           X78           X79 \n 0.0007823787 -0.0038703871 -0.0050937407 -0.0011287754  0.0052345076 \n          X80           X81           X82           X83           X84 \n 0.0043848044 -0.0006836941 -0.0010784050 -0.0013433377 -0.0064516860 \n          X85           X86           X87           X88           X89 \n 0.0092375513 -0.0018808875 -0.0006133343  0.0037939090 -0.0031949315 \n          X90           X91           X92           X93           X94 \n-0.0039879854  0.0019643772 -0.0001691026  0.0029029383 -0.0013385902 \n          X95           X96           X97           X98           X99 \n 0.0043478153 -0.0066887049 -0.0027919455  0.0085703483 -0.0035707441 \n         X100          X101          X102          X103          X104 \n 0.0041815143 -0.0061790306  0.0052011426 -0.0080402651  0.0016673399 \n         X105          X106          X107          X108          X109 \n 0.0079619883 -0.0078739633  0.0053070612 -0.0032932701  0.0038191524 \n         X110          X111          X112          X113          X114 \n-0.0038973101  0.0001299517  0.0038706215 -0.0074718376  0.0056987332 \n         X115          X116          X117          X118          X119 \n-0.0011644932 -0.0042328810  0.0031769041 -0.0002317292  0.0025479447 \n         X120 \n-0.0033566562 \n```\n\n\n:::\n:::\n\n\n\n## Predictions on training and test set \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_train <- predict(reg)                    #get training predictions\npred_test <- predict(reg, newdata = X_test)   #get test predictions\n```\n:::\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](gfx/overfit-plot-train-test-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                     MAE      MASE\ntraining     0.008841864  47.99282\nsplit-sample 0.042037088 277.26268\n```\n\n\n:::\n:::\n\n\n\n\n## Regularization\n\n* If we want to consider a large number of predictors, \nhow can we avoid overfitting?\n\n* [Idea]{.primary}: introduce a regularization parameter $\\lambda$ that [shrinks or sets]{.primary} some \nof the estimated coefficients to zero, i.e. some predictors are estimated to \nhave limited or no predictive power\n\n* Most common regularization methods\n\n  * [Ridge]{.primary}: shrinks coefficients to zero\n  \n  * [Lasso]{.primary}: sets some coefficients to zero\n\n## Choosing $\\lambda$\n\n* The regularization parameter $\\lambda$ can be selected by cross-validation:\n\n  1. Select a sequence of $\\lambda$'s\n  \n  1. Fit and predict for each such $\\lambda$\n  \n  1. Select the $\\lambda$ that leads to smaller error\n  \n* The R library `glmnet` implements ridge and lasso regression, \nand can perform step 1. automatically.\n\n\n\n## Fit ARX + ridge/lasso for COVID deaths\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(glmnet) # Implements ridge and lasso\n\n# We'll need to omit NA values explicitly, as otherwise glmnet will complain\nna_obs <- 1:max(lags)\nX_train <- X_train[-na_obs, ]\ny_train <- y_train[-na_obs]\n\n# Ridge regression: set alpha = 0, lambda sequence will be chosen automatically\nridge <- glmnet(X_train, y_train, alpha = 0)\nbeta_ridge <- coef(ridge)       # matrix of estimated coefficients \nlambda_ridge <- ridge$lambda    # sequence of lambdas used to fit ridge \n\n# Lasso regression: set alpha = 1, lambda sequence will be chosen automatically\nlasso <- glmnet(X_train, y_train, alpha = 1)\nbeta_lasso <- coef(lasso)       # matrix of estimated coefficients \nlambda_lasso <- lasso$lambda    # sequence of lambdas used to fit lasso \n\ndim(beta_lasso)      # One row per coefficient, one column per lambda value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 121 100\n```\n\n\n:::\n:::\n\n\n\n\n## Predictions on test set and selection of $\\lambda$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Predict values for second half of the time series\nyhat_ridge <- predict(ridge, newx = as.matrix(X_test))\nyhat_lasso <- predict(lasso, newx = as.matrix(X_test))\n\n# Compute MAE \nmae_ridge <- colMeans(abs(yhat_ridge - y_test))\nmae_lasso <- colMeans(abs(yhat_lasso - y_test))\n\n# Select index of lambda vector which gives lowest MAE\nmin_ridge <- which.min(mae_ridge)\nmin_lasso <- which.min(mae_lasso)\npaste('Best MAE ridge:', round(min(mae_ridge), 3),\n      '; Best MAE lasso:', round(min(mae_lasso), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Best MAE ridge: 0.046 ; Best MAE lasso: 0.018\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get predictions for train and test sets\npred_train_ridge <- predict(ridge, newx = as.matrix(X_train))[, min_ridge] \npred_test_ridge <- yhat_ridge[, min_ridge]\npred_train_lasso <- predict(lasso, newx = as.matrix(X_train))[, min_lasso] \npred_test_lasso <- yhat_lasso[, min_lasso]\n```\n:::\n\n\n\n## Estimated coefficients: shrinkage vs sparsity\n\n::: flex\n::: w-50\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                    ridge        lasso\n(Intercept) -9.939300e-03 1.189313e-02\nX1           8.872966e-02 9.289459e-01\nX2           6.782888e-02 0.000000e+00\nX3           4.946021e-02 0.000000e+00\nX4           3.515120e-02 0.000000e+00\nX5           2.419016e-02 0.000000e+00\nX6           1.128371e-02 0.000000e+00\nX7           2.612847e-03 0.000000e+00\nX8           6.823573e-03 0.000000e+00\nX9           1.148253e-02 0.000000e+00\nX10          1.400489e-02 0.000000e+00\nX11          1.450184e-02 0.000000e+00\nX12          9.401767e-03 0.000000e+00\nX13          9.216418e-03 0.000000e+00\nX14          9.760208e-03 0.000000e+00\nX15          8.210787e-03 0.000000e+00\nX16          5.431338e-03 0.000000e+00\nX17          4.833249e-03 0.000000e+00\nX18          6.670780e-03 0.000000e+00\nX19          1.199074e-02 3.382723e-04\nX20          1.275594e-02 0.000000e+00\nX21          1.440701e-02 0.000000e+00\nX22          1.840612e-02 0.000000e+00\nX23          2.384495e-02 2.080244e-04\nX24          2.761157e-02 0.000000e+00\nX25          2.542945e-02 0.000000e+00\nX26          2.419727e-02 0.000000e+00\nX27          2.434294e-02 0.000000e+00\nX28          2.449578e-02 0.000000e+00\nX29          2.021843e-02 0.000000e+00\nX30          1.338327e-02 0.000000e+00\nX31          7.529272e-03 0.000000e+00\nX32          5.901396e-03 0.000000e+00\nX33          3.727127e-03 0.000000e+00\nX34          6.462585e-03 0.000000e+00\nX35          1.673897e-03 0.000000e+00\nX36         -3.282419e-03 0.000000e+00\nX37         -7.946291e-03 0.000000e+00\nX38         -1.191724e-02 0.000000e+00\nX39         -1.069952e-02 0.000000e+00\nX40         -8.865466e-03 0.000000e+00\nX41         -6.464374e-03 0.000000e+00\nX42          4.275794e-03 0.000000e+00\nX43          1.919658e-02 0.000000e+00\nX44          2.980180e-02 0.000000e+00\nX45          4.786958e-02 4.480153e-04\nX46          5.324895e-02 0.000000e+00\nX47          4.886655e-02 0.000000e+00\nX48          3.628809e-02 0.000000e+00\nX49          2.580591e-02 0.000000e+00\nX50          2.677230e-02 0.000000e+00\nX51          3.225742e-02 0.000000e+00\nX52          2.766313e-02 0.000000e+00\nX53          3.045624e-02 0.000000e+00\nX54          4.402694e-02 0.000000e+00\nX55          3.934535e-02 0.000000e+00\nX56          2.177537e-02 0.000000e+00\nX57         -2.157397e-02 0.000000e+00\nX58         -6.404024e-02 0.000000e+00\nX59         -1.143864e-01 0.000000e+00\nX60         -1.501638e-01 0.000000e+00\nX61          4.161470e-04 0.000000e+00\nX62          3.253236e-04 0.000000e+00\nX63          2.394225e-04 0.000000e+00\nX64          1.513126e-04 0.000000e+00\nX65          6.424676e-05 0.000000e+00\nX66          3.746423e-05 0.000000e+00\nX67          2.581948e-05 0.000000e+00\nX68          4.789785e-05 0.000000e+00\nX69          7.950204e-05 0.000000e+00\nX70          1.040050e-04 0.000000e+00\nX71          1.400745e-04 8.926210e-06\nX72          1.591103e-04 2.342475e-07\nX73          1.604022e-04 0.000000e+00\nX74          1.744726e-04 1.241179e-05\nX75          1.730420e-04 0.000000e+00\nX76          1.362768e-04 0.000000e+00\nX77          1.275257e-04 0.000000e+00\nX78          1.281437e-04 0.000000e+00\nX79          2.084742e-04 0.000000e+00\nX80          2.942656e-04 6.824032e-04\nX81          3.237605e-04 0.000000e+00\nX82          3.553267e-04 0.000000e+00\nX83          3.928755e-04 0.000000e+00\nX84          3.962876e-04 0.000000e+00\nX85          4.183372e-04 0.000000e+00\nX86          3.568176e-04 0.000000e+00\nX87          2.716336e-04 0.000000e+00\nX88          2.091001e-04 0.000000e+00\nX89          1.229814e-04 0.000000e+00\nX90          6.607063e-05 0.000000e+00\nX91          2.150426e-05 0.000000e+00\nX92         -4.080164e-05 0.000000e+00\nX93         -5.813667e-05 0.000000e+00\nX94         -5.999272e-05 0.000000e+00\nX95         -4.409698e-05 0.000000e+00\nX96         -1.834663e-05 0.000000e+00\nX97         -9.220951e-06 0.000000e+00\nX98          1.791033e-05 0.000000e+00\nX99          1.464231e-05 0.000000e+00\nX100         1.109229e-05 0.000000e+00\nX101         2.655447e-05 0.000000e+00\nX102         5.768881e-05 0.000000e+00\nX103         8.401814e-05 0.000000e+00\nX104         9.262989e-05 0.000000e+00\nX105         7.920049e-05 0.000000e+00\nX106         7.983363e-05 0.000000e+00\nX107         8.220127e-05 0.000000e+00\nX108         6.665727e-05 0.000000e+00\nX109         7.027286e-05 0.000000e+00\nX110         2.697071e-05 0.000000e+00\nX111        -1.231577e-05 0.000000e+00\nX112        -4.742160e-05 0.000000e+00\nX113        -6.578796e-05 0.000000e+00\nX114        -7.822627e-05 0.000000e+00\nX115        -1.489561e-04 0.000000e+00\nX116        -2.535423e-04 0.000000e+00\nX117        -3.046913e-04 0.000000e+00\nX118        -3.284353e-04 0.000000e+00\nX119        -3.492309e-04 0.000000e+00\nX120        -3.962423e-04 0.000000e+00\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: w-50\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/plot-ridge-lasso-coeff-1.svg){fig-align='center' height=600px}\n:::\n:::\n\n\n\n:::\n:::\n\n## Predictions: ARX + ridge/lasso (train and test set)\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](gfx/shrinkage-sparsity-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                          MAE      MASE\nridge training     0.02555976 138.73603\nridge split-sample 0.04613864 304.31515\nlasso training     0.01822366  98.91632\nlasso split-sample 0.01767544 116.58133\n```\n\n\n:::\n:::\n\n\n\n## Time-series CV for ARX + ridge/lasso (trailing)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Initialize matrices for predictions (one column per lambda value)\nyhat_ridge <- matrix(NA, ncol = length(lambda_ridge), nrow = n-t0) \nyhat_lasso <- matrix(NA, ncol = length(lambda_lasso), nrow = n-t0) \n\nh <- 1 #number of days ahead for which prediction is wanted\n\nfor (t in (t0+1):n) {\n  # Indices of data within window\n  inds = t-h-w < 1:n & 1:n <= t-h\n  # Fit ARX + ridge/lasso\n  ridge_trail = glmnet(X[inds, ], y[inds], alpha = 0, lambda = lambda_ridge)\n  lasso_trail = glmnet(X[inds, ], y[inds], alpha = 1, lambda = lambda_lasso)\n  # Predict\n  yhat_ridge[t-t0, ] = predict(ridge_trail, newx = as.matrix(X[t, ]))\n  yhat_lasso[t-t0, ] = predict(lasso_trail, newx = as.matrix(X[t, ]))\n}\n\n# MAE values for each lambda\nmae_ridge <- colMeans(abs(yhat_ridge - y_test))\nmae_lasso <- colMeans(abs(yhat_lasso - y_test))\n\n# Select lambda that minimizes MAE and save corresponding predictions\nmin_ridge <- which.min(mae_ridge)\nmin_lasso <- which.min(mae_lasso)\npred_cv_ridge <- yhat_ridge[, min_ridge]\npred_cv_lasso <- yhat_lasso[, min_lasso]\n\npaste('Best MAE ridge:', round(min(mae_ridge), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Best MAE ridge: 0.019\"\n```\n\n\n:::\n\n```{.r .cell-code}\npaste('Best MAE lasso:', round(min(mae_lasso), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Best MAE lasso: 0.02\"\n```\n\n\n:::\n:::\n\n\n\n\n## Predictions: time-series CV for ARX + ridge/lasso (trailing)\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](gfx/plot-regularization-cv-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                           MAE     MASE\nridge CV + trailing 0.01893437 124.8848\nlasso CV + trailing 0.02040692 134.5973\n```\n\n\n:::\n:::\n\n\n\n# Prediction Intervals\n\n## Point predictions vs intervals \n\n* So far, we have only considered [point predictions]{.primary}, i.e. \nwe have fitted models \nto provide our [best guess on the outcome]{.primary} at time $t$. \n\n::: {.callout-important icon=\"false\"}\n## \n* What if we want to provide a [measure of uncertainty]{.primary} around our point \nprediction or a [likely range of values]{.primary} for the outcome at time $t$?\n\n:::\n\n* For each target time $t$, we can construct [prediction intervals]{.primary}, i.e. provide \nranges of values that are expected to cover the true outcome value a fixed \nfraction of times.\n\n## Prediction intervals for `lm` fits\n\n* To get prediction intervals for the models we previously fitted, \nwe only need to tweak our call to `predict` by adding as an input: \n\n  `interval = \"prediction\", level = p`\n\n  where $p \\in (0, 1)$ is the desired coverage.\n\n* The output from `predict` will then be a matrix with \n\n  * first column a point estimate\n  \n  * second column the lower limit of the interval\n  \n  * third column the upper limit of the interval\n\n## Prediction intervals for ARX (test)\n\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\npred_test_ci <- predict(arx_fit, \n                        newdata = test, \n                        interval = \"prediction\", \n                        level = 0.95)\n\nhead(pred_test_ci)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit       lwr       upr\n1 1.0915747 1.0307660 1.1523834\n2 1.0686306 1.0079132 1.1293480\n3 0.7745034 0.7160805 0.8329263\n4 0.7406785 0.6823080 0.7990490\n5 0.7071047 0.6488102 0.7653992\n6 0.7532920 0.6946283 0.8119557\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/plot-arx-intervals-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Prediction intervals for ARX (time-series CV)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Initialize matrices to store predictions \n# 3 columns: point estimate, lower limit, and upper limit\npred_all_past = pred_trailing <- matrix(NA, nrow = n - t0, ncol = 3)\ncolnames(pred_all_past) = colnames(pred_trailing) <- c('prediction', 'lower', 'upper')\n\nfor (t in (t0+1):n) {\n  # Fit ARX \n  arx_all_past = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, \n                    subset = (1:n) <= (t-h)) \n  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, \n                    subset = (1:n) <= (t-h) & (1:n) > (t-h-w)) \n  # Predict\n  pred_all_past[t-t0, ] = predict(arx_all_past, newdata = data.frame(ca[t, ]),\n                                interval = \"prediction\", level = 0.95)\n  pred_trailing[t-t0, ] = predict(arx_trailing, newdata = data.frame(ca[t, ]),\n                                interval = \"prediction\", level = 0.95)\n}\n\nlm_pred_all_past <- cbind(test, pred_all_past)\nlm_pred_trailing <- cbind(test, pred_trailing)\n```\n:::\n\n\n\n## Prediction intervals for ARX (CV, all past)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/plot arx-intervals-cv-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Prediction intervals for ARX (CV, trailing window)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/plot arx-intervals-cv-trailing-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n[Note]{.primary}: the width of the prediction intervals varies substantially over time.\n\n\n## Quantile regression\n\n* So far we only considered different ways to apply linear regression.\n\n* Quantile regression is a different estimation method, and it directly targets conditional \nquantiles of the outcome over time.\n\n::: {.callout-note}\n## Definition\nConditional quantile = value below which a given percentage (e.g. 25%, 50%, \n75%) of observations fall, given specific values of the predictor variables. \n:::\n\n* [Advantage]{.primary}: it provides a more complete picture of the outcome distribution.\n\n## ARX model for COVID deaths via quantile regression\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#install.packages(\"quantreg\")\nlibrary(quantreg)  #library to perform quantile regression\n\n# Set quantiles of interest: we will focus on 2.5%, 50% (i.e. median), and 97.5% quantiles\nquantiles <- c(0.025, 0.5, 0.975)  \n\n# Fit quantile regression on training set\nq_reg <- rq(deaths ~ lagged_deaths + lagged_cases, data = train, tau = quantiles)\n\n# Estimated coefficients\ncoef(q_reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                tau= 0.025    tau= 0.500   tau= 0.975\n(Intercept)   -0.009625083  0.0010198050  0.005489311\nlagged_deaths  0.926648244  1.0133190428  1.287609030\nlagged_cases   0.000158453 -0.0002417395 -0.002422954\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predict on test set\npred_test <- predict(q_reg, newdata = test)\n```\n:::\n\n\n\n## Predictions via quantile regression (test)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/q-reg-training-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Predictions via quantile regression (time-series CV)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Initialize matrices to store predictions \n# 3 columns: lower limit, median, and upper limit\npred_all_past = pred_trailing <- matrix(NA, nrow = n - t0, ncol = 3)\ncolnames(pred_all_past) = colnames(pred_trailing) <- c('lower', 'median', 'upper')\n\nfor (t in (t0+1):n) {\n  # Fit quantile regression\n  rq_all_past = rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles,\n                   data = ca, subset = (1:n) <= (t-h)) \n  rq_trailing = rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles,\n                   data = ca, subset = (1:n) <= (t-h) & (1:n) > (t-h-w)) \n  # Predict\n  pred_all_past[t-t0, ] = predict(rq_all_past, newdata = data.frame(ca[t, ]))\n  pred_trailing[t-t0, ] = predict(rq_trailing, newdata = data.frame(ca[t, ]))\n}\n\nrq_pred_all_past <- cbind(test, pred_all_past)\nrq_pred_trailing <- cbind(test, pred_trailing)\n```\n:::\n\n\n\n## Predictions via quantile regression (CV, all past)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/q-reg-plot-cv-predictions-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Predictions via quantile regression (CV, trailing)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/q-reg-plot-cv-predictions-trailing-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Actual Coverage\n\n* We would expect the ARX model fitted via `lm` and via `rq` to cover the truth\nabout 95\\% of the times. Is this actually true in practice?\n\n* The actual coverage of each predictive interval is  \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n         lm.all.past lm.trailing rq.all.past rq.trailing\nCoverage   0.9673203   0.9281046    0.875817   0.6699346\n```\n\n\n:::\n:::\n\n\n\n* Notice that the coverage of `lm` is close to 95\\%, while `rq` has lower \ncoverage, especially for the trailing window case.\n\n## Evaluation\n\n* Prediction intervals are “good” if they \n\n  * cover the truth most of the time\n  \n  * are not too wide\n  \n* Error metric that captures both desiderata: [Weighted Interval Score (WIS)]{.primary}\n\n* $F$ = forecast composed of predicted quantiles $q_{\\tau}$ for the set \nof quantile levels $\\tau$. The WIS for target variable $Y$ is represented as \n([McDonald et al., 2021](https://www.pnas.org/doi/full/10.1073/pnas.2111453118)):\n\n$$WIS(F, Y) = 2\\sum_{\\tau} \\phi_{\\tau} (Y - q_{\\tau})$$\n\nwhere $\\phi_{\\tau}(x) = \\tau |x|$ for $x \\geq 0$ and \n$\\phi_{\\tau}(x) = (1-\\tau) |x|$ for $x < 0$.\n\n## Computing the WIS \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nWIS <- function(truth, estimates, quantiles) {\n  2 * sum(pmax(\n    quantiles * (truth - estimates),\n    (1 - quantiles) * (estimates - truth),\n    na.rm = TRUE\n  ))\n}\n```\n:::\n\n\n\n::: {.callout-important icon=\"false\"}\n## Note\nWIS tends to prioritize sharpness (how wide the interval is) relative to \ncoverage (if the interval contains the truth).\n:::\n\n## WIS for ARX fitted via `lm` and `rq`\n\n* The lowest mean WIS is attained by quantile regression trained on all past data. \n\n* Notice that this method has coverage below 95\\% but it is still preferred under WIS \nbecause its intervals are narrower than for linear regression.\n\n::: flex\n\n::: w-50\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  method      mean_wis\n  <chr>          <dbl>\n1 lm.all.past   0.0247\n2 lm.trailing   0.0274\n```\n\n\n:::\n:::\n\n\n:::\n\n::: w-50\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  method      mean_wis\n  <chr>          <dbl>\n1 rq.all.past   0.0242\n2 rq.trailing   0.0348\n```\n\n\n:::\n:::\n\n\n\n:::\n:::\n\n# Forecasting with Versioned Data\n\n## Versioned data\n\n* In our forecasting examples, we have assumed the data are never revised \n(or have simply ignored revisions, and used data `as_of` today)\n\n::: {.callout-important icon=\"false\"}\n## \nHow can we train forecasters when dealing with versioned data?\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata_archive\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-04-01 / 2021-12-31\nℹ First/last version with update: 2020-04-02 / 2022-01-01\nℹ Versions end: 2022-01-01\nℹ A preview of the table (93566 rows x 5 columns):\nKey: <geo_value, time_value, version>\n       geo_value time_value    version case_rate death_rate\n          <char>     <Date>     <Date>     <num>      <num>\n    1:        ak 2020-04-01 2020-04-02  1.797489          0\n    2:        ak 2020-04-01 2020-05-07  1.777061          0\n    3:        ak 2020-04-01 2020-10-28  1.106147          0\n    4:        ak 2020-04-01 2020-10-29  1.797489          0\n    5:        ak 2020-04-01 2020-10-30  1.797489          0\n   ---                                                     \n93562:        wy 2021-12-27 2021-12-28 65.598769          0\n93563:        wy 2021-12-28 2021-12-29 50.315286          0\n93564:        wy 2021-12-29 2021-12-30 55.810471          0\n93565:        wy 2021-12-30 2021-12-31 68.002912          0\n93566:        wy 2021-12-31 2022-01-01  0.000000          0\n```\n\n\n:::\n:::\n\n\n\n## Version-aware forecasting\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# initialize dataframe for predictions\n# 5 columns: forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_all_past = pred_trailing <- data.frame(matrix(NA, ncol = 5, nrow = 0))\ncolnames(pred_all_past) = colnames(pred_trailing) <- c(\"forecast_date\", \"target_date\",\n                                                       'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nw <- 30         #trailing window size\nh <- 7          #number of days ahead\n\n# dates when predictions are made (set to be 1 month apart)\nfc_time_values <- seq(from = t0_date, to = as.Date(\"2021-12-31\"), by = \"1 month\")\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data <- epix_as_of(ca_archive, max_version = as.Date(fc_date))\n  # create lagged predictors\n  data$lagged_deaths <- dplyr::lag(data$deaths, h) #since we want to predict h-ahead, \n                                                   #we need to lag deaths by h (at least)\n  data$lagged_cases <- dplyr::lag(data$cases, k)\n  # perform quantile regression\n  rq_all_past <- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, data = data) \n  rq_trailing <- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, \n                    # only consider window of data\n                    data = data %>% filter(time_value > (max(time_value) - w))) \n  # construct data.frame with the right predictors for the target date\n  predictors <- data.frame(lagged_deaths = tail(data$deaths, 1), \n                           lagged_cases = (data %>% \n                                             filter(time_value == (max(time_value) + h - k)))$cases)\n  # make predictions for target date and add them to matrix of predictions\n  pred_all_past <- rbind(pred_all_past, \n                         data.frame('forecast_date' = max(data$time_value),\n                                    'target_date' = max(data$time_value) + h, \n                                    predict(rq_all_past, newdata = predictors)))\n  pred_trailing <- rbind(pred_trailing, \n                         data.frame('forecast_date' = max(data$time_value),\n                                    'target_date' = max(data$time_value) + h, \n                                    predict(rq_trailing, newdata = predictors)))\n}\n```\n:::\n\n\n\n## Version-aware predictions (CV, all past)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/plot-versioned-cv-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Version-awere predictions (CV, trailing)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/plot-versioned-cv-trailing-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n# Geo-pooling\n\n## Using geo information\n\n* Assume we observe data over time from [multiple locations]{.primary}\n(e.g. states or counties).\n\n* We could\n\n  * Estimate coefficients [separately]{.primary} for each location (as we have done so far).\n  \n  * Fit one model using all locations together at each time point ([geo-pooling]{.primary}). \nEstimated coefficients will not be location specific.\n\n  * Estimate coefficients separately for each location, but include predictors capturing \naverages across locations ([partial geo-pooling]{.primary}).\n\n\n\n## Geo-pooling: CV all past\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nusa_archive <- data_archive$DT %>% \n  as_epi_archive()\n\n# initialize dataframe for predictions\n# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_all_past <- data.frame(matrix(NA, ncol = 6, nrow = 0))\ncolnames(pred_all_past) <- c('geo_value', 'forecast_date', 'target_date',\n                             'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nh <- 7     #number of days ahead\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data <- epix_as_of(usa_archive, max_version = as.Date(fc_date))\n  \n  # create lagged predictors for each state \n  data <- data %>%\n    arrange(geo_value, time_value) %>%  \n    group_by(geo_value) %>%\n    mutate(lagged_deaths = dplyr::lag(deaths, h),\n           lagged_cases = dplyr::lag(cases, k)) %>%\n    ungroup()\n  \n  # perform quantile regression\n  rq_all_past <- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, data = data) \n  \n  # construct dataframe with the right predictors for the target date\n  new_lagged_deaths <- data %>% \n    filter(time_value == max(time_value)) %>%\n    select(geo_value, deaths)\n  \n  new_lagged_cases <- data %>% \n    filter(time_value == max(time_value) + h - k) %>%\n    select(geo_value, cases)\n  \n  predictors <- new_lagged_deaths %>%\n    inner_join(new_lagged_cases, join_by(geo_value)) %>%\n    rename(lagged_deaths = deaths,\n           lagged_cases = cases)\n  \n  # make predictions for target date and add them to matrix of predictions\n  pred_all_past <- rbind(pred_all_past, \n                         data.frame(\n                           'geo_value' = predictors$geo_value,\n                           'forecast_date' = max(data$time_value),\n                           'target_date' = max(data$time_value) + h, \n                           predict(rq_all_past, newdata = predictors)))\n}\n\n# geo-pooled predictions for California\npred_ca <- pred_all_past %>%\n  filter(geo_value == 'ca') %>%\n  rename(median = `tau..0.500`,\n         lower = `tau..0.025`,\n         upper = `tau..0.975`) %>%\n  full_join(ca %>% select(time_value, deaths), join_by(target_date == time_value)) %>%\n  arrange(target_date)\n```\n:::\n\n\n\n## Geo-pooled predictions for California\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/plot-geo-pooling-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Partial geo-pooling: CV all past\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# initialize dataframe for predictions\n# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_all_past <- data.frame(matrix(NA, ncol = 6, nrow = 0))\ncolnames(pred_all_past) <- c('geo_value', 'forecast_date', 'target_date',\n                             'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nh <- 7     #number of days ahead\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data <- epix_as_of(usa_archive, max_version = as.Date(fc_date))\n  \n  # create lagged predictors \n  data <- data %>%\n    arrange(geo_value, time_value) %>%  \n    group_by(geo_value) %>%\n    mutate(lagged_deaths = dplyr::lag(deaths, h),\n           lagged_cases = dplyr::lag(cases, k)) %>%\n    ungroup() %>%\n    group_by(time_value) %>%\n    mutate(avg_lagged_deaths = mean(lagged_deaths, na.rm = T),\n           avg_lagged_cases = mean(lagged_cases, na.rm = T)) %>%\n    ungroup() \n  \n  # perform quantile regression\n  rq_all_past <- rq(deaths ~ lagged_deaths + lagged_cases + avg_lagged_deaths +\n                      avg_lagged_cases, tau = quantiles, \n                    data = (data %>% filter(geo_value == 'ca'))) \n  \n  # construct data.frame with the right predictors for the target date\n  new_lagged_deaths <- data %>% \n    filter(time_value == max(time_value)) %>%\n    select(geo_value, deaths) %>%\n    mutate(avg_lagged_deaths = mean(deaths, na.rm = T)) %>%\n    filter(geo_value == 'ca')\n  \n  new_lagged_cases <- data %>% \n    filter(time_value == max(time_value) + h - k) %>%\n    select(geo_value, cases) %>%\n    mutate(avg_lagged_cases = mean(cases, na.rm = T)) %>%\n    filter(geo_value == 'ca')\n  \n  predictors <- new_lagged_deaths %>%\n    inner_join(new_lagged_cases, join_by(geo_value)) %>%\n    rename(lagged_deaths = deaths,\n           lagged_cases = cases)\n  \n  # make predictions for target date and add them to matrix of predictions\n  pred_all_past <- rbind(pred_all_past, \n                         data.frame(\n                           'geo_value' = predictors$geo_value,\n                           'forecast_date' = max(data$time_value),\n                           'target_date' = max(data$time_value) + h, \n                           predict(rq_all_past, newdata = predictors)))\n}\n\n# partially geo-pooled predictions for California\npred_ca <- pred_all_past %>%\n  filter(geo_value == 'ca') %>%\n  rename(median = `tau..0.500`,\n         lower = `tau..0.025`,\n         upper = `tau..0.975`) %>%\n  full_join(ca %>% select(time_value, deaths), join_by(target_date == time_value)) %>%\n  arrange(target_date)\n```\n:::\n\n\n\n## Partially geo-pooled predictions for California\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](gfx/plot-partial-geo-pooling-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Final slide {.smaller}\n\n### Thanks:\n\n\n\n\n\n\n\n- The whole [CMU Delphi Team](https://delphi.cmu.edu/about/team/) (across many institutions)\n- Optum/UnitedHealthcare, Change Healthcare.\n- Google, Facebook, Amazon Web Services.\n- Quidel, SafeGraph, Qualtrics.\n- Centers for Disease Control and Prevention.\n- Council of State and Territorial Epidemiologists\n\n\n::: {layout-row=1 fig-align=\"center\"}\n![](gfx/delphi.jpg){height=\"100px\"}\n![](gfx/berkeley.jpg){height=\"100px\"}\n![](gfx/cmu.jpg){height=\"100px\"}\n![](gfx/ubc.jpg){width=\"250px\"}\n![](gfx/stanford.jpg){width=\"250px\"}\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}